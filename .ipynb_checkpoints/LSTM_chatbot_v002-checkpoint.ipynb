{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaaf389c-7ea8-4635-838f-8fc9c9984e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in /shared/home/u076079/.local/lib/python3.8/site-packages (7.6.5)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.0.7-py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (6.17.1)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Downloading jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.7\n",
      "  Downloading widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.31)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /usr/lib/python3/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.8)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.5.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.5.2\n",
      "    Uninstalling widgetsnbextension-3.5.2:\n",
      "      Successfully uninstalled widgetsnbextension-3.5.2\n",
      "  Attempting uninstall: jupyterlab-widgets\n",
      "    Found existing installation: jupyterlab-widgets 3.0.5\n",
      "    Uninstalling jupyterlab-widgets-3.0.5:\n",
      "      Successfully uninstalled jupyterlab-widgets-3.0.5\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.6.5\n",
      "    Uninstalling ipywidgets-7.6.5:\n",
      "      Successfully uninstalled ipywidgets-7.6.5\n",
      "Successfully installed ipywidgets-8.0.7 jupyterlab-widgets-3.0.8 widgetsnbextension-4.0.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cac7a65-ec61-41de-b1b3-2e4b948878af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b268895-2c44-4e5e-b670-fe5afa5e483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/u076079/envs/LSTM_torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9638f7f9-9f5d-4b07-b1f8-bc606d72a5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu116'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ebbed174-194d-4c25-96cc-5e1edb7c328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9581050a-b891-4dd7-bad9-33831d6b5688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4cfb1200-1c60-4adc-a8da-8ef0e15b57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8329a5c-c38e-4972-ac41-9f62a99e5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8a595278-1d94-4a2b-bf14-8bd33cf3d286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "74d2581e-1792-4661-8bc5-d69f0c431183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import SQuAD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "136c9494-58b0-467e-a3ed-299f5d74ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = SQuAD1(\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "47d556db-92d3-4fb8-bd37-b7859e661c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data import get_dataframe,  get_pairs_from_df, cols, sample_df_perc, get_thresholds, get_outliers, tokenize_sentence, remove_least_common, to_tensor,  filter_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "07b3ff98-f08c-4dd4-823f-517c04d18954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test dataframes of sentences\n",
    "train_df, test_df = get_dataframe(train), get_dataframe(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "86ea0c62-1f46-4d6c-b2e5-24be9f2391ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = sample_df_perc(train_df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "34b23621-67b7-48aa-ba11-a3e596837a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = sample_df_perc(test_df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8730f375-d6ed-4967-95dc-0a51ab303b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17519, 2), (2114, 2))"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1890f013-d804-4aa2-9909-d4cfcbf4ff80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What started the modern petroleum industry ?</td>\n",
       "      <td>the discovery of the process of refining keros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What failure caused the the flares of civil wa...</td>\n",
       "      <td>Second Triumvirate of Octavian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where did Victoria become seriously ill during...</td>\n",
       "      <td>Ramsgate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many domestic overnight visitors came to M...</td>\n",
       "      <td>57.7 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who failed to take advantage of the retreat?</td>\n",
       "      <td>local commanders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0       What started the modern petroleum industry ?   \n",
       "1  What failure caused the the flares of civil wa...   \n",
       "2  Where did Victoria become seriously ill during...   \n",
       "3  How many domestic overnight visitors came to M...   \n",
       "4       Who failed to take advantage of the retreat?   \n",
       "\n",
       "                                              Answer  \n",
       "0  the discovery of the process of refining keros...  \n",
       "1                     Second Triumvirate of Octavian  \n",
       "2                                           Ramsgate  \n",
       "3                                       57.7 million  \n",
       "4                                   local commanders  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "34317e15-c531-4fe8-af27-cfbce8348003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How long is a full-time mission for females?\n",
      "['how', 'long', 'fulltime', 'mission', 'females']\n",
      "['how', 'long', 'fulltim', 'mission', 'femal']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand_question = train_df.at[random.randint(0,train_df.shape[0]), 'Question']\n",
    "print(rand_question)\n",
    "\n",
    "print(tokenize_sentence(rand_question))\n",
    "print(tokenize_sentence(rand_question, normalization='stem'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42619f3-808a-4386-b814-29b605b45eb0",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8074228e-0725-49b0-994c-0a6a7f2c3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211b3a2-cc03-4742-8cc6-bb700b1efbf3",
   "metadata": {},
   "source": [
    "## Make pairs to add to the vocabularies. \n",
    "\n",
    "#### Only the questions will be normalized (stemmed) but not the answers - otherwise we would get stemmed words in the chatbot answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "05a5fafb-9725-4dfc-b2fd-95be53922126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, norm in zip(cols, ['stem', None]):\n",
    "    train_df[f'{col}_tokens'] = train_df[col].apply(lambda s: tokenize_sentence(s, normalization=norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c54bd574-67e0-462c-97e8-9cb0e039ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, norm in zip(cols, ['stem', None]):\n",
    "    test_df[f'{col}_tokens'] = test_df[col].apply(lambda s: tokenize_sentence(s, normalization=norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "22dc471e-79f9-4803-95f5-9f9cc75fa245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Question_tokens</th>\n",
       "      <th>Answer_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>How many fans saw Queen in Knebworth park?</td>\n",
       "      <td>120,000</td>\n",
       "      <td>[how, mani, fan, saw, queen, knebworth, park]</td>\n",
       "      <td>[120000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>what was an earlier technology used to connect...</td>\n",
       "      <td>telephone lines</td>\n",
       "      <td>[what, earlier, technolog, use, connect, inter...</td>\n",
       "      <td>[telephone, lines]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>What does POTS stand for?</td>\n",
       "      <td>Plain Old Telephone System</td>\n",
       "      <td>[what, pot, stand]</td>\n",
       "      <td>[plain, old, telephone, system]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "1835         How many fans saw Queen in Knebworth park?   \n",
       "3679  what was an earlier technology used to connect...   \n",
       "5902                          What does POTS stand for?   \n",
       "\n",
       "                          Answer  \\\n",
       "1835                     120,000   \n",
       "3679             telephone lines   \n",
       "5902  Plain Old Telephone System   \n",
       "\n",
       "                                        Question_tokens  \\\n",
       "1835      [how, mani, fan, saw, queen, knebworth, park]   \n",
       "3679  [what, earlier, technolog, use, connect, inter...   \n",
       "5902                                 [what, pot, stand]   \n",
       "\n",
       "                        Answer_tokens  \n",
       "1835                         [120000]  \n",
       "3679               [telephone, lines]  \n",
       "5902  [plain, old, telephone, system]  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7db09e17-4729-452b-9a87-7648a8e62570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Question_tokens</th>\n",
       "      <th>Answer_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>What type of person can not be attributed civi...</td>\n",
       "      <td>head of government</td>\n",
       "      <td>[what, type, person, attribut, civil, disobedi]</td>\n",
       "      <td>[head, government]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>What was a similar view about the Asian contin...</td>\n",
       "      <td>orientalism</td>\n",
       "      <td>[what, similar, view, asian, contin, call]</td>\n",
       "      <td>[orientalism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Why the narrow part of St. John's River called...</td>\n",
       "      <td>cattle were brought across the river there.</td>\n",
       "      <td>[whi, narrow, part, st, john, river, call, cow...</td>\n",
       "      <td>[cattle, brought, across, river]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "564   What type of person can not be attributed civi...   \n",
       "1564  What was a similar view about the Asian contin...   \n",
       "490   Why the narrow part of St. John's River called...   \n",
       "\n",
       "                                           Answer  \\\n",
       "564                            head of government   \n",
       "1564                                  orientalism   \n",
       "490   cattle were brought across the river there.   \n",
       "\n",
       "                                        Question_tokens  \\\n",
       "564     [what, type, person, attribut, civil, disobedi]   \n",
       "1564         [what, similar, view, asian, contin, call]   \n",
       "490   [whi, narrow, part, st, john, river, call, cow...   \n",
       "\n",
       "                         Answer_tokens  \n",
       "564                 [head, government]  \n",
       "1564                     [orientalism]  \n",
       "490   [cattle, brought, across, river]  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "24afe769-580f-4867-acfd-d7bb226fa9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_tokens = [f'{col}_tokens' for col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f9b0972b-6e60-4f80-838d-07e0237f71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = get_pairs_from_df(train_df, cols_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c48f3440-c41a-4bad-bed0-b0c02eabc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = get_pairs_from_df(test_df, cols_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5a45c26b-0c95-4e47-88ca-786e39fe9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vocab, A_vocab = Vocab(), Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0445aeef-c890-416d-b920-8a818e27917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in train_pairs:\n",
    "    Q_vocab.add_sentence(pair.question)\n",
    "    A_vocab.add_sentence(pair.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ea9d035a-0692-48cb-97be-d79d34f6caf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12856, 16191)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_vocab.n_words, A_vocab.n_words, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4c429759-d81c-4efd-abb1-d1d4a0275a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in test_pairs:\n",
    "    Q_vocab.add_sentence(pair.question)\n",
    "    A_vocab.add_sentence(pair.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "454e54b7-aaa5-4528-8661-a02ab7ccb176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13781, 17429)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_vocab.n_words, A_vocab.n_words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b59c1-0728-4b76-b34d-8509fb6a30f4",
   "metadata": {},
   "source": [
    "## Functions for some data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d2587a5f-77e6-4738-9683-8b03d3c76bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.stats import sentences_stats, histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "aca76fbf-fa8a-49b6-8a6c-392e276f93e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in column Question_tokens:\n",
      "\t         mean: 6.42\n",
      "\t         median: 6.00\n",
      "\t         minimum: 1\n",
      "\t         maximum: 20)\n",
      "Sentences in column Answer_tokens:\n",
      "\t         mean: 2.47\n",
      "\t         median: 2.00\n",
      "\t         minimum: 0\n",
      "\t         maximum: 22)\n"
     ]
    }
   ],
   "source": [
    "# statistics for tokenized sentences\n",
    "sentences_stats(train_df, cols_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c0fe5a7c-8fb8-4bac-87df-402088578244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in column Question_tokens:\n",
      "\t         mean: 6.47\n",
      "\t         median: 6.00\n",
      "\t         minimum: 2\n",
      "\t         maximum: 17)\n",
      "Sentences in column Answer_tokens:\n",
      "\t         mean: 2.32\n",
      "\t         median: 2.00\n",
      "\t         minimum: 0\n",
      "\t         maximum: 15)\n"
     ]
    }
   ],
   "source": [
    "# statistics for tokenized sentences\n",
    "sentences_stats(test_df, cols_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414c03f-828b-4176-a329-df927df12723",
   "metadata": {},
   "source": [
    "## Remove the least common words from the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "5b6c91a6-0c40-4898-a554-0f2fe1859806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times at most a word occurs to be considered an outlier\n",
    "outlier_threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "82a06d1d-9380-42c1-875a-aebc5bea498a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['flare',\n",
       "  'bavarian',\n",
       "  'gesamtschul',\n",
       "  'kralic',\n",
       "  'salami',\n",
       "  'joao',\n",
       "  'durabl',\n",
       "  'lifetim',\n",
       "  'summari',\n",
       "  'eugen',\n",
       "  'bohm',\n",
       "  'grandfath',\n",
       "  'froze',\n",
       "  '89',\n",
       "  'sensation',\n",
       "  '35',\n",
       "  '58',\n",
       "  'fishermen',\n",
       "  'triumph',\n",
       "  '1330',\n",
       "  'narayanh',\n",
       "  'chord',\n",
       "  'imper',\n",
       "  'extol',\n",
       "  'patright',\n",
       "  'nonpolit',\n",
       "  'autosembl',\n",
       "  'corps',\n",
       "  'lynx',\n",
       "  'xinhua',\n",
       "  'joosten',\n",
       "  'lodensteijn',\n",
       "  'noiseless',\n",
       "  'articul',\n",
       "  'brendan',\n",
       "  'snif',\n",
       "  'hyderabadi',\n",
       "  'langaug',\n",
       "  'spirometri',\n",
       "  'dowd',\n",
       "  'firefox',\n",
       "  'scooter',\n",
       "  'intellect',\n",
       "  'vigour',\n",
       "  '30pin',\n",
       "  'melcher',\n",
       "  'mitrailleus',\n",
       "  'elixir',\n",
       "  'sulfanilamid',\n",
       "  'eponym',\n",
       "  'discredit',\n",
       "  'yeast',\n",
       "  'pyometra',\n",
       "  'irokawa',\n",
       "  'daikichi',\n",
       "  'dracula',\n",
       "  'dockyard',\n",
       "  'bali',\n",
       "  'hamid',\n",
       "  'unseat',\n",
       "  'choskunskyab',\n",
       "  'regress',\n",
       "  'postral',\n",
       "  '201415',\n",
       "  'arigucultur',\n",
       "  'mutt',\n",
       "  'purebr',\n",
       "  'aavik',\n",
       "  'commisss',\n",
       "  'aloud',\n",
       "  'tthe',\n",
       "  'irenaeus',\n",
       "  'credibl',\n",
       "  'lyre',\n",
       "  'cittern',\n",
       "  'timbr',\n",
       "  'highintens',\n",
       "  'subleas',\n",
       "  'suicid',\n",
       "  'léon',\n",
       "  'watercool',\n",
       "  'pompom',\n",
       "  'wouldnt',\n",
       "  '18645',\n",
       "  'walrasian',\n",
       "  'rockefel',\n",
       "  '128k',\n",
       "  'habit',\n",
       "  'prunus',\n",
       "  'spinosa',\n",
       "  'marqui',\n",
       "  'quell',\n",
       "  'paranoa',\n",
       "  'synthesi',\n",
       "  'headlamp',\n",
       "  'qaeda',\n",
       "  'woodsid',\n",
       "  'baideng',\n",
       "  'blackori',\n",
       "  'chile',\n",
       "  'highestyield',\n",
       "  'footag',\n",
       "  '114',\n",
       "  'rhodian',\n",
       "  'stateown',\n",
       "  '1734',\n",
       "  '1080i25',\n",
       "  '1080i50',\n",
       "  'rubia',\n",
       "  'tinctorum',\n",
       "  'fade',\n",
       "  'depos',\n",
       "  'dad',\n",
       "  'marketwatch',\n",
       "  'aicraft',\n",
       "  'conspiraci',\n",
       "  'scarantino',\n",
       "  'griffith',\n",
       "  'gearless',\n",
       "  'longrang',\n",
       "  '15000',\n",
       "  'couinti',\n",
       "  'qvc',\n",
       "  'yohann',\n",
       "  'arabia',\n",
       "  'pli',\n",
       "  'mesan',\n",
       "  'kpa',\n",
       "  'drone',\n",
       "  'todd',\n",
       "  'terri',\n",
       "  'kremlin',\n",
       "  'boehner',\n",
       "  'svengöran',\n",
       "  'eriksson',\n",
       "  'dipylon',\n",
       "  'yerevan',\n",
       "  'kidney',\n",
       "  'bloodstream',\n",
       "  '51',\n",
       "  'gridley',\n",
       "  'albatross',\n",
       "  'allston',\n",
       "  'preposit',\n",
       "  'gramposit',\n",
       "  'theer',\n",
       "  'memior',\n",
       "  'crier',\n",
       "  'flew',\n",
       "  'transatlant',\n",
       "  'mfa',\n",
       "  'enmebaragesi',\n",
       "  'phenomena',\n",
       "  'timeasymmetri',\n",
       "  'comparitv',\n",
       "  'buffalo',\n",
       "  'bayou',\n",
       "  'monoid',\n",
       "  'jcb',\n",
       "  'tisch',\n",
       "  'satisfact',\n",
       "  'entrench',\n",
       "  'koch',\n",
       "  'krugman',\n",
       "  'implos',\n",
       "  'undisput',\n",
       "  '150th',\n",
       "  'indianhindu',\n",
       "  'bundi',\n",
       "  'raisina',\n",
       "  'idf',\n",
       "  'howison',\n",
       "  'incompat',\n",
       "  'mixalot',\n",
       "  'macklemor',\n",
       "  'yanke',\n",
       "  'leaguer',\n",
       "  'reinsur',\n",
       "  'feinburg',\n",
       "  '22nd',\n",
       "  'cunard',\n",
       "  '20042005',\n",
       "  'sacramento',\n",
       "  'mini',\n",
       "  'neverhood',\n",
       "  'confucius',\n",
       "  'shipper',\n",
       "  'briberi',\n",
       "  'collus',\n",
       "  'transmt',\n",
       "  'fred',\n",
       "  'hutchington',\n",
       "  'arkefli',\n",
       "  'gemin',\n",
       "  'bowersox',\n",
       "  'sulfurbacteria',\n",
       "  'remak',\n",
       "  'jacqu',\n",
       "  'derrida',\n",
       "  'enriqu',\n",
       "  'nan',\n",
       "  'nonbilaterian',\n",
       "  'pavement',\n",
       "  'coldmix',\n",
       "  'freez',\n",
       "  'intergend',\n",
       "  'newlin',\n",
       "  'modest',\n",
       "  'stagecoach',\n",
       "  'pfizer',\n",
       "  'lipitor',\n",
       "  'decolon',\n",
       "  'cameroon',\n",
       "  'satisfi',\n",
       "  'amanda',\n",
       "  'chanakyapuri',\n",
       "  'kyburg',\n",
       "  '1264',\n",
       "  'rupa',\n",
       "  'tird',\n",
       "  'absent',\n",
       "  'u19',\n",
       "  'gallic',\n",
       "  'gbad',\n",
       "  'aspernessl',\n",
       "  'multic',\n",
       "  'guna',\n",
       "  'concen',\n",
       "  'launc',\n",
       "  'gastric',\n",
       "  'bookstor',\n",
       "  'arrondiss',\n",
       "  'dealkal',\n",
       "  'debussi',\n",
       "  'noneuropean',\n",
       "  'curaçao',\n",
       "  'sint',\n",
       "  'maarten',\n",
       "  'apa',\n",
       "  'subsist',\n",
       "  'startup',\n",
       "  'disorgan',\n",
       "  '1733',\n",
       "  'semi',\n",
       "  'breakwat',\n",
       "  'liliuss',\n",
       "  'neutrino',\n",
       "  'clump',\n",
       "  'arsino',\n",
       "  'berenic',\n",
       "  'inflam',\n",
       "  'nonbriton',\n",
       "  'revolv',\n",
       "  'tharapita',\n",
       "  'homeown',\n",
       "  'exhaust',\n",
       "  'titch',\n",
       "  'dawlish',\n",
       "  'nonnat',\n",
       "  'bureaus',\n",
       "  'hydra',\n",
       "  'stole',\n",
       "  'calabash',\n",
       "  'intrafram',\n",
       "  'easiest',\n",
       "  'shortterm',\n",
       "  'immort',\n",
       "  'flagship',\n",
       "  'endospor',\n",
       "  'dormant',\n",
       "  'degener',\n",
       "  'reburi',\n",
       "  'witch',\n",
       "  'wnyc',\n",
       "  'cracov',\n",
       "  '1740',\n",
       "  'agd',\n",
       "  'semest',\n",
       "  'margo',\n",
       "  'mccafferi',\n",
       "  'sleepi',\n",
       "  'patoi',\n",
       "  '97',\n",
       "  'nonafrican',\n",
       "  'institutionalis',\n",
       "  'letterman',\n",
       "  'ortiz',\n",
       "  'ethnoraci',\n",
       "  'vassal',\n",
       "  'kliment',\n",
       "  'voroshilov',\n",
       "  'teresa',\n",
       "  'banská',\n",
       "  'akadémia',\n",
       "  'formulia',\n",
       "  'nixon',\n",
       "  'gmt',\n",
       "  'gillingham',\n",
       "  'edific',\n",
       "  'sentencestyl',\n",
       "  'gear',\n",
       "  'ecotourist',\n",
       "  'immunologist',\n",
       "  'pannonia',\n",
       "  'natowarsaw',\n",
       "  'intrigu',\n",
       "  'otherwis',\n",
       "  'eplan',\n",
       "  'marian',\n",
       "  'nonintern',\n",
       "  'rescat',\n",
       "  'inshor',\n",
       "  'gallipoli',\n",
       "  'moha',\n",
       "  'maryland',\n",
       "  'irv',\n",
       "  'tuareg',\n",
       "  'conjug',\n",
       "  'bernard',\n",
       "  'suribachi',\n",
       "  'nano',\n",
       "  'chick',\n",
       "  'saltation',\n",
       "  'annul',\n",
       "  'haruspici',\n",
       "  'uttahrir',\n",
       "  'kaiser',\n",
       "  'moldavia',\n",
       "  'wallachia',\n",
       "  'par',\n",
       "  'mateo',\n",
       "  'mato',\n",
       "  'warlik',\n",
       "  'beartooth',\n",
       "  'britannicus',\n",
       "  'ntb',\n",
       "  'heaviest',\n",
       "  'funnel',\n",
       "  'whaen',\n",
       "  '1712',\n",
       "  'sheriff',\n",
       "  'kampong',\n",
       "  'misr',\n",
       "  'battersea',\n",
       "  'longitudin',\n",
       "  'basal',\n",
       "  'meltwat',\n",
       "  '48027',\n",
       "  'urartu',\n",
       "  'sigmund',\n",
       "  'beringia',\n",
       "  'risktak',\n",
       "  'iranoaryan',\n",
       "  'ucr',\n",
       "  'cheshir',\n",
       "  'rifleman',\n",
       "  'sizeabl',\n",
       "  'hysteresi',\n",
       "  'dicot',\n",
       "  'magnoliid',\n",
       "  'shofar',\n",
       "  'wallacea',\n",
       "  'sikh',\n",
       "  'nucleotid',\n",
       "  'terman',\n",
       "  'concillor',\n",
       "  'isaiah',\n",
       "  '714',\n",
       "  'widen',\n",
       "  'cardomin',\n",
       "  'hamper',\n",
       "  'pitchfork',\n",
       "  'ft',\n",
       "  'odyssey',\n",
       "  '1778',\n",
       "  'saltpetr',\n",
       "  'untax',\n",
       "  'synergi',\n",
       "  'nonmed',\n",
       "  'mumbo',\n",
       "  'jumbo',\n",
       "  'pulmonologist',\n",
       "  'transpar',\n",
       "  '1826',\n",
       "  'zincl',\n",
       "  'nontradit',\n",
       "  'microwav',\n",
       "  'slovenia',\n",
       "  'talangana',\n",
       "  'midevil',\n",
       "  'technician',\n",
       "  '181st',\n",
       "  'vincenzo',\n",
       "  'galilei',\n",
       "  'zz',\n",
       "  'armori',\n",
       "  'bow',\n",
       "  '18767779',\n",
       "  'johan',\n",
       "  'cruyff',\n",
       "  'regardin',\n",
       "  'regentgovernor',\n",
       "  '1446',\n",
       "  'mba',\n",
       "  '2ndbest',\n",
       "  'bleed',\n",
       "  'lso',\n",
       "  'lowgrad',\n",
       "  'piri',\n",
       "  'babbag',\n",
       "  'asscoci',\n",
       "  'algerian',\n",
       "  'm1',\n",
       "  'buildt',\n",
       "  'anyway',\n",
       "  'idaho',\n",
       "  'nec',\n",
       "  'octav',\n",
       "  'tethi',\n",
       "  'bmt',\n",
       "  'mma',\n",
       "  'kidinnu',\n",
       "  'chanakypuri',\n",
       "  'drydock',\n",
       "  'nascent',\n",
       "  'omniprest',\n",
       "  'weaponri',\n",
       "  'secotr',\n",
       "  'soror',\n",
       "  'trolley',\n",
       "  'modal',\n",
       "  '9808',\n",
       "  'bestknown',\n",
       "  'harmon',\n",
       "  'heighten',\n",
       "  'mum',\n",
       "  'openmind',\n",
       "  'jordin',\n",
       "  'solicitor',\n",
       "  'swingl',\n",
       "  'sanha',\n",
       "  'commer',\n",
       "  '00s',\n",
       "  'nadg',\n",
       "  'germanaustria',\n",
       "  'wy',\n",
       "  'b2a',\n",
       "  'dartford',\n",
       "  'poissi',\n",
       "  'brasseri',\n",
       "  'shikai',\n",
       "  'emomalii',\n",
       "  'rahmon',\n",
       "  'shmuel',\n",
       "  'yosef',\n",
       "  'agnon',\n",
       "  '201314',\n",
       "  'foramina',\n",
       "  'premaxilla',\n",
       "  'quigg',\n",
       "  'tagma',\n",
       "  'ampute',\n",
       "  'conseil',\n",
       "  'virtuoso',\n",
       "  'needl',\n",
       "  'ivori',\n",
       "  'sos',\n",
       "  'mise',\n",
       "  'frediano',\n",
       "  'lucca',\n",
       "  'trocadéro',\n",
       "  'reptilian',\n",
       "  'cephal',\n",
       "  'overrod',\n",
       "  'daimyo',\n",
       "  'yengal',\n",
       "  'blitz',\n",
       "  'nucleusorbit',\n",
       "  'stanley',\n",
       "  'schachter',\n",
       "  'repay',\n",
       "  'moin',\n",
       "  'osha',\n",
       "  'mp',\n",
       "  'zif',\n",
       "  'philosphi',\n",
       "  'soonest',\n",
       "  'emitt',\n",
       "  'oberhaus',\n",
       "  'perdicca',\n",
       "  'siemen',\n",
       "  'halsk',\n",
       "  'zhongshan',\n",
       "  'hanzhong',\n",
       "  'morph',\n",
       "  'imperfect',\n",
       "  'holli',\n",
       "  'compton',\n",
       "  'phenomenalist',\n",
       "  'yoritomo',\n",
       "  'mili',\n",
       "  '95',\n",
       "  'cardiovascular',\n",
       "  'nemb',\n",
       "  'creek',\n",
       "  'invold',\n",
       "  'pratihara',\n",
       "  'raetian',\n",
       "  'ltro',\n",
       "  'marieantoinett',\n",
       "  'betwen',\n",
       "  'ego',\n",
       "  'pslc',\n",
       "  'archimed',\n",
       "  'kay',\n",
       "  'guangwu',\n",
       "  'capsul',\n",
       "  'deistic',\n",
       "  'inimit',\n",
       "  '1078',\n",
       "  'hearer',\n",
       "  'ajuran',\n",
       "  'arenafootball2',\n",
       "  'neopren',\n",
       "  'airland',\n",
       "  'willi',\n",
       "  'brandt',\n",
       "  '28',\n",
       "  'phramacist',\n",
       "  'diderot',\n",
       "  'hohokam',\n",
       "  'zoetrop',\n",
       "  'attic',\n",
       "  'pastri',\n",
       "  'allaccord',\n",
       "  'comrad',\n",
       "  'rot',\n",
       "  'sheahan',\n",
       "  'barney',\n",
       "  'mckenna',\n",
       "  'jovanka',\n",
       "  'noctul',\n",
       "  'kennel',\n",
       "  '899',\n",
       "  'carv',\n",
       "  'orchestr',\n",
       "  'beidou1d',\n",
       "  'fundrais',\n",
       "  'downturn',\n",
       "  'cimetier',\n",
       "  'catacomb',\n",
       "  'click',\n",
       "  'hemagglutinin',\n",
       "  'europeon',\n",
       "  'itu',\n",
       "  'dissapear',\n",
       "  'embarrass',\n",
       "  'emmett',\n",
       "  'marj',\n",
       "  'rahit',\n",
       "  'fdni',\n",
       "  'inconsist',\n",
       "  'guilford',\n",
       "  'courthous',\n",
       "  'apra',\n",
       "  'flush',\n",
       "  'openair',\n",
       "  'tynwald',\n",
       "  'thunderbolt',\n",
       "  'bamako',\n",
       "  'shabell',\n",
       "  'gravelott',\n",
       "  'hikmah',\n",
       "  'overus',\n",
       "  'borgata',\n",
       "  'crunch',\n",
       "  'boulogn',\n",
       "  'vincenn',\n",
       "  'calcutta',\n",
       "  'parker',\n",
       "  'priveleg',\n",
       "  'habsburg',\n",
       "  'garamant',\n",
       "  'ocf',\n",
       "  'malici',\n",
       "  'illinoi',\n",
       "  'sinojapanes',\n",
       "  '150',\n",
       "  'envi',\n",
       "  'seawat',\n",
       "  'summaris',\n",
       "  'forgotten',\n",
       "  'rediscov',\n",
       "  'cocci',\n",
       "  'characterict',\n",
       "  'goiter',\n",
       "  'egalitarian',\n",
       "  'conceal',\n",
       "  'eclect',\n",
       "  'dbms',\n",
       "  'tn',\n",
       "  'refuel',\n",
       "  'coelom',\n",
       "  'olmec',\n",
       "  'temper',\n",
       "  'emmerg',\n",
       "  'ortega',\n",
       "  'highris',\n",
       "  'mercado',\n",
       "  'manuel',\n",
       "  'bletchley',\n",
       "  'acclimat',\n",
       "  'congoles',\n",
       "  'avalon',\n",
       "  'hilleman',\n",
       "  'returncr',\n",
       "  'cassino',\n",
       "  'mosaicist',\n",
       "  'haiti',\n",
       "  'rousseau',\n",
       "  'abmad',\n",
       "  'ibrahim',\n",
       "  'alghazi',\n",
       "  'aclu',\n",
       "  'stopov',\n",
       "  'quadrupol',\n",
       "  'earlship',\n",
       "  'somtal',\n",
       "  'reval',\n",
       "  'lübeck',\n",
       "  'stern',\n",
       "  'vidin',\n",
       "  'omar',\n",
       "  'pasha',\n",
       "  'atticus',\n",
       "  'benihana',\n",
       "  'burger',\n",
       "  'paley',\n",
       "  'photographi',\n",
       "  'appalachian',\n",
       "  'siku',\n",
       "  'quanshu',\n",
       "  '976',\n",
       "  'bismarck',\n",
       "  'youlous',\n",
       "  'pelham',\n",
       "  'tammani',\n",
       "  'commanderinchief',\n",
       "  'saraiki',\n",
       "  'helepoli',\n",
       "  'undecid',\n",
       "  'tc',\n",
       "  'karachi',\n",
       "  'screvin',\n",
       "  'neck',\n",
       "  'slimmer',\n",
       "  'triatom',\n",
       "  'delaunay',\n",
       "  'kraków',\n",
       "  'disrepair',\n",
       "  'dissent',\n",
       "  'knebworth',\n",
       "  'null',\n",
       "  'offsal',\n",
       "  'ćw',\n",
       "  'dźw',\n",
       "  'folder',\n",
       "  'meiji',\n",
       "  'io',\n",
       "  'iphon',\n",
       "  'ipad',\n",
       "  'wrighton',\n",
       "  'macedoniank',\n",
       "  'disinfect',\n",
       "  'safekeep',\n",
       "  'suround',\n",
       "  'threedimension',\n",
       "  'expatri',\n",
       "  'χριστιανός',\n",
       "  'christiano',\n",
       "  'townsfolk',\n",
       "  'jablonski',\n",
       "  'allgemein',\n",
       "  '1721',\n",
       "  'frost',\n",
       "  'daviau',\n",
       "  'exclusionari',\n",
       "  'strategist',\n",
       "  'anglodutch',\n",
       "  'groundwat',\n",
       "  'optimist',\n",
       "  'xie',\n",
       "  'neurophysiologist',\n",
       "  'ilbrari',\n",
       "  'flightless',\n",
       "  'monthlong',\n",
       "  'avogadro',\n",
       "  'ural',\n",
       "  'arenabowl',\n",
       "  'xxvii',\n",
       "  'bayonc',\n",
       "  'torpor',\n",
       "  'zant',\n",
       "  'matrox',\n",
       "  'unconvent',\n",
       "  'mounti',\n",
       "  'thirti',\n",
       "  'broadest',\n",
       "  'phylogenet',\n",
       "  'eiji',\n",
       "  'yoshikawa',\n",
       "  'eilenried',\n",
       "  'rig',\n",
       "  'walker',\n",
       "  'unjustifi',\n",
       "  'ilkhan',\n",
       "  '1256',\n",
       "  'hydrostat',\n",
       "  'phd',\n",
       "  'selenizza',\n",
       "  'deinonychosaur',\n",
       "  'originallyplan',\n",
       "  'anta',\n",
       "  'spori',\n",
       "  'olden',\n",
       "  'emerald',\n",
       "  'necklac',\n",
       "  'monocentr',\n",
       "  'qutb',\n",
       "  'pagoda',\n",
       "  'slap',\n",
       "  'mimic',\n",
       "  'samaveda',\n",
       "  'tricycl',\n",
       "  'antidepress',\n",
       "  'mistwalk',\n",
       "  'supremaci',\n",
       "  'garfinckel',\n",
       "  'fitzroy',\n",
       "  'kujhlp',\n",
       "  'gnb',\n",
       "  'physiolog',\n",
       "  'tagalog',\n",
       "  'megabus',\n",
       "  'chalfont',\n",
       "  'omega6',\n",
       "  'dgla',\n",
       "  'priceshar',\n",
       "  'divan',\n",
       "  'facemask',\n",
       "  'parasitoidhost',\n",
       "  'farley',\n",
       "  'funk',\n",
       "  'allencopass',\n",
       "  '1678',\n",
       "  'barbieri',\n",
       "  'paleoptera',\n",
       "  'neoptera',\n",
       "  'alkalin',\n",
       "  'ph',\n",
       "  'magist',\n",
       "  'collegi',\n",
       "  'nsdap',\n",
       "  'withheld',\n",
       "  'imagenari',\n",
       "  'perpindicular',\n",
       "  'domagk',\n",
       "  'storedprogram',\n",
       "  'gag',\n",
       "  'singlepanel',\n",
       "  'bluth',\n",
       "  'skirt',\n",
       "  'hemlin',\n",
       "  'neccessari',\n",
       "  'plow',\n",
       "  '98',\n",
       "  'inhous',\n",
       "  'auditorium',\n",
       "  'torii',\n",
       "  'mototada',\n",
       "  'jrs',\n",
       "  'paleontolog',\n",
       "  'stonewal',\n",
       "  'hartshorn',\n",
       "  'partygovern',\n",
       "  'herald',\n",
       "  'uruk',\n",
       "  'seventi',\n",
       "  'lucha',\n",
       "  'libr',\n",
       "  'somewher',\n",
       "  'surfacelevel',\n",
       "  'goodman',\n",
       "  'knockout',\n",
       "  'caretak',\n",
       "  '330',\n",
       "  'genealogist',\n",
       "  'flyway',\n",
       "  'flea',\n",
       "  'tick',\n",
       "  'mite',\n",
       "  'efficiencywis',\n",
       "  'mossad',\n",
       "  'lealiyah',\n",
       "  'halvard',\n",
       "  'lang',\n",
       "  'avantgard',\n",
       "  'manserv',\n",
       "  'chop',\n",
       "  'taxpay',\n",
       "  'transposon',\n",
       "  'tes',\n",
       "  'residentialc',\n",
       "  'paramet',\n",
       "  'gaejangguk',\n",
       "  'morrisania',\n",
       "  'novella',\n",
       "  'saga',\n",
       "  'curious',\n",
       "  'closur',\n",
       "  'lenovo',\n",
       "  'misstep',\n",
       "  'tennese',\n",
       "  'metternich',\n",
       "  'archduk',\n",
       "  'ancetri',\n",
       "  'disgust',\n",
       "  'burgundian',\n",
       "  '436',\n",
       "  'firsthand',\n",
       "  '1666',\n",
       "  'kj',\n",
       "  'elvi',\n",
       "  'presley',\n",
       "  'ronald',\n",
       "  'dworkin',\n",
       "  'redford',\n",
       "  '1002',\n",
       "  'marsupi',\n",
       "  'perk',\n",
       "  'dibben',\n",
       "  'malfunct',\n",
       "  'kiarostami',\n",
       "  'dor',\n",
       "  'cann',\n",
       "  'cisalpin',\n",
       "  'reminisc',\n",
       "  'mausoleum',\n",
       "  'galla',\n",
       "  'placidia',\n",
       "  'bʰ',\n",
       "  'liszi',\n",
       "  'interceptor',\n",
       "  'xianzhong',\n",
       "  'tear',\n",
       "  'residu',\n",
       "  'crude',\n",
       "  'kittl',\n",
       "  'flota',\n",
       "  'sovetskovo',\n",
       "  'soyuza',\n",
       "  'kuznetsov',\n",
       "  'nobilita',\n",
       "  'flute',\n",
       "  '878',\n",
       "  'cyclad',\n",
       "  'shellac',\n",
       "  'distil',\n",
       "  'aerob',\n",
       "  'jodi',\n",
       "  'rosen',\n",
       "  'haganah',\n",
       "  'wright',\n",
       "  'hsv',\n",
       "  'ujelang',\n",
       "  '495',\n",
       "  'replay',\n",
       "  'farthest',\n",
       "  'classbas',\n",
       "  'tigran',\n",
       "  'fnumber',\n",
       "  'contamin',\n",
       "  'skinner',\n",
       "  'ppopul',\n",
       "  'mahabharata',\n",
       "  'highestpaid',\n",
       "  'mailbox',\n",
       "  'extortion',\n",
       "  'transcendent',\n",
       "  'hawkin',\n",
       "  'dakota',\n",
       "  'boundri',\n",
       "  '2037',\n",
       "  '2039',\n",
       "  'hyme',\n",
       "  'onehour',\n",
       "  'jericho',\n",
       "  'dpss',\n",
       "  'nail',\n",
       "  'rave',\n",
       "  'crossov',\n",
       "  'ethernet',\n",
       "  'acceptor',\n",
       "  'qin',\n",
       "  'wki',\n",
       "  '250',\n",
       "  '247',\n",
       "  'hightech',\n",
       "  'partyja',\n",
       "  'bdf',\n",
       "  'mislabel',\n",
       "  'hornet',\n",
       "  'bandwith',\n",
       "  'fallout',\n",
       "  'bikini',\n",
       "  'anticircumvent',\n",
       "  'motel',\n",
       "  'shevardnadz',\n",
       "  'politburo',\n",
       "  'moray',\n",
       "  'camel',\n",
       "  'folli',\n",
       "  'pickler',\n",
       "  '129',\n",
       "  'phonem',\n",
       "  'patholog',\n",
       "  'boon',\n",
       "  'cumberland',\n",
       "  'neurotic',\n",
       "  '4470',\n",
       "  'rewritten',\n",
       "  'pupilteach',\n",
       "  'duchesss',\n",
       "  'up',\n",
       "  'trajectori',\n",
       "  'unrestrict',\n",
       "  'lifeim',\n",
       "  'subclass',\n",
       "  'remembr',\n",
       "  'darnton',\n",
       "  'jurgen',\n",
       "  'ezana',\n",
       "  'heliocentr',\n",
       "  'bolus',\n",
       "  'dct',\n",
       "  'lapita',\n",
       "  'overwork',\n",
       "  'cinemascor',\n",
       "  'peta',\n",
       "  'angloirish',\n",
       "  'highdefinit',\n",
       "  'taglin',\n",
       "  '199495',\n",
       "  'frenchbritish',\n",
       "  'terahertz',\n",
       "  'maciel',\n",
       "  'matrix',\n",
       "  'reborn',\n",
       "  'tradeoff',\n",
       "  'longmenshan',\n",
       "  'carthaginian',\n",
       "  'tres',\n",
       "  'hombr',\n",
       "  'tantalum',\n",
       "  'cousin',\n",
       "  'horac',\n",
       "  'oxygenrich',\n",
       "  'treelin',\n",
       "  'berbic',\n",
       "  'skepi',\n",
       "  'shinar',\n",
       "  'lux',\n",
       "  'stamper',\n",
       "  '1920x1080p25',\n",
       "  'nigel',\n",
       "  'lythgo',\n",
       "  'od',\n",
       "  'goldman',\n",
       "  'sach',\n",
       "  'tufail',\n",
       "  'spawn',\n",
       "  'wader',\n",
       "  'furnish',\n",
       "  'silk',\n",
       "  'franchthi',\n",
       "  'wyatt',\n",
       "  'antagonist',\n",
       "  'andophilia',\n",
       "  'gynephilia',\n",
       "  'streamabl',\n",
       "  'nazaren',\n",
       "  'bristol',\n",
       "  'blenheim',\n",
       "  'f1',\n",
       "  'zhengd',\n",
       "  'sheep',\n",
       "  'imac',\n",
       "  'incendri',\n",
       "  'tokyo',\n",
       "  '910',\n",
       "  ...],\n",
       " ['kerosene',\n",
       "  'coal',\n",
       "  'octavian',\n",
       "  'ramsgate',\n",
       "  '577',\n",
       "  'commanders',\n",
       "  'rubbish',\n",
       "  'interception',\n",
       "  'caliber',\n",
       "  'schabas',\n",
       "  'angloboer',\n",
       "  'followers',\n",
       "  'sputnik',\n",
       "  'hickory',\n",
       "  'biochemicals',\n",
       "  'biotic',\n",
       "  'micro',\n",
       "  'tag',\n",
       "  'río',\n",
       "  'plata',\n",
       "  'juraschek',\n",
       "  'weeds',\n",
       "  'clustering',\n",
       "  'creationevolution',\n",
       "  'aboolian',\n",
       "  'charlie',\n",
       "  'chaplin',\n",
       "  'bbva',\n",
       "  'us143000',\n",
       "  'sufficient',\n",
       "  'philharmonic',\n",
       "  '256930',\n",
       "  'entropy',\n",
       "  'velbazhd',\n",
       "  'debussy',\n",
       "  'vegetables',\n",
       "  'grecoroman',\n",
       "  'gustaf',\n",
       "  'cutoff',\n",
       "  'realnetworks',\n",
       "  'napster',\n",
       "  'musicmatch',\n",
       "  'nonpolitical',\n",
       "  'warsay',\n",
       "  'yikaalo',\n",
       "  'browsing',\n",
       "  'bellevue',\n",
       "  'cnn',\n",
       "  'humane',\n",
       "  'aground',\n",
       "  'kennington',\n",
       "  'oval',\n",
       "  'semifinal',\n",
       "  'venue',\n",
       "  'longwood',\n",
       "  'pause',\n",
       "  'recitation',\n",
       "  'altitudinal',\n",
       "  'cordon',\n",
       "  'cessation',\n",
       "  'rhythmic',\n",
       "  'dakhini',\n",
       "  'orator',\n",
       "  'cicero',\n",
       "  'laid',\n",
       "  'troughs',\n",
       "  'bistro',\n",
       "  'phanerozoic',\n",
       "  'qasim',\n",
       "  'expanded',\n",
       "  '286',\n",
       "  'pavarotti',\n",
       "  'cranes',\n",
       "  'kayaks',\n",
       "  'footwear',\n",
       "  'hasmoneans',\n",
       "  'vichy',\n",
       "  'rudé',\n",
       "  'beiyang',\n",
       "  'gluconeogenesis',\n",
       "  'tsuibushi',\n",
       "  'se',\n",
       "  'massengill',\n",
       "  'tr808',\n",
       "  'unspayed',\n",
       "  'bureaucratic',\n",
       "  'superstate',\n",
       "  'illustrated',\n",
       "  'abstractly',\n",
       "  'multidimensionally',\n",
       "  'rhymed',\n",
       "  'consummated',\n",
       "  'respect',\n",
       "  'unstained',\n",
       "  'superinnocent',\n",
       "  'singularly',\n",
       "  'selfclarity',\n",
       "  'zant',\n",
       "  'ngari',\n",
       "  'wanhu',\n",
       "  'victim',\n",
       "  'poptanning',\n",
       "  'heraldic',\n",
       "  'colin',\n",
       "  '36000',\n",
       "  '1773',\n",
       "  'zeila',\n",
       "  'sclerites',\n",
       "  'aavikisms',\n",
       "  '1584',\n",
       "  'legend',\n",
       "  'enmetena',\n",
       "  'urukagina',\n",
       "  'thickwalled',\n",
       "  'urbanized',\n",
       "  'ambitions',\n",
       "  'multinational',\n",
       "  'hurdygurdy',\n",
       "  'jody',\n",
       "  'rosen',\n",
       "  'yegor',\n",
       "  'ligachev',\n",
       "  'eléonore',\n",
       "  'denuelle',\n",
       "  'plaigne',\n",
       "  'aircooled',\n",
       "  'archdiocese',\n",
       "  'abba',\n",
       "  'lerner',\n",
       "  'endemic',\n",
       "  'wroclaw',\n",
       "  'advocates',\n",
       "  'senioritybased',\n",
       "  'layoffs',\n",
       "  'upscale',\n",
       "  'nucleic',\n",
       "  'constructions',\n",
       "  'dated',\n",
       "  'fossils',\n",
       "  '345',\n",
       "  'prudhoe',\n",
       "  'comparsas',\n",
       "  'mothership',\n",
       "  'gamestop',\n",
       "  'colossus',\n",
       "  'rhodes',\n",
       "  'alfarabi',\n",
       "  'governmentorganised',\n",
       "  'paint',\n",
       "  '486',\n",
       "  'usenet',\n",
       "  'colocation',\n",
       "  'shore',\n",
       "  '1859',\n",
       "  'washing',\n",
       "  '1632',\n",
       "  'attdirectv',\n",
       "  'baccalaureate',\n",
       "  'haskalah',\n",
       "  'biotrophic',\n",
       "  'cessna',\n",
       "  'caravan',\n",
       "  'sheathing',\n",
       "  'phobias',\n",
       "  '4000',\n",
       "  'ftmin',\n",
       "  'employing',\n",
       "  'acidcompliant',\n",
       "  'rainier',\n",
       "  'millionyearold',\n",
       "  '347000',\n",
       "  '356000',\n",
       "  'breaching',\n",
       "  'distributors',\n",
       "  'zhu',\n",
       "  'yuanzhang',\n",
       "  'rahm',\n",
       "  'emanuel',\n",
       "  '732',\n",
       "  'pre1923',\n",
       "  '1677',\n",
       "  'virginally',\n",
       "  'conceived',\n",
       "  'commandant',\n",
       "  'uricotelic',\n",
       "  'miquelon',\n",
       "  'sura',\n",
       "  'frigate',\n",
       "  'dryness',\n",
       "  'aimed',\n",
       "  'journalism',\n",
       "  'rime',\n",
       "  'mariner',\n",
       "  'viagra',\n",
       "  'teichoic',\n",
       "  'acids',\n",
       "  'aseel',\n",
       "  'subcontinent',\n",
       "  'behaviour',\n",
       "  'highlands',\n",
       "  'hun',\n",
       "  'positivism',\n",
       "  'explain',\n",
       "  'participants',\n",
       "  'buses',\n",
       "  'georgios',\n",
       "  'amiroutzes',\n",
       "  'alcock',\n",
       "  'ballet',\n",
       "  'outsourcing',\n",
       "  'gilgamesh',\n",
       "  'alliances',\n",
       "  'macroscopic',\n",
       "  '784',\n",
       "  'fringe',\n",
       "  'extract',\n",
       "  'remnant',\n",
       "  'effra',\n",
       "  'topcoat',\n",
       "  'presumably',\n",
       "  '295',\n",
       "  'kang',\n",
       "  'youwei',\n",
       "  'curia',\n",
       "  'massscale',\n",
       "  'industrialization',\n",
       "  'runnerup',\n",
       "  'canonical',\n",
       "  'gospels',\n",
       "  'scheff',\n",
       "  'diamonds',\n",
       "  'antijapanese',\n",
       "  'wilson',\n",
       "  'zianon',\n",
       "  'pazniak',\n",
       "  'pt',\n",
       "  'mitra',\n",
       "  'adiperkasa',\n",
       "  'watched',\n",
       "  'meenas',\n",
       "  'microcontroller',\n",
       "  'dinapanah',\n",
       "  'citadel',\n",
       "  'soriano',\n",
       "  'ballhof',\n",
       "  'peristalsis',\n",
       "  'hannover',\n",
       "  'kerwin',\n",
       "  'cruises',\n",
       "  'banc',\n",
       "  'lockout',\n",
       "  'triangles',\n",
       "  '782',\n",
       "  'truly',\n",
       "  'compelling',\n",
       "  'chess',\n",
       "  'glory',\n",
       "  'naphtol',\n",
       "  'lithol',\n",
       "  'eightymillimeter',\n",
       "  'lin',\n",
       "  'hatfield',\n",
       "  'dodds',\n",
       "  'peck',\n",
       "  'surnames',\n",
       "  'hire',\n",
       "  'scribes',\n",
       "  'unbearable',\n",
       "  'unconscious',\n",
       "  '2644',\n",
       "  'proximity',\n",
       "  'quakes',\n",
       "  'pc',\n",
       "  'monitors',\n",
       "  'rangers',\n",
       "  'retracted',\n",
       "  'eastlake',\n",
       "  'latrunculi',\n",
       "  'warwick',\n",
       "  'autotrophic',\n",
       "  'say',\n",
       "  'congregationalists',\n",
       "  '10meter',\n",
       "  'greatgrandmother',\n",
       "  'hasnt',\n",
       "  'tested',\n",
       "  'yalu',\n",
       "  'quanzhou',\n",
       "  'zhangzhou',\n",
       "  'detritivory',\n",
       "  'federative',\n",
       "  'immediate',\n",
       "  'doctrina',\n",
       "  'christiana',\n",
       "  'letra',\n",
       "  'lengua',\n",
       "  'porifera',\n",
       "  'ctenophora',\n",
       "  'cnidaria',\n",
       "  'placozoa',\n",
       "  'hotmixed',\n",
       "  'capable',\n",
       "  'loads',\n",
       "  'phoenician',\n",
       "  'preserved',\n",
       "  'entrants',\n",
       "  '1402',\n",
       "  '1424',\n",
       "  'earlytomid19th',\n",
       "  'arne',\n",
       "  'frager',\n",
       "  'box',\n",
       "  'dense',\n",
       "  'woods',\n",
       "  'franchisees',\n",
       "  'brands',\n",
       "  'donalds',\n",
       "  'turnbull',\n",
       "  'crosssanagabioko',\n",
       "  'capsule',\n",
       "  'habsburgs',\n",
       "  'majjhima',\n",
       "  'nikaya',\n",
       "  'centred',\n",
       "  'humankind',\n",
       "  'nva',\n",
       "  'migrate',\n",
       "  'transalpine',\n",
       "  'sign',\n",
       "  'bilateral',\n",
       "  '110000',\n",
       "  'dogsled',\n",
       "  'hatton',\n",
       "  'accumulated',\n",
       "  'rajas',\n",
       "  'saboteurs',\n",
       "  'accession',\n",
       "  'sulfur',\n",
       "  'fluorine',\n",
       "  'durand',\n",
       "  'elmore',\n",
       "  'muddy',\n",
       "  'waters',\n",
       "  'howlin',\n",
       "  'peat',\n",
       "  'showers',\n",
       "  'gratitude',\n",
       "  'aruba',\n",
       "  'contextsensitive',\n",
       "  '16775',\n",
       "  '661',\n",
       "  'toshiba',\n",
       "  'smartcamp',\n",
       "  'zakaria',\n",
       "  'einstein',\n",
       "  'presbytery',\n",
       "  'nagle',\n",
       "  'blackheath',\n",
       "  'chalk',\n",
       "  'ngo',\n",
       "  'dinh',\n",
       "  'diem',\n",
       "  'leap',\n",
       "  '826',\n",
       "  'amundsen',\n",
       "  'carr',\n",
       "  'clash',\n",
       "  'opinions',\n",
       "  'romulus',\n",
       "  'multinucleated',\n",
       "  'camilo',\n",
       "  'castelo',\n",
       "  'branco',\n",
       "  'eça',\n",
       "  'queiroz',\n",
       "  'fernando',\n",
       "  'pessoa',\n",
       "  'sophia',\n",
       "  'mello',\n",
       "  'breyner',\n",
       "  'andresen',\n",
       "  'lobo',\n",
       "  'antunes',\n",
       "  'torga',\n",
       "  'ptolemaic',\n",
       "  'hyperresponsiveness',\n",
       "  'jasmine',\n",
       "  'bligh',\n",
       "  'forested',\n",
       "  'virumaa',\n",
       "  'lyrics',\n",
       "  'karaoke',\n",
       "  'lilibet',\n",
       "  'leontiy',\n",
       "  'stadtpark',\n",
       "  'mossad',\n",
       "  'regency',\n",
       "  'assistant',\n",
       "  'cadets',\n",
       "  'preaztec',\n",
       "  'domesticated',\n",
       "  'pursue',\n",
       "  'longstanding',\n",
       "  'policymilitary',\n",
       "  'shifts',\n",
       "  'procedure',\n",
       "  'prostitution',\n",
       "  'supervised',\n",
       "  'dissertation',\n",
       "  'willard',\n",
       "  'orman',\n",
       "  'quine',\n",
       "  'appraisal',\n",
       "  'shambhala',\n",
       "  'showcase',\n",
       "  'mbit',\n",
       "  'enlist',\n",
       "  'literal',\n",
       "  'rhythmically',\n",
       "  'holocene',\n",
       "  'epipaleolithic',\n",
       "  '1095',\n",
       "  'manchuria',\n",
       "  'kongens',\n",
       "  'nytorv',\n",
       "  'copenhagen',\n",
       "  'feodor',\n",
       "  'rostopchin',\n",
       "  'baikove',\n",
       "  'cemetery',\n",
       "  'parenting',\n",
       "  'comes',\n",
       "  '1484',\n",
       "  'ohmic',\n",
       "  'europeanamerican',\n",
       "  'worker',\n",
       "  'banishing',\n",
       "  'deposed',\n",
       "  '2239',\n",
       "  'refillable',\n",
       "  'galois',\n",
       "  'composted',\n",
       "  'turbines',\n",
       "  'atari',\n",
       "  'hrant',\n",
       "  'shahinyan',\n",
       "  'textiles',\n",
       "  'renegade',\n",
       "  'yorkino',\n",
       "  'lawton',\n",
       "  'enewetak',\n",
       "  'bikini',\n",
       "  'ethnoracial',\n",
       "  'thereby',\n",
       "  'eminem',\n",
       "  'mutinied',\n",
       "  'comuneros',\n",
       "  'theresa',\n",
       "  'maid',\n",
       "  'radius',\n",
       "  'vector',\n",
       "  'millimeters',\n",
       "  'sake',\n",
       "  '1271',\n",
       "  'bagan',\n",
       "  'mrauku',\n",
       "  'beaches',\n",
       "  'nabule',\n",
       "  'ngapali',\n",
       "  'ngwesaung',\n",
       "  'mergui',\n",
       "  '30237',\n",
       "  'milne',\n",
       "  'friendship',\n",
       "  'tanzania',\n",
       "  'synagogue',\n",
       "  'censorship',\n",
       "  'exact',\n",
       "  'judgemade',\n",
       "  '1664',\n",
       "  'séléka',\n",
       "  'planctomycetes',\n",
       "  'poribacteria',\n",
       "  'russianukrainian',\n",
       "  'hacking',\n",
       "  'loaning',\n",
       "  '03',\n",
       "  'halloween',\n",
       "  'delusion',\n",
       "  'annexed',\n",
       "  'malian',\n",
       "  'conjugative',\n",
       "  'plasmid',\n",
       "  'integrates',\n",
       "  'protectionism',\n",
       "  'mivart',\n",
       "  'lifted',\n",
       "  '28th',\n",
       "  'evergreens',\n",
       "  'oaks',\n",
       "  'closeness',\n",
       "  'helvetica',\n",
       "  'runic',\n",
       "  'femalefemale',\n",
       "  'malemale',\n",
       "  'malefemale',\n",
       "  'penetration',\n",
       "  'arise',\n",
       "  'exports',\n",
       "  '435000',\n",
       "  'euro',\n",
       "  'jump',\n",
       "  'suddhodana',\n",
       "  'catalonia',\n",
       "  'basque',\n",
       "  'entrails',\n",
       "  'bosnians',\n",
       "  'dancing',\n",
       "  'aims',\n",
       "  'overthrow',\n",
       "  'tajiks',\n",
       "  '1315',\n",
       "  '13691',\n",
       "  'shamsher',\n",
       "  'jang',\n",
       "  'bahadur',\n",
       "  'rana',\n",
       "  'staples',\n",
       "  'mill',\n",
       "  'alpha',\n",
       "  'ministerpräsident',\n",
       "  'already',\n",
       "  'fourthlargest',\n",
       "  'cladistics',\n",
       "  'assumes',\n",
       "  'branching',\n",
       "  'tribhuwan',\n",
       "  'missionaries',\n",
       "  'lausanne',\n",
       "  'flips',\n",
       "  'twists',\n",
       "  'sprung',\n",
       "  'tumbling',\n",
       "  'prestige',\n",
       "  'poisoned',\n",
       "  'colen',\n",
       "  'campbell',\n",
       "  'sikhism',\n",
       "  'hawksmoor',\n",
       "  'preparatory',\n",
       "  'negotiation',\n",
       "  'drafting',\n",
       "  'sussex',\n",
       "  'namibia',\n",
       "  'kristang',\n",
       "  'creoles',\n",
       "  'malaysia',\n",
       "  'bottleneck',\n",
       "  'flathead',\n",
       "  'mohametan',\n",
       "  'gasparri',\n",
       "  'exists',\n",
       "  'meadows',\n",
       "  'tuesday',\n",
       "  'olusegun',\n",
       "  'obasanjo',\n",
       "  'marperger',\n",
       "  'curieuses',\n",
       "  'natur',\n",
       "  'kunst',\n",
       "  'berg',\n",
       "  'gewerkund',\n",
       "  'handlungslexicon',\n",
       "  'shibell',\n",
       "  'kampong',\n",
       "  'fieldhockey',\n",
       "  'squash',\n",
       "  'jeu',\n",
       "  'boules',\n",
       "  'talaat',\n",
       "  'harb',\n",
       "  'siad',\n",
       "  'barre',\n",
       "  'hit',\n",
       "  'velocity',\n",
       "  'pinter',\n",
       "  '860',\n",
       "  'timely',\n",
       "  'bisexual',\n",
       "  'congenitally',\n",
       "  '101787',\n",
       "  'menace',\n",
       "  'fiorello',\n",
       "  'guardia',\n",
       "  'ejes',\n",
       "  'viales',\n",
       "  'sensation',\n",
       "  'seeking',\n",
       "  'salvadoran',\n",
       "  'fivefold',\n",
       "  '94000',\n",
       "  '465000',\n",
       "  '483',\n",
       "  'shrine',\n",
       "  'jersey',\n",
       "  'pragmatic',\n",
       "  'supernatural',\n",
       "  'yom',\n",
       "  'kippur',\n",
       "  'javaborneo',\n",
       "  '0400',\n",
       "  'germaine',\n",
       "  'foix',\n",
       "  'dodge',\n",
       "  'whichever',\n",
       "  'revelation',\n",
       "  'ceasing',\n",
       "  'nerthus',\n",
       "  'freyr',\n",
       "  'patron',\n",
       "  'alma',\n",
       "  'hoddle',\n",
       "  'qualified',\n",
       "  'defecate',\n",
       "  'gutters',\n",
       "  'bushes',\n",
       "  'mcnamara',\n",
       "  'censored',\n",
       "  'cooling',\n",
       "  'permissible',\n",
       "  'overheating',\n",
       "  'sixty',\n",
       "  'summits',\n",
       "  'nightingales',\n",
       "  '2144',\n",
       "  'alberts',\n",
       "  'sixties',\n",
       "  'brescia',\n",
       "  'lung',\n",
       "  'silica',\n",
       "  'zn22',\n",
       "  'mcms',\n",
       "  'annealing',\n",
       "  '20thcentury',\n",
       "  'positioned',\n",
       "  'farther',\n",
       "  'hudson',\n",
       "  'septuagint',\n",
       "  'plantation',\n",
       "  'resisting',\n",
       "  'kemari',\n",
       "  'finds',\n",
       "  'reveal',\n",
       "  'defendant',\n",
       "  'willfully',\n",
       "  'infringed',\n",
       "  'vital',\n",
       "  'kingsbridge',\n",
       "  'armory',\n",
       "  'garfield',\n",
       "  'waldorf',\n",
       "  'astoria',\n",
       "  'passing',\n",
       "  'hunyadi',\n",
       "  'expectations',\n",
       "  'proved',\n",
       "  'feeding',\n",
       "  'qianlongs',\n",
       "  'proficient',\n",
       "  '1624',\n",
       "  '001',\n",
       "  '025',\n",
       "  'brantford',\n",
       "  'c64',\n",
       "  'critic',\n",
       "  'employs',\n",
       "  '366',\n",
       "  'creative',\n",
       "  'tactical',\n",
       "  '12117',\n",
       "  'ethiopians',\n",
       "  'descended',\n",
       "  'subspecies',\n",
       "  'ezra',\n",
       "  'rhoades',\n",
       "  'activists',\n",
       "  'rietveld',\n",
       "  'schröder',\n",
       "  'listed',\n",
       "  'unescos',\n",
       "  'soprano',\n",
       "  'callas',\n",
       "  'fedor',\n",
       "  'emelianenko',\n",
       "  'acquired',\n",
       "  'chaldean',\n",
       "  'mullen',\n",
       "  'agrichemical',\n",
       "  'ambassadors',\n",
       "  'statisitics',\n",
       "  'adele',\n",
       "  'adelic',\n",
       "  'algebraic',\n",
       "  '590',\n",
       "  'szechuan',\n",
       "  'alicia',\n",
       "  'keys',\n",
       "  '109358',\n",
       "  'pansomalism',\n",
       "  'jeffersonian',\n",
       "  'occurring',\n",
       "  'pottery',\n",
       "  'orestes',\n",
       "  'elisha',\n",
       "  'walker',\n",
       "  'rumsey',\n",
       "  'cockney',\n",
       "  'haar',\n",
       "  '900000',\n",
       "  'hotter',\n",
       "  'moreefficient',\n",
       "  'evaporate',\n",
       "  'bôcher',\n",
       "  'a3',\n",
       "  'sexton',\n",
       "  'brittany',\n",
       "  'ferries',\n",
       "  'sweet',\n",
       "  'oodua',\n",
       "  'origination',\n",
       "  'indicative',\n",
       "  'eliot',\n",
       "  'typeb',\n",
       "  'vary',\n",
       "  'hierarchical',\n",
       "  'pembrokeshire',\n",
       "  'nonsense',\n",
       "  'bmx',\n",
       "  'championshipcaliber',\n",
       "  'heqin',\n",
       "  'okc',\n",
       "  'indigo',\n",
       "  'dye',\n",
       "  'rental',\n",
       "  'tenants',\n",
       "  'highvalueadded',\n",
       "  'gifford',\n",
       "  '19321934',\n",
       "  'vendôme',\n",
       "  'nimbus',\n",
       "  '1035',\n",
       "  '1479',\n",
       "  'deutschösterreich',\n",
       "  'interrupted',\n",
       "  'jukeboxes',\n",
       "  'pollination',\n",
       "  'syndromes',\n",
       "  'guthrie',\n",
       "  'ge',\n",
       "  'sans',\n",
       "  'serif',\n",
       "  'bold',\n",
       "  'phenomena',\n",
       "  'vowel',\n",
       "  'nelly',\n",
       "  'sachs',\n",
       "  '36695',\n",
       "  '426',\n",
       "  'bellboy',\n",
       "  'gonads',\n",
       "  'abdomen',\n",
       "  '163',\n",
       "  'taiseer',\n",
       "  'elias',\n",
       "  'turns',\n",
       "  'archeologists',\n",
       "  'prefecture',\n",
       "  'typewriter',\n",
       "  'christoph',\n",
       "  'cellarius',\n",
       "  'chunking',\n",
       "  'thermonuclear',\n",
       "  'eric',\n",
       "  'voegelin',\n",
       "  'archaeopteryx',\n",
       "  'fanlike',\n",
       "  'holding',\n",
       "  '190',\n",
       "  'rotor',\n",
       "  '54000',\n",
       "  'frontiers',\n",
       "  'remaining',\n",
       "  'lose',\n",
       "  'gilder',\n",
       "  'gregorio',\n",
       "  'marañón',\n",
       "  'supplementation',\n",
       "  'tyburn',\n",
       "  'possibility',\n",
       "  'barnstormers',\n",
       "  'transformations',\n",
       "  'vcr',\n",
       "  'materialistic',\n",
       "  'cārvāka',\n",
       "  'pest',\n",
       "  'browns',\n",
       "  'quadraphonic',\n",
       "  'hannes',\n",
       "  'vertically',\n",
       "  'cartooning',\n",
       "  'halifax',\n",
       "  'arrian',\n",
       "  'royalties',\n",
       "  'xinjiekou',\n",
       "  'chancellor',\n",
       "  'bošnjački',\n",
       "  'embankment',\n",
       "  'gotoba',\n",
       "  '5100',\n",
       "  'rap',\n",
       "  'pv',\n",
       "  'ischaemic',\n",
       "  'cerebrovascular',\n",
       "  'legislativa',\n",
       "  '242',\n",
       "  'nagabhata',\n",
       "  '1754',\n",
       "  'colorblind',\n",
       "  'longerterm',\n",
       "  'refinancing',\n",
       "  'burst',\n",
       "  'ranthambore',\n",
       "  'bihar',\n",
       "  'e1b1b1a',\n",
       "  'ockham',\n",
       "  'saif',\n",
       "  'alislam',\n",
       "  '32nd',\n",
       "  'еврей',\n",
       "  'yevrey',\n",
       "  'lmw',\n",
       "  'aljurjani',\n",
       "  'righteous',\n",
       "  'harmonious',\n",
       "  'kelloggs',\n",
       "  'poptarts',\n",
       "  'qin',\n",
       "  'paynes',\n",
       "  'nieuwland',\n",
       "  'teresa',\n",
       "  'herrera',\n",
       "  'cease',\n",
       "  'fferyllydd',\n",
       "  'towerlike',\n",
       "  'upgrades',\n",
       "  'janissary',\n",
       "  'late17thcentury',\n",
       "  '1450',\n",
       "  'investigate',\n",
       "  'shifted',\n",
       "  'ding',\n",
       "  'transistors',\n",
       "  'grammarians',\n",
       "  'fractional',\n",
       "  'monasteries',\n",
       "  'rightly',\n",
       "  'odyssey',\n",
       "  'farmer',\n",
       "  'spots',\n",
       "  'mold',\n",
       "  'burned',\n",
       "  'speckling',\n",
       "  'ethniki',\n",
       "  'deyang',\n",
       "  'dubliners',\n",
       "  'passerines',\n",
       "  'ellen',\n",
       "  'sirleaf',\n",
       "  'nonscientific',\n",
       "  'input',\n",
       "  'scion',\n",
       "  'gamertag',\n",
       "  'xboxcom',\n",
       "  'michiru',\n",
       "  'ōshima',\n",
       "  '1492',\n",
       "  '68th',\n",
       "  'mesenteron',\n",
       "  'innsbrook',\n",
       "  'relief',\n",
       "  'boeing',\n",
       "  'dissident',\n",
       "  'anatomists',\n",
       "  'matrix',\n",
       "  'λb',\n",
       "  'nonnegative',\n",
       "  'matrices',\n",
       "  '1814',\n",
       "  '2910',\n",
       "  'scratches',\n",
       "  'compromise',\n",
       "  '83085',\n",
       "  'crores',\n",
       "  'inventors',\n",
       "  'cdj',\n",
       "  'timeencoded',\n",
       "  'seeks',\n",
       "  'traced',\n",
       "  'rmb',\n",
       "  'wire',\n",
       "  'kievs',\n",
       "  'khreschatyk',\n",
       "  'trajans',\n",
       "  'aacm4a',\n",
       "  'aiff',\n",
       "  'wav',\n",
       "  'audible',\n",
       "  'audiobook',\n",
       "  '684',\n",
       "  'griffin',\n",
       "  'hewitt',\n",
       "  'suchocki',\n",
       "  'mesle',\n",
       "  'faber',\n",
       "  'certificate',\n",
       "  'chromosomes',\n",
       "  '17704132',\n",
       "  '20013',\n",
       "  'tamagotchi',\n",
       "  'giga',\n",
       "  'toys',\n",
       "  'vereide',\n",
       "  'empiricism',\n",
       "  '1614',\n",
       "  'brittle',\n",
       "  'paralogisms',\n",
       "  'hunan',\n",
       "  '105000',\n",
       "  'disputed',\n",
       "  'reinforcements',\n",
       "  '90th',\n",
       "  'communityowned',\n",
       "  'recessed',\n",
       "  'tynwald',\n",
       "  'invader',\n",
       "  'gigabits',\n",
       "  'sunrise',\n",
       "  'chasetown',\n",
       "  'banjara',\n",
       "  'metz',\n",
       "  'cruelty',\n",
       "  'aspca',\n",
       "  'mbs',\n",
       "  'patriot',\n",
       "  'trent',\n",
       "  'controversial',\n",
       "  'formally',\n",
       "  'recession',\n",
       "  'wilberforce',\n",
       "  'lessons',\n",
       "  'wtf',\n",
       "  '90000',\n",
       "  'mrauk',\n",
       "  'eccm',\n",
       "  'mormon',\n",
       "  'polygamy',\n",
       "  'icy',\n",
       "  'sedan',\n",
       "  'phoenix',\n",
       "  'anthrozoology',\n",
       "  'estonias',\n",
       "  'kinect',\n",
       "  'uighur',\n",
       "  'codified',\n",
       "  'statutes',\n",
       "  'coordinate',\n",
       "  'supplementum',\n",
       "  'hellenisticum',\n",
       "  'naiad',\n",
       "  'michel',\n",
       "  'djotodia',\n",
       "  'jianwen',\n",
       "  'bradbury',\n",
       "  'glasgow',\n",
       "  'ceramics',\n",
       "  'ancestors',\n",
       "  'iodine',\n",
       "  'promagistrate',\n",
       "  'chattanooga',\n",
       "  'pressurised',\n",
       "  'linearity',\n",
       "  'manufacture',\n",
       "  'violation',\n",
       "  '810',\n",
       "  'cavity',\n",
       "  '724',\n",
       "  'alpes',\n",
       "  'echiura',\n",
       "  'sipuncula',\n",
       "  'ntoltso',\n",
       "  ...])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_outliers(Q_vocab, outlier_threshold+1), get_outliers(A_vocab, outlier_threshold+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e988636e-1d1c-4b82-b62c-8e31a7f3c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_outliers, a_outliers = get_outliers(Q_vocab,outlier_threshold+1), get_outliers(A_vocab,outlier_threshold+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2c4425b0-1e02-4111-9585-46a2bcbd18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_final = remove_least_common(train_df, cols_tokens, [q_outliers, a_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e56fe3a5-5df6-4bdf-9dc9-29f4150d236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_final = remove_least_common(test_df, cols_tokens, [q_outliers, a_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8b6aa251-97ee-4ca6-ae09-70e06de4fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in column Question_tokens:\n",
      "\t         mean: 6.06\n",
      "\t         median: 6.00\n",
      "\t         minimum: 1\n",
      "\t         maximum: 19)\n",
      "Sentences in column Answer_tokens:\n",
      "\t         mean: 1.90\n",
      "\t         median: 1.00\n",
      "\t         minimum: 0\n",
      "\t         maximum: 17)\n"
     ]
    }
   ],
   "source": [
    "# tokenized & least common removed\n",
    "sentences_stats(train_df_final, cols_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958f63b-66c0-4232-859e-b35c55d32dea",
   "metadata": {},
   "source": [
    "# remove questions that have less than three words and answers that have less than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd923454-7371-4b87-9c69-aff075b52dbe",
   "metadata": {},
   "source": [
    "\n",
    "## Remove long outliers: long sentences that occure rarely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "64668e14-78d8-488c-8741-f9c1068568f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_final = filter_sentences(train_df_final, cols_tokens, [2,0], condition='longer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "996649db-3b9e-4a0c-98b6-d12cf6cc745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question_tokens 3\n",
      "Answer_tokens 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df_final_thresholds = histograms(train_df_final, cols_tokens, name = 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "dbbcbdfc-4417-411b-b17a-1e3a4f6937bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14416, 4)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9ca6daee-6325-4e48-bc3d-7e1858ce8450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in column Question_tokens:\n",
      "\t         mean: 6.17\n",
      "\t         median: 6.00\n",
      "\t         minimum: 3\n",
      "\t         maximum: 19)\n",
      "Sentences in column Answer_tokens:\n",
      "\t         mean: 2.25\n",
      "\t         median: 2.00\n",
      "\t         minimum: 1\n",
      "\t         maximum: 17)\n"
     ]
    }
   ],
   "source": [
    "# shortest sentences removed\n",
    "sentences_stats(train_df_final, cols_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d5d9d4b5-78f5-4417-bb1c-e837992ae875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only the 95% of the data\n",
    "\n",
    "cutoff = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "cde7c06f-cf1e-47c2-aa56-65809a53ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question_tokens': 10, 'Answer_tokens': 6}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeping only the 95% of the data\n",
    "\n",
    "get_thresholds(train_df_final_thresholds, cutoff = cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "b3dec7d1-3cdf-481e-a53a-8c334bc3fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_MAX, A_MAX = get_thresholds(train_df_final_thresholds, cutoff = cutoff).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "39553408-5871-4f2f-9778-f2f5bd0b0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_final = filter_sentences(train_df_final, cols_tokens, [Q_MAX+1,A_MAX+1], condition='shorter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3585bff9-ae72-4536-9d18-1eb7f507be5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in column Question_tokens:\n",
      "\t         mean: 5.99\n",
      "\t         median: 6.00\n",
      "\t         minimum: 3\n",
      "\t         maximum: 10)\n",
      "Sentences in column Answer_tokens:\n",
      "\t         mean: 1.97\n",
      "\t         median: 2.00\n",
      "\t         minimum: 1\n",
      "\t         maximum: 6)\n"
     ]
    }
   ],
   "source": [
    "# long outliers removed\n",
    "sentences_stats(train_df_final, cols_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "5dbbf6d5-1c20-4ac9-8910-986fe457123e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13391, 4)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee608e-e8b8-49b6-85fb-6c2e370135da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "269601d3-8ef4-423f-9dd1-45ad01bcb8a1",
   "metadata": {},
   "source": [
    "# Must make pairs from the dataset with removed short and long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "53174fa0-b463-43b5-829e-f147b7f5a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs_final = get_pairs_from_df(train_df_final, cols_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f23a364e-b19c-4453-a493-c6464391bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs_final = get_pairs_from_df(test_df_final, cols_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "faf05038-054f-482c-ac82-9c764ee6e5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13391, 2114)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pairs_final), len(test_pairs_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e4513-494a-43ec-983f-8165c388336a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2444ab25-2074-489c-991e-a57cea7e0fbc",
   "metadata": {},
   "source": [
    "# building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61581914-4772-4a0f-8010-b2c75126dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from modules.models import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cc8a6-632f-4c82-a8f9-ec619f451162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2f32432a-42cd-4a28-9d69-2d405401b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(vocab, tokens, seq_len, device):\n",
    "    '''Converts a tokenized sentence into a tensor of indices of a given length.\n",
    "    If too short, it uses padding at the beginning of the sentence as suggested by the mentor.'''\n",
    "    \n",
    "    tokens = [t for t in tokens if t in vocab.word2count.keys()]\n",
    "    \n",
    "    padded = [vocab.word2index['PAD']] * (seq_len-len(tokens)) + [vocab.word2index['SOS']] + [vocab.word2index[t] for t in tokens] + [vocab.word2index['EOS']]\n",
    "\n",
    "    #print(len(padded))\n",
    "    \n",
    "    tensor = torch.Tensor(padded).long().to(device).view(-1,1)\n",
    "    \n",
    "    #print(tensor.shape)\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7285fa-ec3e-4f66-817b-c27d18cecf50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "edaf9954-96f6-4726-8f10-0d644084a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    # We initialize the Encoder object with appropriate layers\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, embedding_size):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        #self.hidden = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size).to(device)\n",
    "        \n",
    "        # The LSTM is our last cell because it produces the hidden state        \n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, 1).to(device)\n",
    "    \n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = x.view(1, 1, -1)\n",
    "        \n",
    "        #x = x.view(x.shape[0], 1, -1)\n",
    "        \n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        return x, hidden, cell_state\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    # The Decoder is initialized in the same manner.\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, embedding_size):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size)\n",
    "        \n",
    "        # The LSTM produces an output by passing the hidden state to the   Linear layer\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim= 1)     \n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        x = self.softmax(self.fc(x[0]))\n",
    "        return x, hidden, cell_state\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    #def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, output_size):    \n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = Encoder(self.input_size, self.hidden_size, self.embedding_size).to(device)\n",
    "        self.decoder = Decoder(self.hidden_size, self.output_size, self.embedding_size).to(device)\n",
    "        #self.device = device\n",
    "        \n",
    "    def forward(self, src_batch: torch.LongTensor, trg_batch: torch.LongTensor, src_len, trg_len, teacher_forcing_ratio: float = 0.5):\n",
    "        \n",
    "        max_len, batch_size = trg_batch.shape\n",
    "                \n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device) #.to(self.device) \n",
    "\n",
    "         # initialize hidden and cell state\n",
    "        encoder_hidden = torch.zeros([1, 1, self.hidden_size]).to(device) \n",
    "        cell_state = torch.zeros([1, 1, self.hidden_size]).to(device)\n",
    "\n",
    "        for i in range(src_len):\n",
    "        \n",
    "            # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "            _, hidden, cell = self.encoder(src_batch[i], encoder_hidden, cell_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "        trg = trg_batch[0]\n",
    "        \n",
    "        for i in range(trg_len):\n",
    "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "            outputs[i] = prediction\n",
    "            \n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                trg = trg_batch[i]\n",
    "            else:\n",
    "                trg = prediction.argmax(1)\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8151e-ab72-4b6a-b3ce-074ecd48302e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "034cea43-8428-458c-b354-c939d85dbd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "1fa05580-3539-45d8-82c8-f2db40e4eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "87823425-7091-493f-a4a1-9f292aa56af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(input_size=Q_vocab.n_words, hidden_size=hidden_size, embedding_size=embedding_dim, output_size=A_vocab.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "24a0d67c-200c-4349-a172-d35dd9d6a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = seq2seq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6216d45-7339-4bca-be35-7903b348b12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2710aacf-cfa1-4193-a300-2efa21b8989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "25f62c5e-1c44-44b6-a7df-b33dd67e43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3bf76d06-3d6c-4311-86c6-36d3307bbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(seq2seq.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=0).to(device) # 0 is padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9584cf05-26eb-4ebb-9aa2-3f9cc6349248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, pairs, optimizer, criterion, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for pair in pairs:\n",
    "        \n",
    "        src = pair.question\n",
    "        tgt = pair.answer\n",
    "        \n",
    "        src_tensor = to_tensor(vocab=Q_vocab, tokens=src, seq_len=Q_MAX, device=device)#.to(device) #.unsqueeze(0)\n",
    "        tgt_tensor = to_tensor(vocab=A_vocab, tokens=tgt, seq_len=A_MAX, device=device)#.to(device) #.unsqueeze(0)\n",
    "\n",
    "        # print(src_tensor.shape, tgt_tensor.shape)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src_tensor, tgt_tensor, src_len=src_tensor.size(0), trg_len=tgt_tensor.size(0), teacher_forcing_ratio=1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_tensor.view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        count += 1 \n",
    "    \n",
    "        if count % 10 == 0:\n",
    "            print(f'Loss {total_loss/count}')\n",
    "    \n",
    "    return total_loss / len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806cb39-bb7d-493a-afda-234c650b4812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f4714c4-3f60-4a8f-8e25-466c8c6a9bbf",
   "metadata": {},
   "source": [
    "test after each epoch\n",
    "\n",
    "use minibatches (e.g. 16)\n",
    "\n",
    "either count pairs and only update loss/optimizer after n pairs\n",
    " or\n",
    "use bucket iterator\n",
    "\n",
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "https://github.com/yunjey/seq2seq-dataloader/blob/master/data_loader.py\n",
    "\n",
    "learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bc591-bb75-42f4-872b-49eade518bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fea572-6091-4b62-bf25-84394954b044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "d349d25f-d960-4506-a205-5ec1ea1f40ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 3.817245101928711\n",
      "Loss 3.415850853919983\n",
      "Loss 3.380204200744629\n",
      "Loss 3.1860016882419586\n",
      "Loss 3.1794493341445924\n",
      "Loss 3.23957728544871\n",
      "Loss 3.3607630491256715\n",
      "Loss 3.382536855340004\n",
      "Loss 3.2930691798528033\n",
      "Loss 3.31447012424469\n",
      "Loss 3.270938916639848\n",
      "Loss 3.2254539946715037\n",
      "Loss 3.3397824104015643\n",
      "Loss 3.302507024151938\n",
      "Loss 3.2698235082626343\n",
      "Loss 3.2759212017059327\n",
      "Loss 3.354841308032765\n",
      "Loss 3.4090925322638617\n",
      "Loss 3.4273820312399614\n",
      "Loss 3.383825205564499\n",
      "Loss 3.356225958324614\n",
      "Loss 3.4012522372332485\n",
      "Loss 3.3926990747451784\n",
      "Loss 3.4021784524122873\n",
      "Loss 3.408015480995178\n",
      "Loss 3.4167467089799732\n",
      "Loss 3.4302720334794787\n",
      "Loss 3.435304928677423\n",
      "Loss 3.4221851217335666\n",
      "Loss 3.451386038462321\n",
      "Loss 3.4730906225019886\n",
      "Loss 3.4811643831431867\n",
      "Loss 3.479972798174078\n",
      "Loss 3.4630424843114964\n",
      "Loss 3.4654397528512138\n",
      "Loss 3.4772454215420616\n",
      "Loss 3.454471899689855\n",
      "Loss 3.464203120846497\n",
      "Loss 3.4496308360344323\n",
      "Loss 3.4419856432080267\n",
      "Loss 3.45276402758389\n",
      "Loss 3.4676883206481026\n",
      "Loss 3.460332447706267\n",
      "Loss 3.4727465225891634\n",
      "Loss 3.4787832713127136\n",
      "Loss 3.464632134333901\n",
      "Loss 3.4641017685545252\n",
      "Loss 3.4454057914515337\n",
      "Loss 3.4414411644546354\n",
      "Loss 3.4416101162433623\n",
      "Loss 3.4505330709850086\n",
      "Loss 3.44319388453777\n",
      "Loss 3.4521141929446526\n",
      "Loss 3.4554754833380383\n",
      "Loss 3.45142198714343\n",
      "Loss 3.440374660704817\n",
      "Loss 3.4435778799809906\n",
      "Loss 3.4559694068185216\n",
      "Loss 3.4449253765203185\n",
      "Loss 3.4386493744452795\n",
      "Loss 3.4330327106303855\n",
      "Loss 3.4298135638237\n",
      "Loss 3.4217329661051434\n",
      "Loss 3.4225144051015377\n",
      "Loss 3.4209024306444022\n",
      "Loss 3.4152756080482947\n",
      "Loss 3.4077304352575273\n",
      "Loss 3.4174380176207597\n",
      "Loss 3.432543831631757\n",
      "Loss 3.437742074217115\n",
      "Loss 3.430178242837879\n",
      "Loss 3.4229875069525506\n",
      "Loss 3.407957889772441\n",
      "Loss 3.4064322616602922\n",
      "Loss 3.4071250298817954\n",
      "Loss 3.4049154383571523\n",
      "Loss 3.3966196711961327\n",
      "Loss 3.3936206828325224\n",
      "Loss 3.386095365089706\n",
      "Loss 3.3952954280376435\n",
      "Loss 3.3959031773202213\n",
      "Loss 3.3896118669975093\n",
      "Loss 3.3835765475250152\n",
      "Loss 3.380706277773494\n",
      "Loss 3.379623636778663\n",
      "Loss 3.381739782039509\n",
      "Loss 3.3829650909051128\n",
      "Loss 3.372348026118495\n",
      "Loss 3.3637929973977334\n",
      "Loss 3.368773693773482\n",
      "Loss 3.372312847467569\n",
      "Loss 3.388797702607901\n",
      "Loss 3.380031628890704\n",
      "Loss 3.379840072418781\n",
      "Loss 3.3755383289487737\n",
      "Loss 3.3698781659205754\n",
      "Loss 3.369026317055692\n",
      "Loss 3.3658186520848954\n",
      "Loss 3.36824834021655\n",
      "Loss 3.3672310301065447\n",
      "Loss 3.3704362484488155\n",
      "Loss 3.371312997271033\n",
      "Loss 3.3633630625252584\n",
      "Loss 3.3538329256268646\n",
      "Loss 3.345865479537419\n",
      "Loss 3.3430902131323545\n",
      "Loss 3.3394699777398156\n",
      "Loss 3.3421995284380737\n",
      "Loss 3.338717458882463\n",
      "Loss 3.3389107262004507\n",
      "Loss 3.3353782138308965\n",
      "Loss 3.3296075658074447\n",
      "Loss 3.3364111341206373\n",
      "Loss 3.335230758524778\n",
      "Loss 3.3407920197818592\n",
      "Loss 3.3492789680587833\n",
      "Loss 3.339490375559554\n",
      "Loss 3.3407705000901626\n",
      "Loss 3.338870829393884\n",
      "Loss 3.3401074209809303\n",
      "Loss 3.340634742945679\n",
      "Loss 3.338239100722016\n",
      "Loss 3.3393420306647696\n",
      "Loss 3.3391397046466027\n",
      "Loss 3.338642563533783\n",
      "Loss 3.339023679211026\n",
      "Loss 3.34491475998886\n",
      "Loss 3.344898998737335\n",
      "Loss 3.345367082976556\n",
      "Loss 3.348827412586946\n",
      "Loss 3.3477798299025032\n",
      "Loss 3.347898219390349\n",
      "Loss 3.350835162266753\n",
      "Loss 3.344302385511683\n",
      "Loss 3.3412348937988283\n",
      "Loss 3.3360697087996147\n",
      "Loss 3.332853131746724\n",
      "Loss 3.3274366926455845\n",
      "Loss 3.3264731368572593\n",
      "Loss 3.3184193691185544\n",
      "Loss 3.322297903831969\n",
      "Loss 3.3173907825644586\n",
      "Loss 3.325368526432064\n",
      "Loss 3.3210885237488483\n",
      "Loss 3.3213245633552813\n",
      "Loss 3.314305247999217\n",
      "Loss 3.3113871292192107\n",
      "Loss 3.308466367866542\n",
      "Loss 3.3096308004936117\n",
      "Loss 3.321161348025004\n",
      "Loss 3.3215796004067983\n",
      "Loss 3.3157868553149075\n",
      "Loss 3.313582572282529\n",
      "Loss 3.3096577380384717\n",
      "Loss 3.303187566880257\n",
      "Loss 3.301267515390347\n",
      "Loss 3.297596041336181\n",
      "Loss 3.294011089620711\n",
      "Loss 3.292226282980457\n",
      "Loss 3.2902927581220864\n",
      "Loss 3.2930308795863796\n",
      "Loss 3.289350625099959\n",
      "Loss 3.289292097896155\n",
      "Loss 3.2866672507873393\n",
      "Loss 3.283126486937205\n",
      "Loss 3.282107274575406\n",
      "Loss 3.2798748737323784\n",
      "Loss 3.278362537636643\n",
      "Loss 3.272497252105961\n",
      "Loss 3.26671046369216\n",
      "Loss 3.261493763101031\n",
      "Loss 3.2540519750395487\n",
      "Loss 3.254897232482888\n",
      "Loss 3.2532293842441735\n",
      "Loss 3.2492783448355538\n",
      "Loss 3.2422956192357972\n",
      "Loss 3.246362308526443\n",
      "Loss 3.2479681372642517\n",
      "Loss 3.244889494293895\n",
      "Loss 3.240226211481624\n",
      "Loss 3.234087312880142\n",
      "Loss 3.229925216292287\n",
      "Loss 3.2285836120120814\n",
      "Loss 3.226990825909635\n",
      "Loss 3.225464936527046\n",
      "Loss 3.2239291026387162\n",
      "Loss 3.2263836456492623\n",
      "Loss 3.2289678372601247\n",
      "Loss 3.2303898437313303\n",
      "Loss 3.228258062475606\n",
      "Loss 3.2290137925073115\n",
      "Loss 3.2285110082477333\n",
      "Loss 3.2264186381058373\n",
      "Loss 3.228498822020501\n",
      "Loss 3.2279092310636472\n",
      "Loss 3.225698950278516\n",
      "Loss 3.224239515350555\n",
      "Loss 3.2206663529680233\n",
      "Loss 3.218448997622159\n",
      "Loss 3.2208685747385024\n",
      "Loss 3.2208631866606905\n",
      "Loss 3.220356767956573\n",
      "Loss 3.219591864752652\n",
      "Loss 3.218260669532944\n",
      "Loss 3.218262282697166\n",
      "Loss 3.217771069864625\n",
      "Loss 3.216065091499384\n",
      "Loss 3.2146358125484906\n",
      "Loss 3.21125897146298\n",
      "Loss 3.214169883330663\n",
      "Loss 3.2184347821637918\n",
      "Loss 3.21297006770125\n",
      "Loss 3.2070650671569396\n",
      "Loss 3.206470059568637\n",
      "Loss 3.2044374118294825\n",
      "Loss 3.2029223680496215\n",
      "Loss 3.204129517517881\n",
      "Loss 3.1994849586158716\n",
      "Loss 3.198264378989668\n",
      "Loss 3.19751026857983\n",
      "Loss 3.194923957602471\n",
      "Loss 3.1957792945273287\n",
      "Loss 3.1950580674436595\n",
      "Loss 3.194293773759689\n",
      "Loss 3.1923829596307542\n",
      "Loss 3.193656361314048\n",
      "Loss 3.191842220113141\n",
      "Loss 3.197085625769799\n",
      "Loss 3.199127279410716\n",
      "Loss 3.198511855394944\n",
      "Loss 3.196027648810184\n",
      "Loss 3.1928955821127727\n",
      "Loss 3.188662898949799\n",
      "Loss 3.1904839154492075\n",
      "Loss 3.188812035499735\n",
      "Loss 3.1893305967419834\n",
      "Loss 3.185123518700338\n",
      "Loss 3.185026595121672\n",
      "Loss 3.1874512189102973\n",
      "Loss 3.184538410256306\n",
      "Loss 3.1867919441080685\n",
      "Loss 3.187950053146063\n",
      "Loss 3.1884003647560935\n",
      "Loss 3.189519893878796\n",
      "Loss 3.1894418299441436\n",
      "Loss 3.1921804085010437\n",
      "Loss 3.1890808110777664\n",
      "Loss 3.1879627066754526\n",
      "Loss 3.1861037214597068\n",
      "Loss 3.1870522922992706\n",
      "Loss 3.184796414004854\n",
      "Loss 3.1855989140650585\n",
      "Loss 3.1831918991130332\n",
      "Loss 3.1836832915234754\n",
      "Loss 3.184669211892521\n",
      "Loss 3.1838667151518165\n",
      "Loss 3.182083730660524\n",
      "Loss 3.181026537242786\n",
      "Loss 3.1789528776319793\n",
      "Loss 3.181674573650727\n",
      "Loss 3.1803992059495716\n",
      "Loss 3.1770663816965263\n",
      "Loss 3.1737735147711894\n",
      "Loss 3.1720479943987097\n",
      "Loss 3.1687458597489124\n",
      "Loss 3.1660355805902554\n",
      "Loss 3.1704364763217026\n",
      "Loss 3.171904128256129\n",
      "Loss 3.172332814945164\n",
      "Loss 3.1715822301087555\n",
      "Loss 3.1699345632232863\n",
      "Loss 3.168062344544074\n",
      "Loss 3.1684961506735276\n",
      "Loss 3.1722862887556533\n",
      "Loss 3.175871704708446\n",
      "Loss 3.1774220802645754\n",
      "Loss 3.1788061980736386\n",
      "Loss 3.1795993472174775\n",
      "Loss 3.178696469508619\n",
      "Loss 3.1814985870889254\n",
      "Loss 3.1798955900389103\n",
      "Loss 3.177318700281441\n",
      "Loss 3.1786859419236335\n",
      "Loss 3.1774329728223907\n",
      "Loss 3.180292869785376\n",
      "Loss 3.1798968428081564\n",
      "Loss 3.1782711028222006\n",
      "Loss 3.1760402431504593\n",
      "Loss 3.180319706585168\n",
      "Loss 3.1781902083035174\n",
      "Loss 3.1773912388434526\n",
      "Loss 3.1762968099280577\n",
      "Loss 3.1755052927004193\n",
      "Loss 3.1734655638130342\n",
      "Loss 3.1740471314171614\n",
      "Loss 3.175386840225877\n",
      "Loss 3.1723251595641626\n",
      "Loss 3.171279496874585\n",
      "Loss 3.1731608369278668\n",
      "Loss 3.1715954590638478\n",
      "Loss 3.1694185927460756\n",
      "Loss 3.167785578374042\n",
      "Loss 3.167611590195017\n",
      "Loss 3.1663391007796715\n",
      "Loss 3.1642404830260356\n",
      "Loss 3.1661461984410004\n",
      "Loss 3.1623528620707484\n",
      "Loss 3.1599529181594974\n",
      "Loss 3.156550992191031\n",
      "Loss 3.1551604607028345\n",
      "Loss 3.1540750006004163\n",
      "Loss 3.1508104528754184\n",
      "Loss 3.1492122085711447\n",
      "Loss 3.1473622109859614\n",
      "Loss 3.1445576126234873\n",
      "Loss 3.1418173857127565\n",
      "Loss 3.140608379367022\n",
      "Loss 3.137888437434562\n",
      "Loss 3.13845065249171\n",
      "Loss 3.139621566385031\n",
      "Loss 3.1377104122690693\n",
      "Loss 3.1359299991071596\n",
      "Loss 3.136232434817512\n",
      "Loss 3.1356628986052524\n",
      "Loss 3.132962783409999\n",
      "Loss 3.1311439085957464\n",
      "Loss 3.127639020291308\n",
      "Loss 3.128990725555071\n",
      "Loss 3.1271170394398884\n",
      "Loss 3.1249268383329567\n",
      "Loss 3.1322136787129313\n",
      "Loss 3.1334375242870975\n",
      "Loss 3.131649274439425\n",
      "Loss 3.1297831490368186\n",
      "Loss 3.127979762981187\n",
      "Loss 3.127910709239188\n",
      "Loss 3.1269817868043126\n",
      "Loss 3.124830111455635\n",
      "Loss 3.122662282693351\n",
      "Loss 3.125107073047582\n",
      "Loss 3.123805471494401\n",
      "Loss 3.122067769124494\n",
      "Loss 3.1199630010232284\n",
      "Loss 3.119999608847984\n",
      "Loss 3.122976348400116\n",
      "Loss 3.120800844816803\n",
      "Loss 3.121852963733398\n",
      "Loss 3.1193567212285664\n",
      "Loss 3.117648160525926\n",
      "Loss 3.1170515239579335\n",
      "Loss 3.1171014206361907\n",
      "Loss 3.1153791848231447\n",
      "Loss 3.1125056524790042\n",
      "Loss 3.1138149304241782\n",
      "Loss 3.1113663979315422\n",
      "Loss 3.1141500794485713\n",
      "Loss 3.1155922326387144\n",
      "Loss 3.112731007823731\n",
      "Loss 3.1118900409315953\n",
      "Loss 3.1090916785928937\n",
      "Loss 3.1097899325666667\n",
      "Loss 3.1070022359737375\n",
      "Loss 3.103771084891863\n",
      "Loss 3.1025871699000453\n",
      "Loss 3.101023124505396\n",
      "Loss 3.1013701092350026\n",
      "Loss 3.1013586962905175\n",
      "Loss 3.1001610554106858\n",
      "Loss 3.0989421391551732\n",
      "Loss 3.0972559173687086\n",
      "Loss 3.096382869028981\n",
      "Loss 3.095847739423475\n",
      "Loss 3.0950941254242497\n",
      "Loss 3.095417065225183\n",
      "Loss 3.0941503971735638\n",
      "Loss 3.0937070737493797\n",
      "Loss 3.0934602919560845\n",
      "Loss 3.0941015115490664\n",
      "Loss 3.095679200041577\n",
      "Loss 3.0933150294266247\n",
      "Loss 3.0909504821294247\n",
      "Loss 3.0904377218316363\n",
      "Loss 3.09095869602796\n",
      "Loss 3.092042461130768\n",
      "Loss 3.093952237135404\n",
      "Loss 3.0950178422767265\n",
      "Loss 3.0953037781924855\n",
      "Loss 3.0950984679239313\n",
      "Loss 3.098857761134219\n",
      "Loss 3.1004721694114883\n",
      "Loss 3.0996760490605286\n",
      "Loss 3.0999559601654814\n",
      "Loss 3.0984544643918976\n",
      "Loss 3.0993228837923352\n",
      "Loss 3.1021170976493933\n",
      "Loss 3.101632169250286\n",
      "Loss 3.100790598290393\n",
      "Loss 3.100314777999667\n",
      "Loss 3.100492771974482\n",
      "Loss 3.0989615094959735\n",
      "Loss 3.099585574285645\n",
      "Loss 3.09968821732559\n",
      "Loss 3.099926616802405\n",
      "Loss 3.099664279611984\n",
      "Loss 3.0988085649337296\n",
      "Loss 3.0976437889883672\n",
      "Loss 3.096489112500184\n",
      "Loss 3.095705427638456\n",
      "Loss 3.0931943962509885\n",
      "Loss 3.0927880923631714\n",
      "Loss 3.0936775915524097\n",
      "Loss 3.0951244834846663\n",
      "Loss 3.09394917473666\n",
      "Loss 3.094783789034627\n",
      "Loss 3.0995355043928305\n",
      "Loss 3.098834058212546\n",
      "Loss 3.09868622106328\n",
      "Loss 3.097162414737866\n",
      "Loss 3.0986540575300596\n",
      "Loss 3.100075770701681\n",
      "Loss 3.0987782905050807\n",
      "Loss 3.0969532910398962\n",
      "Loss 3.0934355887679064\n",
      "Loss 3.091266816629554\n",
      "Loss 3.088848774966072\n",
      "Loss 3.089005085923862\n",
      "Loss 3.086925975695706\n",
      "Loss 3.088890884412783\n",
      "Loss 3.088448224006555\n",
      "Loss 3.088349848292595\n",
      "Loss 3.086581960659293\n",
      "Loss 3.08487335893291\n",
      "Loss 3.0842737992971654\n",
      "Loss 3.0855610185504507\n",
      "Loss 3.085117268945979\n",
      "Loss 3.0830778641985095\n",
      "Loss 3.0826754029064483\n",
      "Loss 3.084081999379206\n",
      "Loss 3.0841256722230845\n",
      "Loss 3.0824136314337904\n",
      "Loss 3.0816281102952505\n",
      "Loss 3.080569369275106\n",
      "Loss 3.0804700324551515\n",
      "Loss 3.079752878106392\n",
      "Loss 3.079430707920803\n",
      "Loss 3.078990552965301\n",
      "Loss 3.0783837465365194\n",
      "Loss 3.0767342329025267\n",
      "Loss 3.0772276548075514\n",
      "Loss 3.0782015921009913\n",
      "Loss 3.0787381186717893\n",
      "Loss 3.077399647499608\n",
      "Loss 3.077866476418956\n",
      "Loss 3.075618820358478\n",
      "Loss 3.0764090946218468\n",
      "Loss 3.07620880316224\n",
      "Loss 3.0761383946667094\n",
      "Loss 3.0754454674418836\n",
      "Loss 3.0740919582984025\n",
      "Loss 3.0764175545391828\n",
      "Loss 3.077281855735241\n",
      "Loss 3.0766163859790536\n",
      "Loss 3.074667907224618\n",
      "Loss 3.075912531850667\n",
      "Loss 3.075260286228631\n",
      "Loss 3.0751815679247287\n",
      "Loss 3.075553665375556\n",
      "Loss 3.074561430908676\n",
      "Loss 3.0738147042707595\n",
      "Loss 3.0735081540016416\n",
      "Loss 3.0747265620849187\n",
      "Loss 3.073418070956812\n",
      "Loss 3.07265848616969\n",
      "Loss 3.0734210095828094\n",
      "Loss 3.071271555900574\n",
      "Loss 3.071709986093665\n",
      "Loss 3.072225060502938\n",
      "Loss 3.073457002564953\n",
      "Loss 3.073284008209292\n",
      "Loss 3.071132536480824\n",
      "Loss 3.0690128314024196\n",
      "Loss 3.06775389627797\n",
      "Loss 3.067606124275713\n",
      "Loss 3.0671686580851056\n",
      "Loss 3.0681785584970847\n",
      "Loss 3.069287410930351\n",
      "Loss 3.0713443021509925\n",
      "Loss 3.0697919106630027\n",
      "Loss 3.0699302699424496\n",
      "Loss 3.0681345404167564\n",
      "Loss 3.067114415843715\n",
      "Loss 3.068840081226535\n",
      "Loss 3.067579442806708\n",
      "Loss 3.0669132775623305\n",
      "Loss 3.0656707391594393\n",
      "Loss 3.0651533816610614\n",
      "Loss 3.0654852144914853\n",
      "Loss 3.0646123334346527\n",
      "Loss 3.064524838609065\n",
      "Loss 3.064552657294273\n",
      "Loss 3.0636177608114994\n",
      "Loss 3.0627878019296793\n",
      "Loss 3.063816765098875\n",
      "Loss 3.063469843661028\n",
      "Loss 3.0625181640728867\n",
      "Loss 3.064794613933375\n",
      "Loss 3.0633158445358277\n",
      "Loss 3.0629991642133456\n",
      "Loss 3.0622705582785468\n",
      "Loss 3.0610395643524093\n",
      "Loss 3.059905140553902\n",
      "Loss 3.059976779622957\n",
      "Loss 3.0588087169747604\n",
      "Loss 3.0567978502247586\n",
      "Loss 3.0573820400700984\n",
      "Loss 3.055390129093976\n",
      "Loss 3.054186722918682\n",
      "Loss 3.0535521287025174\n",
      "Loss 3.052067693396111\n",
      "Loss 3.052089854960258\n",
      "Loss 3.051797236705238\n",
      "Loss 3.0510201397983505\n",
      "Loss 3.0495715582347964\n",
      "Loss 3.051024852636206\n",
      "Loss 3.0507647695768445\n",
      "Loss 3.0503346484196956\n",
      "Loss 3.050648803751654\n",
      "Loss 3.048970889390418\n",
      "Loss 3.0480896630224072\n",
      "Loss 3.0506438410282133\n",
      "Loss 3.0505467585700154\n",
      "Loss 3.048306330859213\n",
      "Loss 3.04636832654588\n",
      "Loss 3.0458332145481966\n",
      "Loss 3.0463538955973686\n",
      "Loss 3.0451992031353625\n",
      "Loss 3.043825602531433\n",
      "Loss 3.042231788289591\n",
      "Loss 3.0411888336869914\n",
      "Loss 3.0414224969457697\n",
      "Loss 3.0411022641522165\n",
      "Loss 3.042086919163426\n",
      "Loss 3.0426355925932334\n",
      "Loss 3.0415959419134784\n",
      "Loss 3.043061306804692\n",
      "Loss 3.0419837913888714\n",
      "Loss 3.0433606793065393\n",
      "Loss 3.0422569873559215\n",
      "Loss 3.0414071734485733\n",
      "Loss 3.0414356397932227\n",
      "Loss 3.041344915457516\n",
      "Loss 3.0404835653261864\n",
      "Loss 3.0398853321403103\n",
      "Loss 3.0400786390373424\n",
      "Loss 3.0390637964386125\n",
      "Loss 3.040364164092558\n",
      "Loss 3.040303878831264\n",
      "Loss 3.038776719313796\n",
      "Loss 3.039242884426083\n",
      "Loss 3.038213093365942\n",
      "Loss 3.0393650864534836\n",
      "Loss 3.0390251711382135\n",
      "Loss 3.0369203660881965\n",
      "Loss 3.0376842156792363\n",
      "Loss 3.0358189325417038\n",
      "Loss 3.035127649943315\n",
      "Loss 3.035874761559555\n",
      "Loss 3.0357867585731224\n",
      "Loss 3.035430575443697\n",
      "Loss 3.0360917068991746\n",
      "Loss 3.0348194400399873\n",
      "Loss 3.03351536218103\n",
      "Loss 3.031964825418815\n",
      "Loss 3.0308799017389476\n",
      "Loss 3.0303374032766923\n",
      "Loss 3.0292540311813356\n",
      "Loss 3.0287053347460318\n",
      "Loss 3.0281753562519707\n",
      "Loss 3.0282991845784824\n",
      "Loss 3.0289288636528213\n",
      "Loss 3.0283625071102085\n",
      "Loss 3.0283816405997652\n",
      "Loss 3.0289214465262346\n",
      "Loss 3.0283282336919277\n",
      "Loss 3.028525403238769\n",
      "Loss 3.028950392976149\n",
      "Loss 3.027216918037941\n",
      "Loss 3.027043055717637\n",
      "Loss 3.0269822582121817\n",
      "Loss 3.026245816845005\n",
      "Loss 3.027489472222207\n",
      "Loss 3.0255621864988997\n",
      "Loss 3.02522766638365\n",
      "Loss 3.024682985813128\n",
      "Loss 3.0232245870998926\n",
      "Loss 3.021539433190487\n",
      "Loss 3.020322016874949\n",
      "Loss 3.01930996099842\n",
      "Loss 3.0207670121837737\n",
      "Loss 3.019098059515158\n",
      "Loss 3.0176461243986488\n",
      "Loss 3.016915396935124\n",
      "Loss 3.0177181021491095\n",
      "Loss 3.0167329927351303\n",
      "Loss 3.015361052840209\n",
      "Loss 3.014871258959912\n",
      "Loss 3.015546868012411\n",
      "Loss 3.016481502452179\n",
      "Loss 3.015256795742242\n",
      "Loss 3.0138127799698564\n",
      "Loss 3.015493880882981\n",
      "Loss 3.015579543335765\n",
      "Loss 3.0137102966985343\n",
      "Loss 3.013315608485902\n",
      "Loss 3.0130902750705317\n",
      "Loss 3.012358358115345\n",
      "Loss 3.0120775053914595\n",
      "Loss 3.0124876330778436\n",
      "Loss 3.010824738873804\n",
      "Loss 3.0092162690047295\n",
      "Loss 3.010780293488464\n",
      "Loss 3.009865980650451\n",
      "Loss 3.010842142824568\n",
      "Loss 3.011052855027792\n",
      "Loss 3.0111763683891297\n",
      "Loss 3.0105003674190267\n",
      "Loss 3.008664071521881\n",
      "Loss 3.0093004949912903\n",
      "Loss 3.008999117228109\n",
      "Loss 3.0089833695926362\n",
      "Loss 3.0085088814306182\n",
      "Loss 3.0099455934159365\n",
      "Loss 3.0090427982863654\n",
      "Loss 3.0087974100834938\n",
      "Loss 3.0086356123038165\n",
      "Loss 3.0072516750239724\n",
      "Loss 3.006107072590659\n",
      "Loss 3.0053516631029242\n",
      "Loss 3.0056128776689093\n",
      "Loss 3.0074310735054315\n",
      "Loss 3.0074696699654258\n",
      "Loss 3.0068961316551377\n",
      "Loss 3.0084336892442423\n",
      "Loss 3.007956690373628\n",
      "Loss 3.007022131912468\n",
      "Loss 3.008077414393794\n",
      "Loss 3.00697109376813\n",
      "Loss 3.0084458339361495\n",
      "Loss 3.007092035184105\n",
      "Loss 3.005717870290463\n",
      "Loss 3.0043977843085377\n",
      "Loss 3.0038838645614727\n",
      "Loss 3.003461939744526\n",
      "Loss 3.001921953702921\n",
      "Loss 3.0014631588404415\n",
      "Loss 3.0016267514446886\n",
      "Loss 3.000937388567438\n",
      "Loss 3.000469177236673\n",
      "Loss 3.0004856105277677\n",
      "Loss 2.999420003349131\n",
      "Loss 2.999398345864306\n",
      "Loss 2.9978876057888444\n",
      "Loss 2.997187475471295\n",
      "Loss 2.9960913621158487\n",
      "Loss 2.995874914692757\n",
      "Loss 2.9952313560623307\n",
      "Loss 2.9949593100948135\n",
      "Loss 2.994280176926516\n",
      "Loss 2.993817068963842\n",
      "Loss 2.9939817273438867\n",
      "Loss 2.9939231411888416\n",
      "Loss 2.9934912071873745\n",
      "Loss 2.9937449667464393\n",
      "Loss 2.9954187762666526\n",
      "Loss 2.99580087712959\n",
      "Loss 2.9951323224242623\n",
      "Loss 2.9951251684122635\n",
      "Loss 2.9952936193760165\n",
      "Loss 2.9944776477975243\n",
      "Loss 2.993389711835805\n",
      "Loss 2.992956267947142\n",
      "Loss 2.9938964678220383\n",
      "Loss 2.993236694171872\n",
      "Loss 2.99254103879831\n",
      "Loss 2.992314561788183\n",
      "Loss 2.992725213241299\n",
      "Loss 2.992080243428548\n",
      "Loss 2.9917003234111985\n",
      "Loss 2.99155720153983\n",
      "Loss 2.992512492943501\n",
      "Loss 2.9937434275312467\n",
      "Loss 2.9924859319291364\n",
      "Loss 2.991655216846631\n",
      "Loss 2.9912311660109747\n",
      "Loss 2.991449017850615\n",
      "Loss 2.9906028711076442\n",
      "Loss 2.9897610265350067\n",
      "Loss 2.9889221276628937\n",
      "Loss 2.987906195813835\n",
      "Loss 2.98779313491072\n",
      "Loss 2.988644909637631\n",
      "Loss 2.988634361653586\n",
      "Loss 2.9887104096148125\n",
      "Loss 2.9878786592320963\n",
      "Loss 2.9876942144048977\n",
      "Loss 2.98627970370625\n",
      "Loss 2.986658118283732\n",
      "Loss 2.9858474922382223\n",
      "Loss 2.985254952584739\n",
      "Loss 2.986548237397637\n",
      "Loss 2.986828872439898\n",
      "Loss 2.9880791856499203\n",
      "Loss 2.987229489243549\n",
      "Loss 2.988067802377776\n",
      "Loss 2.9874714106613105\n",
      "Loss 2.987156650990081\n",
      "Loss 2.9874363477592523\n",
      "Loss 2.9876190800022613\n",
      "Loss 2.9877377820114432\n",
      "Loss 2.9873081125484573\n",
      "Loss 2.985926702779804\n",
      "Loss 2.984887324063071\n",
      "Loss 2.984501038382492\n",
      "Loss 2.984098212560896\n",
      "Loss 2.983071226218651\n",
      "Loss 2.98273063152618\n",
      "Loss 2.981746160705283\n",
      "Loss 2.981749127477735\n",
      "Loss 2.980994571612532\n",
      "Loss 2.9805649309615565\n",
      "Loss 2.9805707346072112\n",
      "Loss 2.9805802963307646\n",
      "Loss 2.9805752742014024\n",
      "Loss 2.9793906080950183\n",
      "Loss 2.979931241103581\n",
      "Loss 2.9807788776800686\n",
      "Loss 2.9804118152227623\n",
      "Loss 2.9804100900969206\n",
      "Loss 2.981785394374347\n",
      "Loss 2.9815514936962644\n",
      "Loss 2.9809355821686716\n",
      "Loss 2.979975470522986\n",
      "Loss 2.9808219885441045\n",
      "Loss 2.9796291317029664\n",
      "Loss 2.9794251229779034\n",
      "Loss 2.979602935566659\n",
      "Loss 2.980601265583651\n",
      "Loss 2.9802198264847464\n",
      "Loss 2.9794428024018242\n",
      "Loss 2.979962429555257\n",
      "Loss 2.9809328156527126\n",
      "Loss 2.9808539551306277\n",
      "Loss 2.980078673283576\n",
      "Loss 2.980735804389579\n",
      "Loss 2.9807118076362356\n",
      "Loss 2.9792028316272\n",
      "Loss 2.979728686494588\n",
      "Loss 2.9816363563795516\n",
      "Loss 2.981055485998061\n",
      "Loss 2.9800945778583223\n",
      "Loss 2.978952383243145\n",
      "Loss 2.97907695859436\n",
      "Loss 2.9789748602113435\n",
      "Loss 2.979492013897571\n",
      "Loss 2.979478515737197\n",
      "Loss 2.97891664196866\n",
      "Loss 2.979710896929053\n",
      "Loss 2.9799802085384726\n",
      "Loss 2.979731460712046\n",
      "Loss 2.978842010993462\n",
      "Loss 2.9789653278510393\n",
      "Loss 2.9785243106629564\n",
      "Loss 2.9783422611200794\n",
      "Loss 2.9788715761145266\n",
      "Loss 2.977756993709072\n",
      "Loss 2.977815738043834\n",
      "Loss 2.977758292802052\n",
      "Loss 2.9777543809389395\n",
      "Loss 2.9774089449789467\n",
      "Loss 2.9774420252518774\n",
      "Loss 2.9773525778523786\n",
      "Loss 2.9771077531225543\n",
      "Loss 2.976023777523869\n",
      "Loss 2.977150183566371\n",
      "Loss 2.978172991032813\n",
      "Loss 2.9777887761896196\n",
      "Loss 2.9786377167459364\n",
      "Loss 2.9773827379730147\n",
      "Loss 2.977006003370152\n",
      "Loss 2.976447397817539\n",
      "Loss 2.9764081857448583\n",
      "Loss 2.975846959710723\n",
      "Loss 2.9744313996646925\n",
      "Loss 2.973865898069867\n",
      "Loss 2.9734109359267373\n",
      "Loss 2.9732133698223824\n",
      "Loss 2.9733602630748055\n",
      "Loss 2.971942547792779\n",
      "Loss 2.972614269710154\n",
      "Loss 2.9731800884604453\n",
      "Loss 2.973518624957581\n",
      "Loss 2.973355609386639\n",
      "Loss 2.9731531572223155\n",
      "Loss 2.971926282353662\n",
      "Loss 2.9708708498937004\n",
      "Loss 2.9705640374283044\n",
      "Loss 2.9693158089597578\n",
      "Loss 2.9686279724730125\n",
      "Loss 2.9684417352835535\n",
      "Loss 2.96786540194794\n",
      "Loss 2.9661272328820094\n",
      "Loss 2.96575941919106\n",
      "Loss 2.9649052440460317\n",
      "Loss 2.9656696287068454\n",
      "Loss 2.9659859490394593\n",
      "Loss 2.9649348965900786\n",
      "Loss 2.9650372819042556\n",
      "Loss 2.965356471282054\n",
      "Loss 2.9657715969906624\n",
      "Loss 2.9652452648267515\n",
      "Loss 2.9655752791847294\n",
      "Loss 2.96713727569058\n",
      "Loss 2.9666561434074854\n",
      "Loss 2.966137692838618\n",
      "Loss 2.9654126310637503\n",
      "Loss 2.965106234495634\n",
      "Loss 2.9642557392962177\n",
      "Loss 2.964853394103511\n",
      "Loss 2.9642097577542534\n",
      "Loss 2.96470674013517\n",
      "Loss 2.964205074984531\n",
      "Loss 2.964897653474831\n",
      "Loss 2.9642175133059436\n",
      "Loss 2.963846546868912\n",
      "Loss 2.9629930886251485\n",
      "Loss 2.9634805197065526\n",
      "Loss 2.96377161388044\n",
      "Loss 2.963969217607116\n",
      "Loss 2.9627970927804532\n",
      "Loss 2.9639999541214532\n",
      "Loss 2.964993722685452\n",
      "Loss 2.9648017177247663\n",
      "Loss 2.9649837837100455\n",
      "Loss 2.964485973712957\n",
      "Loss 2.964126196556543\n",
      "Loss 2.9649109060882677\n",
      "Loss 2.964415571104115\n",
      "Loss 2.9637551899366783\n",
      "Loss 2.962586379402237\n",
      "Loss 2.961592437099008\n",
      "Loss 2.9622339612168513\n",
      "Loss 2.9615457814344217\n",
      "Loss 2.961054818613448\n",
      "Loss 2.961034384935187\n",
      "Loss 2.9604052847728393\n",
      "Loss 2.9609804583347845\n",
      "Loss 2.961007573863311\n",
      "Loss 2.961762557552133\n",
      "Loss 2.9606276441647372\n",
      "Loss 2.960452364877213\n",
      "Loss 2.960465187741212\n",
      "Loss 2.9607781062131693\n",
      "Loss 2.959943630562735\n",
      "Loss 2.9598834283787894\n",
      "Loss 2.960014322562025\n",
      "Loss 2.960475976211094\n",
      "Loss 2.9606811840778953\n",
      "Loss 2.9601907362311666\n",
      "Loss 2.960494679370327\n",
      "Loss 2.9603628351633575\n",
      "Loss 2.9613430544511594\n",
      "Loss 2.9630067330708196\n",
      "Loss 2.963157621323038\n",
      "Loss 2.9635076240625753\n",
      "Loss 2.963187032072885\n",
      "Loss 2.963037556369011\n",
      "Loss 2.9627089688688213\n",
      "Loss 2.96250024759688\n",
      "Loss 2.961917430481135\n",
      "Loss 2.9617485141076827\n",
      "Loss 2.9622297665182495\n",
      "Loss 2.961545339976849\n",
      "Loss 2.960900100859753\n",
      "Loss 2.961081597667474\n",
      "Loss 2.960576672971585\n",
      "Loss 2.9600762863460566\n",
      "Loss 2.9588119429692608\n",
      "Loss 2.9587049921219415\n",
      "Loss 2.9580471947839477\n",
      "Loss 2.9572625732020046\n",
      "Loss 2.957560243684167\n",
      "Loss 2.9567404978478438\n",
      "Loss 2.9559174983525462\n",
      "Loss 2.9544918839163428\n",
      "Loss 2.954901349224858\n",
      "Loss 2.954601002750652\n",
      "Loss 2.9539751128342373\n",
      "Loss 2.9536418906564434\n",
      "Loss 2.953351532458198\n",
      "Loss 2.952855135493808\n",
      "Loss 2.9519144179818366\n",
      "Loss 2.950670815809339\n",
      "Loss 2.9506760163956702\n",
      "Loss 2.950612503923146\n",
      "Loss 2.9502915252242956\n",
      "Loss 2.95015916107243\n",
      "Loss 2.949716182646178\n",
      "Loss 2.9491073581484444\n",
      "Loss 2.949455312507512\n",
      "Loss 2.9499905199008984\n",
      "Loss 2.9495196538097117\n",
      "Loss 2.9490478227797308\n",
      "Loss 2.948896418670182\n",
      "Loss 2.948270172832831\n",
      "Loss 2.947479500731484\n",
      "Loss 2.9462754097177473\n",
      "Loss 2.945964153447354\n",
      "Loss 2.9451763965984834\n",
      "Loss 2.9441222853577567\n",
      "Loss 2.9441311540940536\n",
      "Loss 2.943215772020959\n",
      "Loss 2.942787916243981\n",
      "Loss 2.943006904011558\n",
      "Loss 2.9416398235858776\n",
      "Loss 2.94206924841855\n",
      "Loss 2.94218750947495\n",
      "Loss 2.9414051228356595\n",
      "Loss 2.9414057366935342\n",
      "Loss 2.941661836949565\n",
      "Loss 2.942086846290096\n",
      "Loss 2.9422464820419294\n",
      "Loss 2.9418221950531005\n",
      "Loss 2.9419708433514113\n",
      "Loss 2.941481945665286\n",
      "Loss 2.941340412723827\n",
      "Loss 2.9409020370525174\n",
      "Loss 2.9402984804761068\n",
      "Loss 2.9402148984134326\n",
      "Loss 2.9397331038603007\n",
      "Loss 2.9393098396570125\n",
      "Loss 2.9389880816564045\n",
      "Loss 2.9394374178726457\n",
      "Loss 2.939103023940154\n",
      "Loss 2.938963127843404\n",
      "Loss 2.9377941575630633\n",
      "Loss 2.9376437961173107\n",
      "Loss 2.9370016537095323\n",
      "Loss 2.936412556236806\n",
      "Loss 2.9362863903804373\n",
      "Loss 2.9355408884851557\n",
      "Loss 2.9356349927895202\n",
      "Loss 2.935176525872295\n",
      "Loss 2.9348674531623424\n",
      "Loss 2.9342476026567024\n",
      "Loss 2.9335168288765154\n",
      "Loss 2.933087176496015\n",
      "Loss 2.9331086266750828\n",
      "Loss 2.932245964359391\n",
      "Loss 2.932596284853405\n",
      "Loss 2.9335368104403217\n",
      "Loss 2.9334034519190593\n",
      "Loss 2.9337279036362305\n",
      "Loss 2.9330094786695478\n",
      "Loss 2.9322630632467783\n",
      "Loss 2.9318386546564845\n",
      "Loss 2.932615265688294\n",
      "Loss 2.9317392959836353\n",
      "Loss 2.9311719864606856\n",
      "Loss 2.9320492649102974\n",
      "Loss 2.9328130304076008\n",
      "Loss 2.9333927536600037\n",
      "Loss 2.9329735558096766\n",
      "Loss 2.9336132511083046\n",
      "Loss 2.9324863210228678\n",
      "Loss 2.932251658182878\n",
      "Loss 2.9322669773683194\n",
      "Loss 2.932359418934907\n",
      "Loss 2.9322439103404436\n",
      "Loss 2.9311204882759117\n",
      "Loss 2.930861486889878\n",
      "Loss 2.9304665525754294\n",
      "Loss 2.9310028370796055\n",
      "Loss 2.931224260131316\n",
      "Loss 2.9299429714316276\n",
      "Loss 2.930329387974618\n",
      "Loss 2.9298159235510335\n",
      "Loss 2.9304201894980433\n",
      "Loss 2.9300093204748294\n",
      "Loss 2.9286146938499473\n",
      "Loss 2.9288104110775572\n",
      "Loss 2.92812545434257\n",
      "Loss 2.9275858777545154\n",
      "Loss 2.926974539046081\n",
      "Loss 2.9280099509347615\n",
      "Loss 2.9274810455672107\n",
      "Loss 2.926659126992685\n",
      "Loss 2.926580744174682\n",
      "Loss 2.9262932331624154\n",
      "Loss 2.9257403173007526\n",
      "Loss 2.9260059770941735\n",
      "Loss 2.9253687385198\n",
      "Loss 2.925595250517546\n",
      "Loss 2.925033326472266\n",
      "Loss 2.9254876560423955\n",
      "Loss 2.9259215172487707\n",
      "Loss 2.9254009977605184\n",
      "Loss 2.9244376487859784\n",
      "Loss 2.9239204459128874\n",
      "Loss 2.9242832295485837\n",
      "Loss 2.923637854569029\n",
      "Loss 2.9235058458811216\n",
      "Loss 2.923687252143155\n",
      "Loss 2.924047841394995\n",
      "Loss 2.9240615678491912\n",
      "Loss 2.924190527112613\n",
      "Loss 2.9244616041619946\n",
      "Loss 2.925358799952673\n",
      "Loss 2.926252801771951\n",
      "Loss 2.9281959323232614\n",
      "Loss 2.9279559430187825\n",
      "Loss 2.928376664902858\n",
      "Loss 2.927856950633913\n",
      "Loss 2.9281992113019246\n",
      "Loss 2.9287316508591177\n",
      "Loss 2.9287455870000327\n",
      "Loss 2.92850690333002\n",
      "Loss 2.928858392124863\n",
      "Loss 2.9292404245425754\n",
      "Loss 2.928861122054887\n",
      "Loss 2.9287689801211494\n",
      "Loss 2.929197809721635\n",
      "Loss 2.929091154038906\n",
      "Loss 2.930094988117606\n",
      "Loss 2.9295761279949137\n",
      "Loss 2.9298901276772726\n",
      "Loss 2.9289732894957297\n",
      "Loss 2.9284828817832897\n",
      "Loss 2.9286669482615184\n",
      "Loss 2.9276086262835115\n",
      "Loss 2.9277594075982387\n",
      "Loss 2.9276646465893323\n",
      "Loss 2.9279718206390997\n",
      "Loss 2.9275412794956197\n",
      "Loss 2.927010120331556\n",
      "Loss 2.9262373622410607\n",
      "Loss 2.925976022798746\n",
      "Loss 2.9251772950235275\n",
      "Loss 2.9247696906100704\n",
      "Loss 2.9244610122661574\n",
      "Loss 2.9239375405311585\n",
      "Loss 2.9233003301919926\n",
      "Loss 2.922644008781067\n",
      "Loss 2.9225189534800458\n",
      "Loss 2.9222716232630965\n",
      "Loss 2.9218792438394083\n",
      "Loss 2.92162817839646\n",
      "Loss 2.921324282157162\n",
      "Loss 2.9219224718207446\n",
      "Loss 2.922127972175987\n",
      "Loss 2.92208610607768\n",
      "Loss 2.922093616893671\n",
      "Loss 2.922366867530144\n",
      "Loss 2.9221139775058176\n",
      "Loss 2.9222316623966496\n",
      "Loss 2.9224996269252936\n",
      "Loss 2.921977399672621\n",
      "Loss 2.922394990295367\n",
      "Loss 2.9216219103291685\n",
      "Loss 2.920588538845194\n",
      "Loss 2.920455657299434\n",
      "Loss 2.9199278627679686\n",
      "Loss 2.9201765258921615\n",
      "Loss 2.9194295868513636\n",
      "Loss 2.9196121199281966\n",
      "Loss 2.919233387303907\n",
      "Loss 2.9186015517959807\n",
      "Loss 2.9179402590351406\n",
      "Loss 2.9174285446226267\n",
      "Loss 2.91667910703804\n",
      "Loss 2.916750791448134\n",
      "Loss 2.9158717606709468\n",
      "Loss 2.9156287373544547\n",
      "Loss 2.9148712045708303\n",
      "Loss 2.9156602004467342\n",
      "Loss 2.915448402918978\n",
      "Loss 2.9149051181629\n",
      "Loss 2.915347126414168\n",
      "Loss 2.9144651729832676\n",
      "Loss 2.915466278731221\n",
      "Loss 2.9152305481302627\n",
      "Loss 2.915221324675024\n",
      "Loss 2.915017563333878\n",
      "Loss 2.9146424398003425\n",
      "Loss 2.914126369460627\n",
      "Loss 2.9142653087285013\n",
      "Loss 2.9141869038017125\n",
      "Loss 2.9149538606432426\n",
      "Loss 2.914718815255469\n",
      "Loss 2.9147361572164097\n",
      "Loss 2.9152546489672226\n",
      "Loss 2.915990200116351\n",
      "Loss 2.9153406215449644\n",
      "Loss 2.9153262520636196\n",
      "Loss 2.9166132197755834\n",
      "Loss 2.916155056737667\n",
      "Loss 2.9166486512256573\n",
      "Loss 2.9165141640316827\n",
      "Loss 2.9160357000702986\n",
      "Loss 2.9161418806119905\n",
      "Loss 2.9155302862004118\n",
      "Loss 2.91454840516529\n",
      "Loss 2.913814871270451\n",
      "Loss 2.913481034358557\n",
      "Loss 2.913016538174833\n",
      "Loss 2.912659208036859\n",
      "Loss 2.91229994467937\n",
      "Loss 2.91255471770424\n",
      "Loss 2.912164795729257\n",
      "Loss 2.912178295504525\n",
      "Loss 2.9123956645173688\n",
      "Loss 2.913283163644074\n",
      "Loss 2.9129161062087605\n",
      "Loss 2.9124310866170764\n",
      "Loss 2.9124547369539524\n",
      "Loss 2.9128445499844022\n",
      "Loss 2.9130774489304523\n",
      "Loss 2.912232845455752\n",
      "Loss 2.911503879408887\n",
      "Loss 2.911616773854535\n",
      "Loss 2.9123708378741173\n",
      "Loss 2.9127434476406355\n",
      "Loss 2.9120210957084867\n",
      "Loss 2.9122761806658906\n",
      "Loss 2.911911517423717\n",
      "Loss 2.911543185700404\n",
      "Loss 2.910834504894807\n",
      "Loss 2.909867883294742\n",
      "Loss 2.909782226747704\n",
      "Loss 2.9095569659053915\n",
      "Loss 2.909066490781935\n",
      "Loss 2.9090734779364595\n",
      "Loss 2.909194909931273\n",
      "Loss 2.9087058215003627\n",
      "Loss 2.908259315997154\n",
      "Loss 2.90790367288881\n",
      "Loss 2.908871544551683\n",
      "Loss 2.9085673054613648\n",
      "Loss 2.907882536215649\n",
      "Loss 2.9072981903509434\n",
      "Loss 2.907064116457234\n",
      "Loss 2.9069553138793394\n",
      "Loss 2.906481425722854\n",
      "Loss 2.9060772113250013\n",
      "Loss 2.906848467925186\n",
      "Loss 2.907577138803738\n",
      "Loss 2.9068727435022077\n",
      "Loss 2.907302342031985\n",
      "Loss 2.9070619574894354\n",
      "Loss 2.906818989960668\n",
      "Loss 2.906540963516153\n",
      "Loss 2.9054810452830884\n",
      "Loss 2.905280331951821\n",
      "Loss 2.905193149407978\n",
      "Loss 2.9051140215491102\n",
      "Loss 2.904889781802509\n",
      "Loss 2.9051556875223157\n",
      "Loss 2.904900710543916\n",
      "Loss 2.9049133329971197\n",
      "Loss 2.9047867541133696\n",
      "Loss 2.9039742692515382\n",
      "Loss 2.902923153629678\n",
      "Loss 2.9030687684390326\n",
      "Loss 2.9034748820087795\n",
      "Loss 2.903796103388528\n",
      "Loss 2.903360319695574\n",
      "Loss 2.9025579030821924\n",
      "Loss 2.9020691157501433\n",
      "Loss 2.902255263747505\n",
      "Loss 2.9022938118636152\n",
      "Loss 2.9024465992087025\n",
      "Loss 2.902338859239541\n",
      "Loss 2.901878732392429\n",
      "Loss 2.9016359966182144\n",
      "Loss 2.9012906126577307\n",
      "Loss 2.9010634295346867\n",
      "Loss 2.9011884220836537\n",
      "Loss 2.9005078252364127\n",
      "Loss 2.90119272025948\n",
      "Loss 2.901631770332487\n",
      "Loss 2.9014251598690737\n",
      "Loss 2.90133987573092\n",
      "Loss 2.9017019940802715\n",
      "Loss 2.901900939549704\n",
      "Loss 2.901616566015448\n",
      "Loss 2.9015157423258824\n",
      "Loss 2.902388200642273\n",
      "Loss 2.9021266071419967\n",
      "Loss 2.9015748217229254\n",
      "Loss 2.9014748136533908\n",
      "Loss 2.900882707407077\n",
      "Loss 2.9003239699942585\n",
      "Loss 2.9004582334973055\n",
      "Loss 2.9007169801398107\n",
      "Loss 2.9009813294854276\n",
      "Loss 2.900784388904255\n",
      "Loss 2.9011367847472678\n",
      "Loss 2.9007857542049815\n",
      "Loss 2.90047414589994\n",
      "Loss 2.899687580888778\n",
      "Loss 2.899484393192717\n",
      "Loss 2.9002568094716596\n",
      "Loss 2.9006075138207708\n",
      "Loss 2.9009494637027813\n",
      "Loss 2.9011794014758685\n",
      "Loss 2.901636752081506\n",
      "Loss 2.9010881191217583\n",
      "Loss 2.9011199808355816\n",
      "Loss 2.901435999075572\n",
      "Loss 2.9009766450663683\n",
      "Loss 2.9005068143078536\n",
      "Loss 2.9008491209351948\n",
      "Loss 2.9004134495695366\n",
      "Loss 2.9001229757475055\n",
      "Loss 2.899673188930633\n",
      "Loss 2.89933736591923\n",
      "Loss 2.898675050082059\n",
      "Loss 2.8983511036163243\n",
      "Loss 2.8980301061457063\n",
      "Loss 2.8979450768043202\n",
      "Loss 2.897625775085232\n",
      "Loss 2.8976052196697912\n",
      "Loss 2.89749583573891\n",
      "Loss 2.8972714553107504\n",
      "Loss 2.896488520420931\n",
      "Loss 2.8958417089844524\n",
      "Loss 2.895524669889493\n",
      "Loss 2.895450795564135\n",
      "Loss 2.895941230967665\n",
      "Loss 2.8956045650300526\n",
      "Loss 2.895514102914641\n",
      "Loss 2.895062437622322\n",
      "Loss 2.8945116081387523\n",
      "Loss 2.894203814669952\n",
      "Loss 2.893773929242919\n",
      "Loss 2.893369132151087\n",
      "Loss 2.8923886123572267\n",
      "Loss 2.8928512818734933\n",
      "Loss 2.892542921503385\n",
      "Loss 2.89191746419673\n",
      "Loss 2.892685166730881\n",
      "Loss 2.892500997866563\n",
      "Loss 2.8921739885696587\n",
      "Loss 2.8917410037466174\n",
      "Loss 2.8914328085558645\n",
      "Loss 2.8909899241705816\n",
      "Loss 2.8903522531697705\n",
      "Loss 2.8904798129957254\n",
      "Loss 2.890712504707194\n",
      "Loss 2.890429524782634\n",
      "Loss 2.8897705033563432\n",
      "Loss 2.8894312030828537\n",
      "Loss 2.8892203647058854\n",
      "Loss 2.8897097177290294\n",
      "Loss 2.889616428193035\n",
      "Loss 2.889211525643767\n",
      "Loss 2.889333471839462\n",
      "Loss 2.8893638382018225\n",
      "Loss 2.889394214119445\n",
      "Loss 2.8891978739001227\n",
      "Loss 2.8894990447844107\n",
      "Loss 2.889429972361243\n",
      "Loss 2.8894461514814846\n",
      "Loss 2.890005775623591\n",
      "Loss 2.8894550906152143\n",
      "Loss 2.8890543066006082\n",
      "Loss 2.8895186364650725\n",
      "Loss 2.8901356695588905\n",
      "Loss 2.8902232533142973\n",
      "Loss 2.8907975524920237\n",
      "Loss 2.8905064336024227\n",
      "Loss 2.8902056837249415\n",
      "Loss 2.8903534671445716\n",
      "Loss 2.8905906214717767\n",
      "Loss 2.89038326780187\n",
      "Loss 2.8895507274991346\n",
      "Loss 2.8890404301242176\n",
      "Loss 2.888737543266507\n",
      "Loss 2.888448945585615\n",
      "Loss 2.888351810163449\n",
      "Loss 2.888379890401234\n",
      "Loss 2.887435728953293\n",
      "Loss 2.8879217283921346\n",
      "Loss 2.8883673280869355\n",
      "Loss 2.8886228274156727\n",
      "Loss 2.8881148592937866\n",
      "Loss 2.8875030980066017\n",
      "Loss 2.886870047259349\n",
      "Loss 2.886576641992722\n",
      "Loss 2.88687039086046\n",
      "Loss 2.8861493395016744\n",
      "Loss 2.886288242345586\n",
      "Loss 2.8866927038414687\n",
      "Loss 2.886176996906263\n",
      "Loss 2.885976688609533\n",
      "Loss 2.8855672958618834\n",
      "Loss 2.8851711358082057\n",
      "Loss 2.884549763034844\n",
      "Loss 2.885046286548314\n",
      "Loss 2.8849716265115966\n",
      "Loss 2.8842498640249703\n",
      "Loss 2.883428534426642\n",
      "Loss 2.883622249388477\n",
      "Loss 2.88342290982343\n",
      "Loss 2.883134948771838\n",
      "Loss 2.883637708674819\n",
      "Loss 2.8836796185437668\n",
      "Loss 2.883508767381073\n",
      "Loss 2.88309815294464\n",
      "Loss 2.8828220748196416\n",
      "Loss 2.8820018970243857\n",
      "Loss 2.8814896826126826\n",
      "Loss 2.8820277157241025\n",
      "Loss 2.882195192561939\n",
      "Loss 2.8814846618748144\n",
      "Loss 2.8818905770103886\n",
      "Loss 2.881182581590851\n",
      "Loss 2.8805699445470077\n",
      "Loss 2.880970317403595\n",
      "Loss 2.8805820194557255\n",
      "Loss 2.8809191270340655\n",
      "Loss 2.881214260974385\n",
      "Loss 2.8811630734720746\n",
      "Loss 2.881183290910828\n",
      "Loss 2.880682385843554\n",
      "Loss 2.8796633369467233\n",
      "Loss 2.8798066884814624\n",
      "Loss 2.879836184129665\n",
      "Loss 2.880248119834649\n",
      "Loss 2.8798513666207084\n",
      "Epoch 1/5, Loss: 2.8798\n",
      "Loss 3.0103490233421324\n",
      "Loss 2.6062212944030763\n",
      "Loss 2.5747472286224364\n",
      "Loss 2.381590408086777\n",
      "Loss 2.3745323038101196\n",
      "Loss 2.4376820007960003\n",
      "Loss 2.566318188394819\n",
      "Loss 2.596766898036003\n",
      "Loss 2.507114973333147\n",
      "Loss 2.533543084859848\n",
      "Loss 2.4908133680170232\n",
      "Loss 2.445680715640386\n",
      "Loss 2.5681779962319595\n",
      "Loss 2.5316174336842128\n",
      "Loss 2.4994482437769574\n",
      "Loss 2.5107942461967467\n",
      "Loss 2.592910092718461\n",
      "Loss 2.6527732180224524\n",
      "Loss 2.67354261310477\n",
      "Loss 2.630258167386055\n",
      "Loss 2.602833750702086\n",
      "Loss 2.6502970690077\n",
      "Loss 2.6417879177176435\n",
      "Loss 2.653181099394957\n",
      "Loss 2.6613642077445983\n",
      "Loss 2.6736132122003116\n",
      "Loss 2.6901218665970696\n",
      "Loss 2.6988007843494417\n",
      "Loss 2.6868067474200807\n",
      "Loss 2.7164200353622436\n",
      "Loss 2.7402059408926194\n",
      "Loss 2.748817274719477\n",
      "Loss 2.74861243168513\n",
      "Loss 2.7331657897023596\n",
      "Loss 2.7370623895100183\n",
      "Loss 2.7499381711085635\n",
      "Loss 2.7287647505064268\n",
      "Loss 2.7404113095057636\n",
      "Loss 2.7274683081186737\n",
      "Loss 2.7217181104421617\n",
      "Loss 2.734626430999942\n",
      "Loss 2.7514657395226614\n",
      "Loss 2.745892702978711\n",
      "Loss 2.76044971130111\n",
      "Loss 2.7686294595400494\n",
      "Loss 2.7566198245338773\n",
      "Loss 2.7577309745423335\n",
      "Loss 2.740955060472091\n",
      "Loss 2.738253120500214\n",
      "Loss 2.7408682618141174\n",
      "Loss 2.751971076516544\n",
      "Loss 2.7465948891181213\n",
      "Loss 2.7573900791833985\n",
      "Loss 2.762937450188178\n",
      "Loss 2.7601938728852704\n",
      "Loss 2.750586829866682\n",
      "Loss 2.755397602131492\n",
      "Loss 2.769264277096452\n",
      "Loss 2.759597472821252\n",
      "Loss 2.7546057963371275\n",
      "Loss 2.74979972976153\n",
      "Loss 2.748661665378078\n",
      "Loss 2.742237252280826\n",
      "Loss 2.7447924457490442\n",
      "Loss 2.7452926125893224\n",
      "Loss 2.741143031915029\n",
      "Loss 2.7353761797520653\n",
      "Loss 2.7473256870227702\n",
      "Loss 2.764167967222739\n",
      "Loss 2.770759389911379\n",
      "Loss 2.7648530169272085\n",
      "Loss 2.759145337012079\n",
      "Loss 2.746098080889819\n",
      "Loss 2.746491081489099\n",
      "Loss 2.748676607290904\n",
      "Loss 2.7477685802861265\n",
      "Loss 2.7407376493726456\n",
      "Loss 2.7390610138575235\n",
      "Loss 2.733413695534573\n",
      "Loss 2.7441280394792558\n",
      "Loss 2.746235131923063\n",
      "Loss 2.7410151958465576\n",
      "Loss 2.73638744785125\n",
      "Loss 2.7345168892826353\n",
      "Loss 2.7354140605646022\n",
      "Loss 2.738782115869744\n",
      "Loss 2.7416347409116812\n",
      "Loss 2.7326865861361678\n",
      "Loss 2.725383679384596\n",
      "Loss 2.731716218524509\n",
      "Loss 2.736416326381348\n",
      "Loss 2.7543485498946647\n",
      "Loss 2.747106190137966\n",
      "Loss 2.7483081257089657\n",
      "Loss 2.745330216884613\n",
      "Loss 2.740989064797759\n",
      "Loss 2.741171931974667\n",
      "Loss 2.739510196082446\n",
      "Loss 2.7434278583285785\n",
      "Loss 2.743815987944603\n",
      "Loss 2.747971474298156\n",
      "Loss 2.750515077978957\n",
      "Loss 2.743931162125856\n",
      "Loss 2.7358821021822783\n",
      "Loss 2.729407295272464\n",
      "Loss 2.728221509141742\n",
      "Loss 2.726123837889912\n",
      "Loss 2.730165551326893\n",
      "Loss 2.7279075267118054\n",
      "Loss 2.7295911308852108\n",
      "Loss 2.7273963734910294\n",
      "Loss 2.7229231615151677\n",
      "Loss 2.7309106397417793\n",
      "Loss 2.7311006872277512\n",
      "Loss 2.738187786185223\n",
      "Loss 2.747896001359512\n",
      "Loss 2.7396792175423386\n",
      "Loss 2.7421972292964742\n",
      "Loss 2.7414515889993236\n",
      "Loss 2.7438108457128205\n",
      "Loss 2.7456428875607894\n",
      "Loss 2.744478957574876\n",
      "Loss 2.746939343844003\n",
      "Loss 2.747719350361055\n",
      "Loss 2.7485952649116516\n",
      "Loss 2.7501620696650613\n",
      "Loss 2.7568947963827237\n",
      "Loss 2.758093375246972\n",
      "Loss 2.759686585359795\n",
      "Loss 2.7638896178282226\n",
      "Loss 2.7640607620923574\n",
      "Loss 2.7655371498880963\n",
      "Loss 2.769731725068917\n",
      "Loss 2.764464070280986\n",
      "Loss 2.7625243371504324\n",
      "Loss 2.758403406248373\n",
      "Loss 2.7563782102870245\n",
      "Loss 2.752317964080451\n",
      "Loss 2.752558691124264\n",
      "Loss 2.745768335206168\n",
      "Loss 2.7509863584599596\n",
      "Loss 2.747168053707606\n",
      "Loss 2.7562980905279413\n",
      "Loss 2.753329076866309\n",
      "Loss 2.754602084899771\n",
      "Loss 2.7490043329865963\n",
      "Loss 2.747287465601551\n",
      "Loss 2.745537868947596\n",
      "Loss 2.7476388755260697\n",
      "Loss 2.7598846482435864\n",
      "Loss 2.7614087291900686\n",
      "Loss 2.7567805570207145\n",
      "Loss 2.7556667631747676\n",
      "Loss 2.7530862980074695\n",
      "Loss 2.7478599896738607\n",
      "Loss 2.746937260719446\n",
      "Loss 2.744229786031565\n",
      "Loss 2.7417836258683024\n",
      "Loss 2.7409414571786077\n",
      "Loss 2.7401589775830506\n",
      "Loss 2.7440087185142943\n",
      "Loss 2.741407492057777\n",
      "Loss 2.742444835411259\n",
      "Loss 2.740938978733086\n",
      "Loss 2.7383319472544123\n",
      "Loss 2.7383623711315983\n",
      "Loss 2.7373335688413976\n",
      "Loss 2.736674891483216\n",
      "Loss 2.7319609363403545\n",
      "Loss 2.727305289436789\n",
      "Loss 2.7233140278280827\n",
      "Loss 2.7170712712199188\n",
      "Loss 2.719055469256605\n",
      "Loss 2.7184079624455553\n",
      "Loss 2.715521836893899\n",
      "Loss 2.709531442211433\n",
      "Loss 2.71443787604402\n",
      "Loss 2.716897386044599\n",
      "Loss 2.714742894212627\n",
      "Loss 2.711252896454599\n",
      "Loss 2.706155474475734\n",
      "Loss 2.703068991480293\n",
      "Loss 2.702759839407082\n",
      "Loss 2.702280240279177\n",
      "Loss 2.7018429950121288\n",
      "Loss 2.701390903226791\n",
      "Loss 2.704543272569218\n",
      "Loss 2.707893324786044\n",
      "Loss 2.7103525777973196\n",
      "Loss 2.70927668891455\n",
      "Loss 2.7109427634334065\n",
      "Loss 2.7115042923018335\n",
      "Loss 2.7102451883449454\n",
      "Loss 2.7134005569305617\n",
      "Loss 2.7136651300772643\n",
      "Loss 2.712347169190037\n",
      "Loss 2.7118558463711424\n",
      "Loss 2.7092767372877913\n",
      "Loss 2.7080708482756686\n",
      "Loss 2.711365088224411\n",
      "Loss 2.7123399039999168\n",
      "Loss 2.712790090376788\n",
      "Loss 2.71287790125814\n",
      "Loss 2.71238125603573\n",
      "Loss 2.713158794786872\n",
      "Loss 2.7135787884008535\n",
      "Loss 2.712741816446977\n",
      "Loss 2.712291418875639\n",
      "Loss 2.709776696120723\n",
      "Loss 2.713453202304386\n",
      "Loss 2.7185293656954834\n",
      "Loss 2.7140645052464505\n",
      "Loss 2.709140341998266\n",
      "Loss 2.7094215954575582\n",
      "Loss 2.7082836358491766\n",
      "Loss 2.707797182434135\n",
      "Loss 2.709730579776149\n",
      "Loss 2.706154263019562\n",
      "Loss 2.705784762014537\n",
      "Loss 2.705901587063616\n",
      "Loss 2.70425162903324\n",
      "Loss 2.705841598747013\n",
      "Loss 2.7059177070989735\n",
      "Loss 2.706135541253856\n",
      "Loss 2.705088213072883\n",
      "Loss 2.7071095733515986\n",
      "Loss 2.706170859893513\n",
      "Loss 2.712104115046953\n",
      "Loss 2.714854318606281\n",
      "Loss 2.715087820084199\n",
      "Loss 2.7133796268727357\n",
      "Loss 2.711134848615219\n",
      "Loss 2.707851186995854\n",
      "Loss 2.710458662163498\n",
      "Loss 2.7095969466452905\n",
      "Loss 2.710869793265553\n",
      "Loss 2.7075352191422057\n",
      "Loss 2.708079192568274\n",
      "Loss 2.711113809691313\n",
      "Loss 2.7090829001863796\n",
      "Loss 2.7121357770381627\n",
      "Loss 2.7140624456661793\n",
      "Loss 2.715329668747545\n",
      "Loss 2.717194343346064\n",
      "Loss 2.7178984510168736\n",
      "Loss 2.7213379315244475\n",
      "Loss 2.7192331953087314\n",
      "Loss 2.7189396326099673\n",
      "Loss 2.7178627733724663\n",
      "Loss 2.719367912721634\n",
      "Loss 2.7179620691029673\n",
      "Loss 2.7195275829897985\n",
      "Loss 2.7180440965848476\n",
      "Loss 2.719277260153312\n",
      "Loss 2.720974725882212\n",
      "Loss 2.7209522548597307\n",
      "Loss 2.7199259973221714\n",
      "Loss 2.71962247527847\n",
      "Loss 2.7183627176929166\n",
      "Loss 2.7217070836745774\n",
      "Loss 2.721237295295087\n",
      "Loss 2.7187575682883955\n",
      "Loss 2.7162569432204213\n",
      "Loss 2.7153540094693502\n",
      "Loss 2.7128964219903047\n",
      "Loss 2.711000203772595\n",
      "Loss 2.7159211634696647\n",
      "Loss 2.71808356309115\n",
      "Loss 2.7192575073153558\n",
      "Loss 2.7193409125451686\n",
      "Loss 2.718529012854249\n",
      "Loss 2.717429766015095\n",
      "Loss 2.718548191947378\n",
      "Loss 2.722810251477861\n",
      "Loss 2.7270294870896774\n",
      "Loss 2.7291926553283914\n",
      "Loss 2.7312554436470196\n",
      "Loss 2.732722299476322\n",
      "Loss 2.7325243944762856\n",
      "Loss 2.7359841898509436\n",
      "Loss 2.7350944957699217\n",
      "Loss 2.733326698406368\n",
      "Loss 2.7352542842234824\n",
      "Loss 2.7348144998852635\n",
      "Loss 2.7382727222275314\n",
      "Loss 2.738543141555119\n",
      "Loss 2.7376481089857814\n",
      "Loss 2.7362105345560446\n",
      "Loss 2.7410489822753985\n",
      "Loss 2.7396527934896535\n",
      "Loss 2.7395429286760153\n",
      "Loss 2.739233858822143\n",
      "Loss 2.7391668234262045\n",
      "Loss 2.737840485816099\n",
      "Loss 2.739038552106437\n",
      "Loss 2.740939112813086\n",
      "Loss 2.738588888556869\n",
      "Loss 2.7381925262860802\n",
      "Loss 2.740690425048305\n",
      "Loss 2.7397895589272183\n",
      "Loss 2.7383438935707574\n",
      "Loss 2.737420219340861\n",
      "Loss 2.737808449905698\n",
      "Loss 2.7372154249172462\n",
      "Loss 2.7358351977926785\n",
      "Loss 2.738347527715895\n",
      "Loss 2.735315951967084\n",
      "Loss 2.733611650010208\n",
      "Loss 2.7309634285837316\n",
      "Loss 2.73023980390641\n",
      "Loss 2.729881809110427\n",
      "Loss 2.727334075440199\n",
      "Loss 2.726407674821421\n",
      "Loss 2.7252041860750524\n",
      "Loss 2.723090754766313\n",
      "Loss 2.7209788467310654\n",
      "Loss 2.720429106240017\n",
      "Loss 2.7183746842843184\n",
      "Loss 2.7194494207077264\n",
      "Loss 2.7213297874107956\n",
      "Loss 2.7200907273827313\n",
      "Loss 2.718975356535882\n",
      "Loss 2.7198973754611178\n",
      "Loss 2.7199611174839515\n",
      "Loss 2.7179309170062727\n",
      "Loss 2.7167075427031957\n",
      "Loss 2.713882479281236\n",
      "Loss 2.7158065461167475\n",
      "Loss 2.7146022616186403\n",
      "Loss 2.7130348567890397\n",
      "Loss 2.720708313809421\n",
      "Loss 2.7225494613489474\n",
      "Loss 2.721422342674152\n",
      "Loss 2.720222061836791\n",
      "Loss 2.719039800594102\n",
      "Loss 2.719581516228971\n",
      "Loss 2.719238826782838\n",
      "Loss 2.7176728475375995\n",
      "Loss 2.7161472688030703\n",
      "Loss 2.719191474038012\n",
      "Loss 2.7185112372171845\n",
      "Loss 2.717416193785026\n",
      "Loss 2.7159162865088216\n",
      "Loss 2.7164605010734046\n",
      "Loss 2.7199009634446405\n",
      "Loss 2.7183108936844533\n",
      "Loss 2.719884334654904\n",
      "Loss 2.7180269730159607\n",
      "Loss 2.716897032595637\n",
      "Loss 2.716861184801374\n",
      "Loss 2.7174259250999517\n",
      "Loss 2.7163635808974504\n",
      "Loss 2.714158725400822\n",
      "Loss 2.7159302140696573\n",
      "Loss 2.714134095789681\n",
      "Loss 2.717495309268491\n",
      "Loss 2.7194554576018946\n",
      "Loss 2.7172395767446336\n",
      "Loss 2.716987237691215\n",
      "Loss 2.7147769844532013\n",
      "Loss 2.716040002209988\n",
      "Loss 2.713882728209153\n",
      "Loss 2.711276401501385\n",
      "Loss 2.710715338096514\n",
      "Loss 2.709724617037055\n",
      "Loss 2.71063253026191\n",
      "Loss 2.711138750551832\n",
      "Loss 2.710514718650476\n",
      "Loss 2.7098886507959548\n",
      "Loss 2.708782289672542\n",
      "Loss 2.708436633570175\n",
      "Loss 2.7084790336508906\n",
      "Loss 2.7082712765353616\n",
      "Loss 2.709098536604866\n",
      "Loss 2.708409653631846\n",
      "Loss 2.708517157587599\n",
      "Loss 2.7088117034745154\n",
      "Loss 2.7099856419222697\n",
      "Loss 2.7119622812736632\n",
      "Loss 2.710237646134276\n",
      "Loss 2.7084693720647355\n",
      "Loss 2.7084758956394897\n",
      "Loss 2.7095810934395455\n",
      "Loss 2.7111139986664057\n",
      "Loss 2.7133996753568774\n",
      "Loss 2.714881915687897\n",
      "Loss 2.7156474582292622\n",
      "Loss 2.7159096878521223\n",
      "Loss 2.719963714212256\n",
      "Loss 2.7221163406127538\n",
      "Loss 2.7218466016337697\n",
      "Loss 2.72257950549223\n",
      "Loss 2.721639179455415\n",
      "Loss 2.7229205538173615\n",
      "Loss 2.72608833122857\n",
      "Loss 2.7260901346050126\n",
      "Loss 2.725784705928351\n",
      "Loss 2.7258271456363814\n",
      "Loss 2.726488813481534\n",
      "Loss 2.725503732949495\n",
      "Loss 2.7265881921287782\n",
      "Loss 2.727153956297025\n",
      "Loss 2.727872666827502\n",
      "Loss 2.7281047760850132\n",
      "Loss 2.727749140262604\n",
      "Loss 2.727081728656891\n",
      "Loss 2.726475634797492\n",
      "Loss 2.7261841011690158\n",
      "Loss 2.724207533163663\n",
      "Loss 2.7242805765024047\n",
      "Loss 2.7255827935072627\n",
      "Loss 2.727475255930308\n",
      "Loss 2.7268206966125357\n",
      "Loss 2.7280762343015073\n",
      "Loss 2.7330907044640504\n",
      "Loss 2.7328570743592886\n",
      "Loss 2.733234900279011\n",
      "Loss 2.7322235986946874\n",
      "Loss 2.7341212963716512\n",
      "Loss 2.7359606733208612\n",
      "Loss 2.7351525981749036\n",
      "Loss 2.733804863917319\n",
      "Loss 2.7308675424708824\n",
      "Loss 2.7292166899678842\n",
      "Loss 2.7273389686135685\n",
      "Loss 2.727914234674033\n",
      "Loss 2.726352792768903\n",
      "Loss 2.7286956086615537\n",
      "Loss 2.728731982274489\n",
      "Loss 2.7290631908871408\n",
      "Loss 2.7278082440181564\n",
      "Loss 2.726575782177625\n",
      "Loss 2.7264316466333685\n",
      "Loss 2.728130152478196\n",
      "Loss 2.7281289098728663\n",
      "Loss 2.726604935116724\n",
      "Loss 2.726646531828481\n",
      "Loss 2.7284195187701483\n",
      "Loss 2.728898114033875\n",
      "Loss 2.7276830299875954\n",
      "Loss 2.727366419065566\n",
      "Loss 2.7267415363324714\n",
      "Loss 2.727118932151364\n",
      "Loss 2.726862847805023\n",
      "Loss 2.7269726063160413\n",
      "Loss 2.7270273267420952\n",
      "Loss 2.7268481938630944\n",
      "Loss 2.7256696436554195\n",
      "Loss 2.7265423045923023\n",
      "Loss 2.727887047767639\n",
      "Loss 2.728792300166153\n",
      "Loss 2.7279191742428637\n",
      "Loss 2.728719641909694\n",
      "Loss 2.7269664471632584\n",
      "Loss 2.7281369837823806\n",
      "Loss 2.728356047706646\n",
      "Loss 2.7287011415744504\n",
      "Loss 2.7284176666924007\n",
      "Loss 2.727515984672347\n",
      "Loss 2.7301379977101865\n",
      "Loss 2.7313817259546473\n",
      "Loss 2.731116081393642\n",
      "Loss 2.7296378866127706\n",
      "Loss 2.7312096327800175\n",
      "Loss 2.7309534529460375\n",
      "Loss 2.7312435602169693\n",
      "Loss 2.732017237675522\n",
      "Loss 2.731480039847203\n",
      "Loss 2.7311347916436346\n",
      "Loss 2.7312061909919088\n",
      "Loss 2.7327681704438165\n",
      "Loss 2.731894667168795\n",
      "Loss 2.731576769674555\n",
      "Loss 2.73274686361667\n",
      "Loss 2.7310262171845685\n",
      "Loss 2.7318960048070475\n",
      "Loss 2.7327834691641466\n",
      "Loss 2.734328165238871\n",
      "Loss 2.7345457244765536\n",
      "Loss 2.732841403161486\n",
      "Loss 2.731168743081995\n",
      "Loss 2.730367413126087\n",
      "Loss 2.730642907466454\n",
      "Loss 2.730577603278081\n",
      "Loss 2.731905002446519\n",
      "Loss 2.7333543677879457\n",
      "Loss 2.7356625173615723\n",
      "Loss 2.734559502318257\n",
      "Loss 2.735039518898494\n",
      "Loss 2.7336942253550705\n",
      "Loss 2.7330752898379402\n",
      "Loss 2.735090857017331\n",
      "Loss 2.7342284044677783\n",
      "Loss 2.73392157984166\n",
      "Loss 2.7330958075234384\n",
      "Loss 2.7329514825055674\n",
      "Loss 2.73359012176813\n",
      "Loss 2.7330873109490037\n",
      "Loss 2.733355138774864\n",
      "Loss 2.7337248495340347\n",
      "Loss 2.733185978778108\n",
      "Loss 2.73273135433159\n",
      "Loss 2.7340650450638226\n",
      "Loss 2.7340584438235043\n",
      "Loss 2.733501403756661\n",
      "Loss 2.736032547309936\n",
      "Loss 2.734924443210135\n",
      "Loss 2.7349912477525202\n",
      "Loss 2.7346648005464926\n",
      "Loss 2.7338458410898845\n",
      "Loss 2.7330980595310606\n",
      "Loss 2.7334833754925056\n",
      "Loss 2.7327218034346434\n",
      "Loss 2.73113273138666\n",
      "Loss 2.732034506543169\n",
      "Loss 2.730442790444507\n",
      "Loss 2.7296401007263067\n",
      "Loss 2.7294248064742583\n",
      "Loss 2.7283330136640913\n",
      "Loss 2.7286650007046185\n",
      "Loss 2.7287252229829666\n",
      "Loss 2.7282802609191545\n",
      "Loss 2.727227617381408\n",
      "Loss 2.7289754973795577\n",
      "Loss 2.7290062646411712\n",
      "Loss 2.7289352633427306\n",
      "Loss 2.7295714279505967\n",
      "Loss 2.7282812367560285\n",
      "Loss 2.727771159989622\n",
      "Loss 2.730556053170618\n",
      "Loss 2.7307871159636146\n",
      "Loss 2.7289457047568226\n",
      "Loss 2.727413452409669\n",
      "Loss 2.727205120311694\n",
      "Loss 2.7280157167666426\n",
      "Loss 2.7272357768087243\n",
      "Loss 2.7262094746532832\n",
      "Loss 2.7249943096619997\n",
      "Loss 2.7242868570317142\n",
      "Loss 2.724857016691455\n",
      "Loss 2.72487573074986\n",
      "Loss 2.726196525369623\n",
      "Loss 2.7270496507595476\n",
      "Loss 2.726345797956866\n",
      "Loss 2.7281080229129264\n",
      "Loss 2.72741714486709\n",
      "Loss 2.7290338496185527\n",
      "Loss 2.7282668696050227\n",
      "Loss 2.727762084862791\n",
      "Loss 2.7280904119664973\n",
      "Loss 2.728348422093746\n",
      "Loss 2.7278556706032893\n",
      "Loss 2.7275829811423855\n",
      "Loss 2.728091499478378\n",
      "Loss 2.727421224697216\n",
      "Loss 2.728987540015214\n",
      "Loss 2.7292428844707017\n",
      "Loss 2.7280665524330616\n",
      "Loss 2.7288200885324017\n",
      "Loss 2.7281259690438\n",
      "Loss 2.729559270137134\n",
      "Loss 2.7295405801292842\n",
      "Loss 2.7278100127225238\n",
      "Loss 2.7288100007366625\n",
      "Loss 2.7273105547280436\n",
      "Loss 2.726963914114679\n",
      "Loss 2.7279628433668215\n",
      "Loss 2.7281787430526503\n",
      "Loss 2.728132949374053\n",
      "Loss 2.7290593341149783\n",
      "Loss 2.7281079856401567\n",
      "Loss 2.727147464777206\n",
      "Loss 2.7259581675496194\n",
      "Loss 2.7252428253353265\n",
      "Loss 2.7250323642025824\n",
      "Loss 2.72428065619121\n",
      "Loss 2.724056082333808\n",
      "Loss 2.723821264340391\n",
      "Loss 2.7242578484439686\n",
      "Loss 2.7251509729747116\n",
      "Loss 2.7249200146292654\n",
      "Loss 2.72523470291567\n",
      "Loss 2.7260530013776356\n",
      "Loss 2.725769952837735\n",
      "Loss 2.7262112132504455\n",
      "Loss 2.7268876017563985\n",
      "Loss 2.72551315563907\n",
      "Loss 2.7256026957513524\n",
      "Loss 2.72581497580572\n",
      "Loss 2.725379917399358\n",
      "Loss 2.726857309091313\n",
      "Loss 2.725297714085192\n",
      "Loss 2.7252735849576695\n",
      "Loss 2.7250404465479483\n",
      "Loss 2.7239185548429727\n",
      "Loss 2.7225612628379925\n",
      "Loss 2.721666722042077\n",
      "Loss 2.7209927001525727\n",
      "Loss 2.722671899433327\n",
      "Loss 2.721341220597426\n",
      "Loss 2.720230609624834\n",
      "Loss 2.719800452675138\n",
      "Loss 2.7208107924184595\n",
      "Loss 2.720144250357388\n",
      "Loss 2.719085222315197\n",
      "Loss 2.718893270248627\n",
      "Loss 2.7198534627722752\n",
      "Loss 2.7210200262304984\n",
      "Loss 2.7201298787088817\n",
      "Loss 2.7190112262866535\n",
      "Loss 2.7209088821653062\n",
      "Loss 2.721259987062099\n",
      "Loss 2.7197467737446797\n",
      "Loss 2.719622489780867\n",
      "Loss 2.719660253834918\n",
      "Loss 2.71921480232245\n",
      "Loss 2.7192506846771054\n",
      "Loss 2.7199115264377163\n",
      "Loss 2.7185746398611483\n",
      "Loss 2.717287176674412\n",
      "Loss 2.7190841713389338\n",
      "Loss 2.718484801054001\n",
      "Loss 2.7196862409241125\n",
      "Loss 2.7201322119969586\n",
      "Loss 2.7205366065216063\n",
      "Loss 2.7201519135469066\n",
      "Loss 2.7186509048159233\n",
      "Loss 2.719542314235572\n",
      "Loss 2.7194943616803386\n",
      "Loss 2.7197448562818862\n",
      "Loss 2.7195590587690024\n",
      "Loss 2.721202735961238\n",
      "Loss 2.720583637235289\n",
      "Loss 2.7206006290972797\n",
      "Loss 2.720685720988146\n",
      "Loss 2.7196072336825186\n",
      "Loss 2.7187531281490713\n",
      "Loss 2.718289910046658\n",
      "Loss 2.718792110858762\n",
      "Loss 2.720768029037863\n",
      "Loss 2.721051591179114\n",
      "Loss 2.720737095822426\n",
      "Loss 2.7224516095862974\n",
      "Loss 2.7222507821662085\n",
      "Loss 2.7215997534759286\n",
      "Loss 2.7228608753474504\n",
      "Loss 2.7220306979080626\n",
      "Loss 2.723719202368348\n",
      "Loss 2.7226716032380867\n",
      "Loss 2.721577557508762\n",
      "Loss 2.72054115206049\n",
      "Loss 2.720280076663918\n",
      "Loss 2.7200831316879297\n",
      "Loss 2.7188378107292572\n",
      "Loss 2.7186284019746854\n",
      "Loss 2.7190274711425713\n",
      "Loss 2.7186033494940633\n",
      "Loss 2.7184086930788034\n",
      "Loss 2.7186658145806133\n",
      "Loss 2.7178821434216065\n",
      "Loss 2.718123062980698\n",
      "Loss 2.7169106015445963\n",
      "Loss 2.7164473890179424\n",
      "Loss 2.715614547463785\n",
      "Loss 2.715619605949947\n",
      "Loss 2.7152430755419057\n",
      "Loss 2.715227957697644\n",
      "Loss 2.7148019882019407\n",
      "Loss 2.7146015139616835\n",
      "Loss 2.7150081799991095\n",
      "Loss 2.7151969724369476\n",
      "Loss 2.7150278008055118\n",
      "Loss 2.715487394237235\n",
      "Loss 2.717299659811071\n",
      "Loss 2.717912447611491\n",
      "Loss 2.717490977013605\n",
      "Loss 2.7177210835269494\n",
      "Loss 2.7181243353361224\n",
      "Loss 2.7175746362936164\n",
      "Loss 2.7167507239124356\n",
      "Loss 2.7165751417652975\n",
      "Loss 2.717693511901363\n",
      "Loss 2.717278599616898\n",
      "Loss 2.716838770808532\n",
      "Loss 2.716861757901463\n",
      "Loss 2.71748121537203\n",
      "Loss 2.7170780449802003\n",
      "Loss 2.7169241760012715\n",
      "Loss 2.7169928102431\n",
      "Loss 2.7181604639689128\n",
      "Loss 2.719578250099712\n",
      "Loss 2.7185850178230706\n",
      "Loss 2.7180066208130698\n",
      "Loss 2.7178317400323553\n",
      "Loss 2.718270272453912\n",
      "Loss 2.717689859849968\n",
      "Loss 2.717083410228171\n",
      "Loss 2.7165022667806946\n",
      "Loss 2.7157388809583387\n",
      "Loss 2.7158355843680244\n",
      "Loss 2.7168782193718557\n",
      "Loss 2.7170869699910156\n",
      "Loss 2.71735094836562\n",
      "Loss 2.7167781275781717\n",
      "Loss 2.7168229270319566\n",
      "Loss 2.7156779072440043\n",
      "Loss 2.7162639500733987\n",
      "Loss 2.715696446420783\n",
      "Loss 2.7153311676636065\n",
      "Loss 2.7167964455779168\n",
      "Loss 2.717245886111226\n",
      "Loss 2.718651174059075\n",
      "Loss 2.7180505751225925\n",
      "Loss 2.719044370627871\n",
      "Loss 2.7186810817585125\n",
      "Loss 2.7185762579547625\n",
      "Loss 2.7190423567591018\n",
      "Loss 2.7194290139382926\n",
      "Loss 2.71976968318597\n",
      "Loss 2.719569843709469\n",
      "Loss 2.718446128097222\n",
      "Loss 2.7176475890785703\n",
      "Loss 2.71749669542616\n",
      "Loss 2.7173043157349634\n",
      "Loss 2.7165205605605554\n",
      "Loss 2.7163885605236717\n",
      "Loss 2.715644298934543\n",
      "Loss 2.7158565453775636\n",
      "Loss 2.715319753576208\n",
      "Loss 2.715118133989099\n",
      "Loss 2.715325979804862\n",
      "Loss 2.7155352386946237\n",
      "Loss 2.7157681215671365\n",
      "Loss 2.7148230454415003\n",
      "Loss 2.715590588138217\n",
      "Loss 2.7165839105196623\n",
      "Loss 2.7164239392836995\n",
      "Loss 2.716626392082793\n",
      "Loss 2.718130486259925\n",
      "Loss 2.71811252521502\n",
      "Loss 2.717716914149914\n",
      "Loss 2.716999852994703\n",
      "Loss 2.7179804822477647\n",
      "Loss 2.7170338539667025\n",
      "Loss 2.7170359454378987\n",
      "Loss 2.7174068357445917\n",
      "Loss 2.7185487440471827\n",
      "Loss 2.7183892043038487\n",
      "Loss 2.7178468403415144\n",
      "Loss 2.7185515066305794\n",
      "Loss 2.719700270184823\n",
      "Loss 2.7198471660785217\n",
      "Loss 2.719287372933599\n",
      "Loss 2.7200935404401876\n",
      "Loss 2.7202786012516906\n",
      "Loss 2.7190136840734533\n",
      "Loss 2.7196768190618235\n",
      "Loss 2.721723540304204\n",
      "Loss 2.721341490525659\n",
      "Loss 2.7206130708675635\n",
      "Loss 2.7197020668864407\n",
      "Loss 2.7200112478783125\n",
      "Loss 2.720088729930268\n",
      "Loss 2.7207939527421723\n",
      "Loss 2.7209799676938773\n",
      "Loss 2.7206449818050893\n",
      "Loss 2.7215784937959295\n",
      "Loss 2.72202310920693\n",
      "Loss 2.72198393402236\n",
      "Loss 2.721314330116495\n",
      "Loss 2.721631502996932\n",
      "Loss 2.7214069446905906\n",
      "Loss 2.721416889140708\n",
      "Loss 2.722115570676419\n",
      "Loss 2.7212145120097744\n",
      "Loss 2.721451824857402\n",
      "Loss 2.721565496399903\n",
      "Loss 2.7217510110921297\n",
      "Loss 2.721605810457996\n",
      "Loss 2.7218357463066396\n",
      "Loss 2.72192739320015\n",
      "Loss 2.721904288396201\n",
      "Loss 2.721053458127939\n",
      "Loss 2.7223114213317023\n",
      "Loss 2.723465283494087\n",
      "Loss 2.7232756431624483\n",
      "Loss 2.724282447688916\n",
      "Loss 2.7232620859963035\n",
      "Loss 2.7230746452648225\n",
      "Loss 2.722726058190382\n",
      "Loss 2.7228720373209145\n",
      "Loss 2.7225082679077834\n",
      "Loss 2.7213270466685446\n",
      "Loss 2.720954486825304\n",
      "Loss 2.7206603122807147\n",
      "Loss 2.7206526496751824\n",
      "Loss 2.7209879951913805\n",
      "Loss 2.7198039579660374\n",
      "Loss 2.7206234050334173\n",
      "Loss 2.7213235443234445\n",
      "Loss 2.7218153526571656\n",
      "Loss 2.721842457855729\n",
      "Loss 2.721826640814357\n",
      "Loss 2.720817336099065\n",
      "Loss 2.7199690734821815\n",
      "Loss 2.719820737942277\n",
      "Loss 2.7187754993988236\n",
      "Loss 2.718293541581324\n",
      "Loss 2.7182877707098263\n",
      "Loss 2.71789778219329\n",
      "Loss 2.7163904192415322\n",
      "Loss 2.7162179224596823\n",
      "Loss 2.7155673373962532\n",
      "Loss 2.7164666187851085\n",
      "Loss 2.716954507476713\n",
      "Loss 2.71611371861369\n",
      "Loss 2.7163822931934014\n",
      "Loss 2.716880460939664\n",
      "Loss 2.717449402954698\n",
      "Loss 2.717106567257788\n",
      "Loss 2.717605842147181\n",
      "Loss 2.719284877333328\n",
      "Loss 2.7189874885438567\n",
      "Loss 2.7186583499161943\n",
      "Loss 2.718120283386924\n",
      "Loss 2.7179904899066067\n",
      "Loss 2.7173530070928735\n",
      "Loss 2.7180849102960116\n",
      "Loss 2.717631713386324\n",
      "Loss 2.718293693266719\n",
      "Loss 2.7179727540262912\n",
      "Loss 2.7188032240965043\n",
      "Loss 2.718322641752204\n",
      "Loss 2.7181354305012335\n",
      "Loss 2.717474657361379\n",
      "Loss 2.718096108313953\n",
      "Loss 2.7185499683645604\n",
      "Loss 2.7188985704522146\n",
      "Loss 2.7179331059677523\n",
      "Loss 2.71924336992559\n",
      "Loss 2.7203639997455085\n",
      "Loss 2.720344422676218\n",
      "Loss 2.720679019891777\n",
      "Loss 2.7203764004306206\n",
      "Loss 2.720184819035276\n",
      "Loss 2.721090644696644\n",
      "Loss 2.7207717872988937\n",
      "Loss 2.720303393506779\n",
      "Loss 2.7193457853667726\n",
      "Loss 2.7185452087065753\n",
      "Loss 2.719326808786\n",
      "Loss 2.718818904914207\n",
      "Loss 2.718494718225732\n",
      "Loss 2.7186408993231885\n",
      "Loss 2.718184239125391\n",
      "Loss 2.7188985124091123\n",
      "Loss 2.7190810417350004\n",
      "Loss 2.7199880816858686\n",
      "Loss 2.719048687146068\n",
      "Loss 2.7190407449284266\n",
      "Loss 2.7192029646464757\n",
      "Loss 2.7196498381151124\n",
      "Loss 2.719012934957153\n",
      "Loss 2.719111364983298\n",
      "Loss 2.719387702790299\n",
      "Loss 2.7199929349829914\n",
      "Loss 2.720325592548652\n",
      "Loss 2.7200059685426923\n",
      "Loss 2.720464393280455\n",
      "Loss 2.720498769420317\n",
      "Loss 2.721574200116671\n",
      "Loss 2.7233321946421896\n",
      "Loss 2.723626537612487\n",
      "Loss 2.724120622996062\n",
      "Loss 2.723974706813267\n",
      "Loss 2.7239853909434792\n",
      "Loss 2.7238253038992495\n",
      "Loss 2.723760431977774\n",
      "Loss 2.7233482911318236\n",
      "Loss 2.7233439151265406\n",
      "Loss 2.723955469504929\n",
      "Loss 2.723439874805831\n",
      "Loss 2.722957395278719\n",
      "Loss 2.723288778293187\n",
      "Loss 2.722948383220845\n",
      "Loss 2.7226110196544138\n",
      "Loss 2.721538843579061\n",
      "Loss 2.721545653179422\n",
      "Loss 2.7210580991545466\n",
      "Loss 2.7204513924577265\n",
      "Loss 2.7208870872771564\n",
      "Loss 2.720235509161457\n",
      "Loss 2.719575077568964\n",
      "Loss 2.7183407986724135\n",
      "Loss 2.7189091165904893\n",
      "Loss 2.718765652046672\n",
      "Loss 2.718299860340297\n",
      "Loss 2.718121404769957\n",
      "Loss 2.717988749552887\n",
      "Loss 2.7176502711640462\n",
      "Loss 2.716892806459611\n",
      "Loss 2.715836596145334\n",
      "Loss 2.7159903970007617\n",
      "Loss 2.716049044783664\n",
      "Loss 2.7158955561785407\n",
      "Loss 2.7159036660036504\n",
      "Loss 2.7156030839307044\n",
      "Loss 2.7151604294908203\n",
      "Loss 2.7156336617548473\n",
      "Loss 2.7162839070388247\n",
      "Loss 2.7159746454392777\n",
      "Loss 2.715665213855212\n",
      "Loss 2.7156813335183676\n",
      "Loss 2.7152089997767357\n",
      "Loss 2.714574267460349\n",
      "Loss 2.7135523463440774\n",
      "Loss 2.713391358641788\n",
      "Loss 2.7127805508559564\n",
      "Loss 2.7119060651262386\n",
      "Loss 2.7120632526667223\n",
      "Loss 2.711312947107578\n",
      "Loss 2.7110405751526225\n",
      "Loss 2.711379130379996\n",
      "Loss 2.710182565940923\n",
      "Loss 2.710740156856743\n",
      "Loss 2.7109881247609016\n",
      "Loss 2.710372837911789\n",
      "Loss 2.710532139716991\n",
      "Loss 2.7109364281172903\n",
      "Loss 2.71147796883378\n",
      "Loss 2.711773816432912\n",
      "Loss 2.7114912347220557\n",
      "Loss 2.711756103521758\n",
      "Loss 2.7114160441612025\n",
      "Loss 2.711411098592422\n",
      "Loss 2.7111335672501826\n",
      "Loss 2.7106908845926934\n",
      "Loss 2.7107393081635554\n",
      "Loss 2.710403951250326\n",
      "Loss 2.710124125239697\n",
      "Loss 2.7099351613858556\n",
      "Loss 2.7105171503028544\n",
      "Loss 2.7103310143328168\n",
      "Loss 2.710311530808271\n",
      "Loss 2.709316455762853\n",
      "Loss 2.7092977381029795\n",
      "Loss 2.7087973986533025\n",
      "Loss 2.708359076290191\n",
      "Loss 2.7083596494727944\n",
      "Loss 2.707776036249964\n",
      "Loss 2.7080020661108377\n",
      "Loss 2.707703257057847\n",
      "Loss 2.70753686948438\n",
      "Loss 2.7070771247710823\n",
      "Loss 2.7064969150308538\n",
      "Loss 2.7062113699927988\n",
      "Loss 2.706359988531972\n",
      "Loss 2.7056578743184034\n",
      "Loss 2.7061151742562264\n",
      "Loss 2.707148807483415\n",
      "Loss 2.7071404128987635\n",
      "Loss 2.7075828603300383\n",
      "Loss 2.7070033152402995\n",
      "Loss 2.706411031049317\n",
      "Loss 2.706126080979955\n",
      "Loss 2.706996034699691\n",
      "Loss 2.7062806879352266\n",
      "Loss 2.7058523733753805\n",
      "Loss 2.7068347276302807\n",
      "Loss 2.707671386839188\n",
      "Loss 2.708376098059724\n",
      "Loss 2.708100050503825\n",
      "Loss 2.7088273144821957\n",
      "Loss 2.7078723837218\n",
      "Loss 2.707763384782351\n",
      "Loss 2.707909062297129\n",
      "Loss 2.7081127899330006\n",
      "Loss 2.7081281690265993\n",
      "Loss 2.70716190514696\n",
      "Loss 2.7070424018830668\n",
      "Loss 2.7067909789741584\n",
      "Loss 2.707413376263834\n",
      "Loss 2.707743844423265\n",
      "Loss 2.706622812260942\n",
      "Loss 2.707107023817634\n",
      "Loss 2.706738358361242\n",
      "Loss 2.707445427893143\n",
      "Loss 2.707169697834895\n",
      "Loss 2.7059454411177835\n",
      "Loss 2.706252117385768\n",
      "Loss 2.7057113766429643\n",
      "Loss 2.7053118967240857\n",
      "Loss 2.7048419132933756\n",
      "Loss 2.7059555465546654\n",
      "Loss 2.7055569333766574\n",
      "Loss 2.704883216076108\n",
      "Loss 2.704940082974276\n",
      "Loss 2.7047949627191126\n",
      "Loss 2.7043878880229677\n",
      "Loss 2.7047707537174226\n",
      "Loss 2.7042826186765088\n",
      "Loss 2.704604302444858\n",
      "Loss 2.7041867078479718\n",
      "Loss 2.7047328363376786\n",
      "Loss 2.705266124976808\n",
      "Loss 2.7048861730880813\n",
      "Loss 2.704075768403524\n",
      "Loss 2.703694443747638\n",
      "Loss 2.7041450852563287\n",
      "Loss 2.7036356669369312\n",
      "Loss 2.7036114770981725\n",
      "Loss 2.7038987790525195\n",
      "Loss 2.704366924343787\n",
      "Loss 2.7044941647165626\n",
      "Loss 2.7047210164727837\n",
      "Loss 2.705113798802293\n",
      "Loss 2.7060898350637808\n",
      "Loss 2.7070736407765237\n",
      "Loss 2.7090452343956644\n",
      "Loss 2.7089358383650874\n",
      "Loss 2.709473604273259\n",
      "Loss 2.7090820406747658\n",
      "Loss 2.7095191246253654\n",
      "Loss 2.710152045940049\n",
      "Loss 2.7102810653709786\n",
      "Loss 2.7101699363302303\n",
      "Loss 2.710624483102949\n",
      "Loss 2.711115836552145\n",
      "Loss 2.710869166130459\n",
      "Loss 2.7108922590677023\n",
      "Loss 2.7114172231561344\n",
      "Loss 2.7114295303937075\n",
      "Loss 2.7125018688970015\n",
      "Loss 2.7121075481234023\n",
      "Loss 2.712522647668774\n",
      "Loss 2.7117507319767964\n",
      "Loss 2.71139876205643\n",
      "Loss 2.7116777254896585\n",
      "Loss 2.710768021965853\n",
      "Loss 2.711024973289325\n",
      "Loss 2.711037940502625\n",
      "Loss 2.7114337820466785\n",
      "Loss 2.7111215887110847\n",
      "Loss 2.7107234561009426\n",
      "Loss 2.710085486749713\n",
      "Loss 2.7099610806537178\n",
      "Loss 2.7093003641796205\n",
      "Loss 2.709024177150417\n",
      "Loss 2.7088405929143593\n",
      "Loss 2.708440672931217\n",
      "Loss 2.7079398658590925\n",
      "Loss 2.7074229825925467\n",
      "Loss 2.7074233853352827\n",
      "Loss 2.7073057722208396\n",
      "Loss 2.7070346880637075\n",
      "Loss 2.706914180076935\n",
      "Loss 2.70671134747304\n",
      "Loss 2.7073802297980665\n",
      "Loss 2.707671506096441\n",
      "Loss 2.707731994547934\n",
      "Loss 2.7078480659398574\n",
      "Loss 2.7082433092953346\n",
      "Loss 2.7080876939944916\n",
      "Loss 2.7083259224555545\n",
      "Loss 2.7087100234165997\n",
      "Loss 2.708315687734235\n",
      "Loss 2.7088203326942994\n",
      "Loss 2.708175919453303\n",
      "Loss 2.7072755519757212\n",
      "Loss 2.7072394620369526\n",
      "Loss 2.7068241371605355\n",
      "Loss 2.707179455459118\n",
      "Loss 2.7065613395347845\n",
      "Loss 2.7068342367253044\n",
      "Loss 2.7065777346477953\n",
      "Loss 2.7060734456245785\n",
      "Loss 2.705543237719806\n",
      "Loss 2.7051532661649422\n",
      "Loss 2.7045290522601895\n",
      "Loss 2.7046781660561208\n",
      "Loss 2.703937565365067\n",
      "Loss 2.7038200056817305\n",
      "Loss 2.7031874614188984\n",
      "Loss 2.704058399345602\n",
      "Loss 2.703953505551211\n",
      "Loss 2.7035376530545414\n",
      "Loss 2.704061249482489\n",
      "Loss 2.703322935400202\n",
      "Loss 2.7043914571609706\n",
      "Loss 2.7042716415650254\n",
      "Loss 2.704360476991215\n",
      "Loss 2.704273980801359\n",
      "Loss 2.7040190420451777\n",
      "Loss 2.7036337254470184\n",
      "Loss 2.703876026242835\n",
      "Loss 2.703901446964184\n",
      "Loss 2.7047646455038867\n",
      "Loss 2.7046355814555954\n",
      "Loss 2.704733323226526\n",
      "Loss 2.705339561028914\n",
      "Loss 2.70615726417027\n",
      "Loss 2.7056346957783086\n",
      "Loss 2.7057323404473816\n",
      "Loss 2.707081926398087\n",
      "Loss 2.7067469885877893\n",
      "Loss 2.7073321334707803\n",
      "Loss 2.7073112030339437\n",
      "Loss 2.7069486415020396\n",
      "Loss 2.707156595095952\n",
      "Loss 2.7066750625339715\n",
      "Loss 2.705831715043443\n",
      "Loss 2.705219041389956\n",
      "Loss 2.7050014676966327\n",
      "Loss 2.704654860603531\n",
      "Loss 2.7044039928004344\n",
      "Loss 2.7041525405794915\n",
      "Loss 2.704490437396638\n",
      "Loss 2.7042225630743983\n",
      "Loss 2.7043391077938197\n",
      "Loss 2.7046348468107837\n",
      "Loss 2.705602371277924\n",
      "Loss 2.7053475802276226\n",
      "Loss 2.7049765237929666\n",
      "Loss 2.7051168692811953\n",
      "Loss 2.7055901161087883\n",
      "Loss 2.705930695408826\n",
      "Loss 2.705209937768376\n",
      "Loss 2.70460577224586\n",
      "Loss 2.7048064117309156\n",
      "Loss 2.7056386102617314\n",
      "Loss 2.706104530474446\n",
      "Loss 2.7055018781562574\n",
      "Loss 2.7058482647363\n",
      "Loss 2.705594420338434\n",
      "Loss 2.7053278722006846\n",
      "Loss 2.704748391989671\n",
      "Loss 2.703911765007566\n",
      "Loss 2.703927376201157\n",
      "Loss 2.703802398760346\n",
      "Loss 2.7034263759537747\n",
      "Loss 2.703533055414556\n",
      "Loss 2.7037599035507744\n",
      "Loss 2.70339330821734\n",
      "Loss 2.7030593024788203\n",
      "Loss 2.702802928920396\n",
      "Loss 2.7038249366362472\n",
      "Loss 2.703624874015632\n",
      "Loss 2.70305478220825\n",
      "Loss 2.702583586215143\n",
      "Loss 2.7024405296885448\n",
      "Loss 2.7024342853436774\n",
      "Loss 2.702070175990876\n",
      "Loss 2.7017578333331933\n",
      "Loss 2.702605324167519\n",
      "Loss 2.703411284236165\n",
      "Loss 2.702823117437247\n",
      "Loss 2.7033386044803116\n",
      "Loss 2.7032002311502303\n",
      "Loss 2.7030452541691976\n",
      "Loss 2.7028579355416626\n",
      "Loss 2.7019303001585344\n",
      "Loss 2.701826098207352\n",
      "Loss 2.7018404115199632\n",
      "Loss 2.7018586252972843\n",
      "Loss 2.701735740045621\n",
      "Loss 2.7020841493614745\n",
      "Loss 2.701925552432723\n",
      "Loss 2.7020378446538156\n",
      "Loss 2.702007977946178\n",
      "Loss 2.7013160939298126\n",
      "Loss 2.7003925169461005\n",
      "Loss 2.700618961595838\n",
      "Loss 2.701093148646554\n",
      "Loss 2.7015067142156766\n",
      "Loss 2.701168319996367\n",
      "Loss 2.700487792177671\n",
      "Loss 2.7001015692417565\n",
      "Loss 2.7003524628580124\n",
      "Loss 2.7004845109200257\n",
      "Loss 2.7007193936028724\n",
      "Loss 2.700706367640047\n",
      "Loss 2.7003497738321824\n",
      "Loss 2.7001931867857456\n",
      "Loss 2.699954034377997\n",
      "Loss 2.6998298551664073\n",
      "Loss 2.700038509093489\n",
      "Loss 2.6994666455268055\n",
      "Loss 2.7002285521038454\n",
      "Loss 2.7007431787538168\n",
      "Loss 2.7006366495024254\n",
      "Loss 2.7006424586678834\n",
      "Loss 2.701085384180082\n",
      "Loss 2.701355504739814\n",
      "Loss 2.701154373968067\n",
      "Loss 2.7011388689903035\n",
      "Loss 2.7020765993308062\n",
      "Loss 2.7019060908503203\n",
      "Loss 2.701464670020869\n",
      "Loss 2.701452326128342\n",
      "Loss 2.7009778684675694\n",
      "Loss 2.700530563028131\n",
      "Loss 2.700744742551778\n",
      "Loss 2.7010725136013303\n",
      "Loss 2.7014151347435984\n",
      "Loss 2.701310136308314\n",
      "Loss 2.70172850720918\n",
      "Loss 2.7014677515480274\n",
      "Loss 2.7012559407396823\n",
      "Loss 2.700583400590248\n",
      "Loss 2.7004722688217795\n",
      "Loss 2.7012916215188634\n",
      "Loss 2.7017211809606834\n",
      "Loss 2.7021497143661297\n",
      "Loss 2.7024528596998243\n",
      "Loss 2.7030007452434965\n",
      "Loss 2.7025576851673816\n",
      "Loss 2.702667577235348\n",
      "Loss 2.703058534126564\n",
      "Loss 2.702707239572292\n",
      "Loss 2.702324366872428\n",
      "Loss 2.702735469626365\n",
      "Loss 2.702406142493512\n",
      "Loss 2.702211776885066\n",
      "Loss 2.7018607513772115\n",
      "Loss 2.701628717130544\n",
      "Loss 2.701081931027075\n",
      "Loss 2.7008543061840893\n",
      "Loss 2.700627545978425\n",
      "Loss 2.7006349579603133\n",
      "Loss 2.700414150575312\n",
      "Loss 2.700468463059076\n",
      "Loss 2.700440796754964\n",
      "Loss 2.7003110391766874\n",
      "Loss 2.6996428901604355\n",
      "Loss 2.6990999506938795\n",
      "Loss 2.698873716043037\n",
      "Loss 2.6988899030122626\n",
      "Loss 2.699434549020835\n",
      "Loss 2.6991898192812878\n",
      "Loss 2.699191952549642\n",
      "Loss 2.698832356468696\n",
      "Loss 2.698379606594018\n",
      "Loss 2.698159013979874\n",
      "Loss 2.697829580412417\n",
      "Loss 2.6975188775330663\n",
      "Loss 2.6966512814857797\n",
      "Loss 2.6971781549782587\n",
      "Loss 2.6969540886771983\n",
      "Loss 2.696431185093949\n",
      "Loss 2.6972650942230225\n",
      "Loss 2.6971700640319347\n",
      "Loss 2.6969320666390106\n",
      "Loss 2.6965921360211476\n",
      "Loss 2.696369152738337\n",
      "Loss 2.6960350822642507\n",
      "Loss 2.6954979387248397\n",
      "Loss 2.6956951653378867\n",
      "Loss 2.6960103754587887\n",
      "Loss 2.6958163927243377\n",
      "Loss 2.6952584178012513\n",
      "Loss 2.695006083165909\n",
      "Loss 2.6948768450038747\n",
      "Loss 2.6954181516349833\n",
      "Loss 2.6954100026548664\n",
      "Loss 2.6950953885312137\n",
      "Loss 2.6952962404649594\n",
      "Loss 2.6954137881440072\n",
      "Loss 2.695528062078103\n",
      "Loss 2.6954234649103586\n",
      "Loss 2.6957852486456475\n",
      "Loss 2.695807178408775\n",
      "Loss 2.695902197822085\n",
      "Loss 2.6965268941255225\n",
      "Loss 2.6960733946208117\n",
      "Loss 2.695767162481944\n",
      "Loss 2.696297554723148\n",
      "Loss 2.696970004576113\n",
      "Loss 2.6971258976556514\n",
      "Loss 2.6977631999702543\n",
      "Loss 2.697554610539228\n",
      "Loss 2.6973467543849154\n",
      "Loss 2.6975568159787033\n",
      "Loss 2.697865412389498\n",
      "Loss 2.697757324511388\n",
      "Loss 2.69703466773497\n",
      "Loss 2.696617952448028\n",
      "Loss 2.6963985957928935\n",
      "Loss 2.6961915189249916\n",
      "Loss 2.696179489824918\n",
      "Loss 2.696274832995363\n",
      "Loss 2.695435234523392\n",
      "Loss 2.695981195421411\n",
      "Loss 2.6964846916652334\n",
      "Loss 2.6967992380376575\n",
      "Loss 2.6963916149194636\n",
      "Loss 2.695880879783704\n",
      "Loss 2.6953504559344847\n",
      "Loss 2.6951420797772694\n",
      "Loss 2.69549741932206\n",
      "Loss 2.694875916536038\n",
      "Loss 2.6950902269035004\n",
      "Loss 2.69554127069296\n",
      "Loss 2.695114063374921\n",
      "Loss 2.6949916018481637\n",
      "Loss 2.69466495441751\n",
      "Loss 2.6943520902857117\n",
      "Loss 2.693819129667669\n",
      "Loss 2.6943661552685847\n",
      "Loss 2.6943706417921574\n",
      "Loss 2.693747060080521\n",
      "Loss 2.693026472247347\n",
      "Loss 2.693282026443176\n",
      "Loss 2.6931724463994504\n",
      "Loss 2.6929735206124263\n",
      "Loss 2.6935468996976266\n",
      "Loss 2.6936551575421563\n",
      "Loss 2.693571312054596\n",
      "Loss 2.6932560564896404\n",
      "Loss 2.6930646398376568\n",
      "Loss 2.6923452352393755\n",
      "Loss 2.691920872369921\n",
      "Loss 2.6925182744883194\n",
      "Loss 2.6927443349082094\n",
      "Loss 2.6921374850672897\n",
      "Loss 2.692598089146164\n",
      "Loss 2.6919819012846165\n",
      "Loss 2.6914520006837392\n",
      "Loss 2.6918972417712212\n",
      "Loss 2.6915974292295872\n",
      "Loss 2.6919981441730845\n",
      "Loss 2.6923483479031938\n",
      "Loss 2.6923719059508127\n",
      "Loss 2.692462500323949\n",
      "Loss 2.692049981450391\n",
      "Loss 2.6911361996511394\n",
      "Loss 2.691345892462902\n",
      "Loss 2.6914574991256868\n",
      "Loss 2.6919268225518755\n",
      "Loss 2.6916142331476616\n",
      "Epoch 2/5, Loss: 2.6916\n",
      "Loss 2.9165783286094666\n",
      "Loss 2.5245153963565827\n",
      "Loss 2.494769835472107\n",
      "Loss 2.306926554441452\n",
      "Loss 2.30101624250412\n",
      "Loss 2.362239158153534\n",
      "Loss 2.489305417878287\n",
      "Loss 2.518573801219463\n",
      "Loss 2.431638643476698\n",
      "Loss 2.4566434013843534\n",
      "Loss 2.4156808603893625\n",
      "Loss 2.3714874843756357\n",
      "Loss 2.4914899294192976\n",
      "Loss 2.4548486343451907\n",
      "Loss 2.4237387696901957\n",
      "Loss 2.4332541942596437\n",
      "Loss 2.511989268134622\n",
      "Loss 2.569134803613027\n",
      "Loss 2.589505780370612\n",
      "Loss 2.547631660103798\n",
      "Loss 2.5207590489160445\n",
      "Loss 2.5655909435315567\n",
      "Loss 2.5574799040089484\n",
      "Loss 2.5684314986069996\n",
      "Loss 2.575612142086029\n",
      "Loss 2.587185320028892\n",
      "Loss 2.6028520977055587\n",
      "Loss 2.6114270874432157\n",
      "Loss 2.600021250905662\n",
      "Loss 2.6290049358208973\n",
      "Loss 2.6520581960678102\n",
      "Loss 2.6605581030249597\n",
      "Loss 2.660466206073761\n",
      "Loss 2.645658812452765\n",
      "Loss 2.6497808149882727\n",
      "Loss 2.661984365516239\n",
      "Loss 2.6414777968380903\n",
      "Loss 2.652952295228055\n",
      "Loss 2.6404581889128074\n",
      "Loss 2.6351553606987\n",
      "Loss 2.6472538596246302\n",
      "Loss 2.6637761768840607\n",
      "Loss 2.6586827225463336\n",
      "Loss 2.6725269011475823\n",
      "Loss 2.680002834002177\n",
      "Loss 2.668421794279762\n",
      "Loss 2.669265115007441\n",
      "Loss 2.6531535744667054\n",
      "Loss 2.6506804106186848\n",
      "Loss 2.6532304644584657\n",
      "Loss 2.664501261711121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[305], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 2\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq2seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pairs_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[304], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, pairs, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/envs/LSTM_torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[294], line 103\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src_batch, trg_batch, src_len, trg_len, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    100\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg_batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trg_len):\n\u001b[0;32m--> 103\u001b[0m     prediction, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     outputs[i] \u001b[38;5;241m=\u001b[39m prediction\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m teacher_forcing_ratio:\n",
      "File \u001b[0;32m~/envs/LSTM_torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[294], line 58\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, hidden, cell_state)\u001b[0m\n\u001b[1;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m x, (hidden, cell_state) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, hidden, cell_state\n",
      "File \u001b[0;32m~/envs/LSTM_torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/LSTM_torch/lib/python3.8/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(seq2seq, train_pairs_final, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ea147-c71c-49b6-be37-8f6022db0b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce4904c8-87c3-4391-9d36-4090ec460ce6",
   "metadata": {},
   "source": [
    "it can also be caused if you have a mismatch between the dimension of your input tensor and the dimensions of your nn.Linear module. (ex. x.shape = (a, b) and nn.Linear(c, c, bias=False) with c not matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fc674-a203-43e9-bf8f-bf7c8fda9ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstmtorch",
   "language": "python",
   "name": "lstmtorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
