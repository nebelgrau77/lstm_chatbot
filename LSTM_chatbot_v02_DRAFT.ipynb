{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa4d069-dda9-478d-99f8-021fe6bfdaf1",
   "metadata": {},
   "source": [
    "# NEW ATTEMPT\n",
    "\n",
    "## RNN LSTM Chatbot project\n",
    "In this project I'm creating a chatbot that is supposed to answer questions from the Stanford Questions & Answers dataset SQuAD1, using a sequence-to-sequence Encoder-Decoder recurrent neural network architecture in PyTorch.\n",
    "\n",
    "To make the notebook more readable and the code more modular, all helper functions (data ingestion and preparation, data analysis, vocabulary creation) were moved to modules.\n",
    "\n",
    "The model for easier debugging is kept in the main notebook for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d838c4-c730-490d-a448-1b3883f6752e",
   "metadata": {},
   "source": [
    "## STRATEGY\n",
    "\n",
    "As I want to follow the example provided by the mentor, and get to the point where I can have a correctly working dataloader and process batches of data, I will make the following modification to my previous approach:\n",
    "\n",
    "- Add words from both questions and answers to the same vocabulary, in other words use only one vocabulary instead of two separate ones\n",
    "    - the consequence of this approach is that the resulting answers from the chatbot would use \"chopped\", stemmed words, meanwhile ideally the answers would have their own vocabulary with unstemmed words\n",
    "- Instead of creating a list of pairs with questions and answers converted to tensors, I will turn the sequences of tokens into simple lists of integers (indexes) and have them in the dataframe, to later feed to the dataset/dataloader\n",
    "- Everything will be processed together and only then the complete dataframe will be split into train and test (and val)\n",
    "- Both questions and answers will be padded to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ce2f19-0379-4ea1-8572-9ce8f0291410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2372d448-2531-401b-ae9f-6282c2c27856",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b5ec54-9fb5-494b-b9bb-01186b1127bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4304ba9f-d1e8-4d32-b4b2-def8e6debfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import SQuAD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd83eaa5-c5c8-4b21-ba1b-22b7bb19392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = SQuAD1(\"root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e56fc51-59ef-4746-b03d-da2263084fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /shared/home/u076079/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /shared/home/u076079/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /shared/home/u076079/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from modules.data import get_dataframe, tokenize_sentence, sample_df_num, sample_df_perc, get_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0374b-29d2-44bc-b560-a9efff7bc6e6",
   "metadata": {},
   "source": [
    "### Using just the train_df for now (big enough to use for training, testing and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277e22c9-ac55-46be-a624-7da8f5a35b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_dataframe(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62aea57-ef21-462a-a530-20fa976b147d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1a1b19-18c6-4962-a53d-ce3c326383bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "\n",
       "                       Answer  \n",
       "0  Saint Bernadette Soubirous  \n",
       "1   a copper statue of Christ  \n",
       "2           the Main Building  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d2364c-aa98-4c03-89b3-748d06b21106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c595b25c-8c27-4cd0-8f57-b629b013fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Question', 'Answer']:\n",
    "    train_df[col + '_tokens'] = train_df[col].apply(lambda s: tokenize_sentence(s, normalization='stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b958309-8e3c-4597-9764-7272da48c0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Question_tokens</th>\n",
       "      <th>Answer_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>[whom, virgin, mari, alleg, appear, 1858, lour...</td>\n",
       "      <td>[saint, bernadett, soubir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>[what, front, notr, dame, main, build]</td>\n",
       "      <td>[copper, statu, christ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>[basilica, sacr, heart, notr, dame, besid, whi...</td>\n",
       "      <td>[main, build]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "\n",
       "                       Answer  \\\n",
       "0  Saint Bernadette Soubirous   \n",
       "1   a copper statue of Christ   \n",
       "2           the Main Building   \n",
       "\n",
       "                                     Question_tokens  \\\n",
       "0  [whom, virgin, mari, alleg, appear, 1858, lour...   \n",
       "1             [what, front, notr, dame, main, build]   \n",
       "2  [basilica, sacr, heart, notr, dame, besid, whi...   \n",
       "\n",
       "                Answer_tokens  \n",
       "0  [saint, bernadett, soubir]  \n",
       "1     [copper, statu, christ]  \n",
       "2               [main, build]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c9121-50b2-465e-9d8e-5c90bb62fc58",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "\n",
    "I could remove the short sentences here, but as there must be also a vocabulary cleanup to get rid of rare words, it might be a better idea to remove those words first, and then drop rows containing the removed words, and also the very short and very long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a49f78-dc80-400f-98c7-f5710c71309d",
   "metadata": {},
   "source": [
    "### Single vocabulary for both questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da1ae8d2-941c-429f-b75a-c6125da70b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "commonVocab = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0eb5ab1-f322-4330-8d50-64dbc8809191",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Question_tokens', 'Answer_tokens']:\n",
    "    for idx, row in train_df.iterrows():\n",
    "        commonVocab.add_sentence(row[col])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54be3fa5-556a-4f86-b3ae-960ad8890deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44534"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commonVocab.n_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c581a57-3f39-4afa-b14d-f37703d73c52",
   "metadata": {},
   "source": [
    "### Remove the least common words from the ~sentences~ vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "458f2628-85cb-4885-a95c-8eaab25f58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times at most a word occurs to be considered an outlier\n",
    "outlier_threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e920271-7ec1-4a6a-a2ba-36635363469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_outliers = get_outliers(commonVocab,outlier_threshold+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67304e49-3dde-4c92-a7b7-98278917b80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lourd',\n",
       " 'grotto',\n",
       " 'businessweek',\n",
       " 'professorship',\n",
       " 'gurian',\n",
       " 'publican',\n",
       " 'kellogg',\n",
       " 'bout',\n",
       " 'showdown',\n",
       " 'anticathol']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_outliers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28efee04-4117-4626-b545-f980e8c58be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31068"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9a6a9ec-062d-45e8-b3ad-1d72f183cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocab_outliers:\n",
    "    commonVocab.remove_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe389876-8d3e-4806-b27a-da705109b742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13466"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commonVocab.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3db6f-c162-4096-ad37-6ce26d7dd15f",
   "metadata": {},
   "source": [
    "### Remove rows containing the words not present in the cleaned vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f62fb431-e310-4bb9-a3a7-8327d7b9b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outlier = 'kellogg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "458e22f2-d0e3-4db1-b27a-ed804ae95f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kellogg', 'institut', 'intern', 'studi', 'part', 'which', 'univers'] 69\n",
      "['kellogg', 'poptart'] 7711\n",
      "['which', 'campus', 'hold', 'undergradu', 'school', 'graduat', 'school', 'kellogg', 'school', 'manag'] 39477\n"
     ]
    }
   ],
   "source": [
    "for idx, row in train_df.iterrows():\n",
    "    for col in ['Question_tokens', 'Answer_tokens']:\n",
    "        if test_outlier in row[col]:\n",
    "            print(row[col], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83c2986d-3c09-40dd-a548-735776f029c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['all_tokens'] = train_df['Question_tokens'] + train_df['Answer_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "499c74d2-f567-4188-a8ed-74ee18d11eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['all_tokens'] = train_df['all_tokens'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abbc65b0-1faf-44b6-bd70-4dafc53ef7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Question_tokens</th>\n",
       "      <th>Answer_tokens</th>\n",
       "      <th>all_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>[whom, virgin, mari, alleg, appear, 1858, lour...</td>\n",
       "      <td>[saint, bernadett, soubir]</td>\n",
       "      <td>[1858, mari, appear, alleg, soubir, saint, who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>[what, front, notr, dame, main, build]</td>\n",
       "      <td>[copper, statu, christ]</td>\n",
       "      <td>[build, front, statu, dame, main, copper, what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>[basilica, sacr, heart, notr, dame, besid, whi...</td>\n",
       "      <td>[main, build]</td>\n",
       "      <td>[build, heart, basilica, dame, which, main, sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "\n",
       "                       Answer  \\\n",
       "0  Saint Bernadette Soubirous   \n",
       "1   a copper statue of Christ   \n",
       "2           the Main Building   \n",
       "\n",
       "                                     Question_tokens  \\\n",
       "0  [whom, virgin, mari, alleg, appear, 1858, lour...   \n",
       "1             [what, front, notr, dame, main, build]   \n",
       "2  [basilica, sacr, heart, notr, dame, besid, whi...   \n",
       "\n",
       "                Answer_tokens  \\\n",
       "0  [saint, bernadett, soubir]   \n",
       "1     [copper, statu, christ]   \n",
       "2               [main, build]   \n",
       "\n",
       "                                          all_tokens  \n",
       "0  [1858, mari, appear, alleg, soubir, saint, who...  \n",
       "1  [build, front, statu, dame, main, copper, what...  \n",
       "2  [build, heart, basilica, dame, which, main, sa...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "829882cb-4013-475c-9d20-1905e88195d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = train_df.shape[0]\n",
    "\n",
    "outliers_set = set(vocab_outliers)\n",
    "\n",
    "outlier_idxs = []\n",
    "\n",
    "# scan row by row\n",
    "# in each row go word by word in 'all_tokens' column\n",
    "# if there IS an intersection between the whole 'all_tokens' and the 'outliers_set', it means that the row contains an outlier and has to be removed\n",
    "\n",
    "for idx, row in train_df.iterrows():                \n",
    "    intersection = outliers_set.intersection(row['all_tokens'])\n",
    "    if len(intersection) > 0:            \n",
    "        #print(f'row {idx} contains an outlier: {intersection}')\n",
    "        outlier_idxs.append(idx)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba6380f9-d939-4e48-9dd8-845ccd2a90c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32636"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outlier_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09a86faf-453a-43df-9196-b96bf48068c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Axis'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Level | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Drop specified labels from rows or columns.\n",
       "\n",
       "Remove rows or columns by specifying label names and corresponding\n",
       "axis, or by specifying directly index or column names. When using a\n",
       "multi-index, labels on different levels can be removed by specifying\n",
       "the level. See the `user guide <advanced.shown_levels>`\n",
       "for more information about the now unused levels.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "labels : single label or list-like\n",
       "    Index or column labels to drop.\n",
       "axis : {0 or 'index', 1 or 'columns'}, default 0\n",
       "    Whether to drop labels from the index (0 or 'index') or\n",
       "    columns (1 or 'columns').\n",
       "index : single label or list-like\n",
       "    Alternative to specifying axis (``labels, axis=0``\n",
       "    is equivalent to ``index=labels``).\n",
       "columns : single label or list-like\n",
       "    Alternative to specifying axis (``labels, axis=1``\n",
       "    is equivalent to ``columns=labels``).\n",
       "level : int or level name, optional\n",
       "    For MultiIndex, level from which the labels will be removed.\n",
       "inplace : bool, default False\n",
       "    If False, return a copy. Otherwise, do operation\n",
       "    inplace and return None.\n",
       "errors : {'ignore', 'raise'}, default 'raise'\n",
       "    If 'ignore', suppress error and only existing labels are\n",
       "    dropped.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame or None\n",
       "    DataFrame without the removed index or column labels or\n",
       "    None if ``inplace=True``.\n",
       "\n",
       "Raises\n",
       "------\n",
       "KeyError\n",
       "    If any of the labels is not found in the selected axis.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.loc : Label-location based indexer for selection by label.\n",
       "DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
       "    where (all or any) data are missing.\n",
       "DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n",
       "    removed, optionally only considering certain columns.\n",
       "Series.drop : Return Series with specified index labels removed.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n",
       "...                   columns=['A', 'B', 'C', 'D'])\n",
       ">>> df\n",
       "   A  B   C   D\n",
       "0  0  1   2   3\n",
       "1  4  5   6   7\n",
       "2  8  9  10  11\n",
       "\n",
       "Drop columns\n",
       "\n",
       ">>> df.drop(['B', 'C'], axis=1)\n",
       "   A   D\n",
       "0  0   3\n",
       "1  4   7\n",
       "2  8  11\n",
       "\n",
       ">>> df.drop(columns=['B', 'C'])\n",
       "   A   D\n",
       "0  0   3\n",
       "1  4   7\n",
       "2  8  11\n",
       "\n",
       "Drop a row by index\n",
       "\n",
       ">>> df.drop([0, 1])\n",
       "   A  B   C   D\n",
       "2  8  9  10  11\n",
       "\n",
       "Drop columns and/or rows of MultiIndex DataFrame\n",
       "\n",
       ">>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n",
       "...                              ['speed', 'weight', 'length']],\n",
       "...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
       "...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
       ">>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n",
       "...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n",
       "...                         [250, 150], [1.5, 0.8], [320, 250],\n",
       "...                         [1, 0.8], [0.3, 0.2]])\n",
       ">>> df\n",
       "                big     small\n",
       "lama    speed   45.0    30.0\n",
       "        weight  200.0   100.0\n",
       "        length  1.5     1.0\n",
       "cow     speed   30.0    20.0\n",
       "        weight  250.0   150.0\n",
       "        length  1.5     0.8\n",
       "falcon  speed   320.0   250.0\n",
       "        weight  1.0     0.8\n",
       "        length  0.3     0.2\n",
       "\n",
       ">>> df.drop(index='cow', columns='small')\n",
       "                big\n",
       "lama    speed   45.0\n",
       "        weight  200.0\n",
       "        length  1.5\n",
       "falcon  speed   320.0\n",
       "        weight  1.0\n",
       "        length  0.3\n",
       "\n",
       ">>> df.drop(index='length', level=1)\n",
       "                big     small\n",
       "lama    speed   45.0    30.0\n",
       "        weight  200.0   100.0\n",
       "cow     speed   30.0    20.0\n",
       "        weight  250.0   150.0\n",
       "falcon  speed   320.0   250.0\n",
       "        weight  1.0     0.8\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.8/site-packages/pandas/core/frame.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f8d7fbf-2b50-4a95-9b31-853a2aff7d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 5)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66a5bd5a-1170-49ff-850f-7cd5d61ec383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54963, 5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(outlier_idxs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa67c1-1755-4e87-adc6-461bd4a3b4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1f709e6-5257-4e2f-bb62-93e73004e1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers_set.intersection('witcher')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114e84f-6184-4f7b-bcde-67cb93a30872",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "\n",
    "I need a function that removes the outliers from the vocabulary. And after those are removed, I need one that removes the dataframe rows without those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fbfd0c0-fde9-41e3-835b-ca2091cde41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modules.vocab.Vocab"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(commonVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccee8985-a98a-4582-8414-5d8a425ca93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):        \n",
    "        self.word2index = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}        \n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        # make sure that the special tokens don't get removed as too rare!\n",
    "        self.word2count = {\"<PAD>\":9999999, \"<SOS>\":9999999, \"<EOS>\":9999999, \"<UNK>\":9999999, }\n",
    "        self.n_words = len(self.word2index) # count PAD, SOS, EOS and UNK tokens\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.add_word(word) # using lists of tokens so no need to split\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1     \n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def remove_word(self, word):\n",
    "        idx = self.word2index[word]\n",
    "        \n",
    "        self.word2index.pop(word)\n",
    "        self.index2word.pop(idx)\n",
    "        self.word2count.pop(word)\n",
    "        self.n_words -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9aa70fc9-ed7b-4d1a-9a40-860a291c0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "testVocab = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb47caac-93a2-4fe5-8c8a-f9fa2eebd8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocab at 0x7fc091839df0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a5fb1c9-a5d0-4b97-b844-58229bc498ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 9999999, '<SOS>': 9999999, '<EOS>': 9999999, '<UNK>': 9999999}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58135f8d-ba49-4871-b41f-9d4b16018c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e3948cc8-f2a9-4cd8-a2b5-5bf3d6cbae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testVocab.add_word('witcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4169135c-4615-49d9-bc98-49c5145550fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>', 4: 'witcher'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4e147ad-6652-4dc1-a186-bdaa3e2f2461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 9999999,\n",
       " '<SOS>': 9999999,\n",
       " '<EOS>': 9999999,\n",
       " '<UNK>': 9999999,\n",
       " 'witcher': 1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b7496ef-78bc-419f-9b0a-690b13c43d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "testVocab.remove_word('witcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "736edfb9-2888-466b-b148-f3c3275f0956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 9999999, '<SOS>': 9999999, '<EOS>': 9999999, '<UNK>': 9999999}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab.word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f34bb0d0-534f-467f-b0a5-db4177e4a697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "066dff54-dd26-474b-bce2-c0d9ae42504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testVocab.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cab1a-be62-4966-95dd-1cebe6d6d998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_chatbot",
   "language": "python",
   "name": "lstm_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
